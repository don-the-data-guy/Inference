{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Inference?","text":"<p>Roboflow Inference is an open-source platform designed to simplify the deployment of computer vision models. It enables developers to perform object detection, classification, and instance segmentation and utilize foundation models like CLIP, Segment Anything, and YOLO-World through a Python-native package, a self-hosted inference server, or a fully managed API.</p> <p>Explore our enterprise options for advanced features like server deployment, device management, active learning, and commercial licenses for YOLOv5 and YOLOv8.</p> <p>Get started with our \"Run your first model\" guide</p> Learn about the various ways you can use Inference See all of the models you can run with Inference <p>Here is an example of a model running on a video using Inference:</p>"},{"location":"#install","title":"\ud83d\udcbb install","text":"<p>Inference package requires Python&gt;=3.8,&lt;=3.11. Click here to learn more about running Inference inside Docker.</p> <pre><code>pip install inference\n</code></pre> \ud83d\udc49 additional considerations    ### Hardware    Enhance model performance in GPU-accelerated environments by installing CUDA-compatible dependencies.    <pre><code>pip install inference-gpu\n</code></pre>    ### Model-specific dependencies    The `inference` and `inference-gpu` packages install only the minimal shared dependencies. Install model-specific dependencies to ensure code compatibility and license compliance. Learn more about the [models](https://inference.roboflow.com/#extras) supported by Inference.    <pre><code>pip install inference[yolo-world]\n</code></pre>"},{"location":"#quickstart","title":"\ud83d\udd25 quickstart","text":"<p>Use Inference SDK to run models locally with just a few lines of code. The image input can be a URL, a numpy array, or a PIL image.</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\n\nresults = model.infer(\"https://media.roboflow.com/inference/people-walking.jpg\")\n</code></pre> \ud83d\udc49 roboflow models   Set up your `ROBOFLOW_API_KEY` to access thousands of fine-tuned models shared by the [Roboflow Universe](https://universe.roboflow.com/) community and your custom model. Navigate to \ud83d\udd11 keys section to learn more.  <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"soccer-players-5fuqs/1\")\n\nresults = model.infer(\n    image=\"https://media.roboflow.com/inference/soccer.jpg\",\n    confidence=0.5,\n    iou_threshold=0.5\n)\n</code></pre> \ud83d\udc49 foundational models   - [CLIP Embeddings](https://inference.roboflow.com/foundation/clip) - generate text and image embeddings that you can use for zero-shot classification or assessing image similarity.    <pre><code>from inference.models import Clip\n\nmodel = Clip()\n\nembeddings_text = clip.embed_text(\"a football match\")\nembeddings_image = model.embed_image(\"https://media.roboflow.com/inference/soccer.jpg\")\n</code></pre>  - [Segment Anything](https://inference.roboflow.com/foundation/sam) - segment all objects visible in the image or only those associated with selected points or boxes.    <pre><code>from inference.models import SegmentAnything\n\nmodel = SegmentAnything()\n\nresult = model.segment_image(\"https://media.roboflow.com/inference/soccer.jpg\")\n</code></pre>  - [YOLO-World](https://inference.roboflow.com/foundation/yolo_world) - an almost real-time zero-shot detector that enables the detection of any objects without any training.    <pre><code>from inference.models import YOLOWorld\n\nmodel = YOLOWorld(model_id=\"yolo_world/l\")\n\nresult = model.infer(\n    image=\"https://media.roboflow.com/inference/dog.jpeg\",\n    text=[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"],\n    confidence=0.03\n)\n</code></pre>"},{"location":"#inference-server","title":"\ud83d\udcdf inference server","text":"<p>You can also run Inference as a microservice with Docker.</p>"},{"location":"#deploy-server","title":"deploy server","text":"<p>The inference server is distributed via Docker. Behind the scenes, inference will download and run the image that is appropriate for your hardware. Here, you can learn more about the supported images.</p> <pre><code>inference server start\n</code></pre>"},{"location":"#run-client","title":"run client","text":"<p>Consume inference server predictions using the HTTP client available in the Inference SDK.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=&lt;ROBOFLOW_API_KEY&gt;\n)\nwith client.use_model(model_id=\"soccer-players-5fuqs/1\"):\n    predictions = client.infer(\"https://media.roboflow.com/inference/soccer.jpg\")\n</code></pre> <p>If you're using the hosted API, change the local API URL to <code>https://detect.roboflow.com</code>. Accessing the hosted inference server and/or using any of the fine-tuned models require a <code>ROBOFLOW_API_KEY</code>. For further information, visit the \ud83d\udd11 keys section.</p>"},{"location":"#inference-pipeline","title":"\ud83c\udfa5 inference pipeline","text":"<p>The inference pipeline is an efficient method for processing static video files and streams. Select a model, define the video source, and set a callback action. You can choose from predefined callbacks that allow you to display results on the screen or save them to a file.</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://media.roboflow.com/inference/people-walking.mp4\",\n    on_prediction=render_boxes\n)\n\npipeline.start()\npipeline.join()\n</code></pre>"},{"location":"#keys","title":"\ud83d\udd11 keys","text":"<p>Inference enables the deployment of a wide range of pre-trained and foundational models without an API key. To access thousands of fine-tuned models shared by the Roboflow Universe community, configure your API key.</p> <pre><code>export ROBOFLOW_API_KEY=&lt;YOUR_API_KEY&gt;\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda documentation","text":"<p>Visit our documentation to explore comprehensive guides, detailed API references, and a wide array of tutorials designed to help you harness the full potential of the Inference package.</p>"},{"location":"#license","title":"\u00a9 license","text":"<p>The Roboflow Inference code is distributed under the Apache 2.0 license. However, each supported model is subject to its licensing. Detailed information on each model's license can be found here.</p>"},{"location":"api/","title":"Api","text":"<p>The Roboflow Inference Server provides OpenAPI documentation at the <code>/docs</code> endpoint for use in development.</p> <p>Below is the OpenAPI specification for the Inference Server, rendered with Swagger.</p> <p></p>"},{"location":"contributing/","title":"Contributing to the Roboflow Inference Server \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to Inference.</p> <p>We welcome any contributions to help us improve the quality of Inference.</p> <p>Note</p> <p>Interested in seeing a new model in Inference? File a Feature Request on GitHub.</p>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add support for running inference on a new model.</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new task or feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features","text":"<p>The Inference Server provides a standard interface through which you can work with computer vision models. With Inference Server, you can use state-of-the-art models with your own weights without having to spend time installing dependencies, configuring environments, and writing inference code.</p> <p>We welcome contributions that add support for new models to the project. Before you begin, please make sure that another contributor has not already begun work on the model you want to add. You can check the project README for our roadmap on adding more models.</p> <p>You will need to add documentation for your model and link to it from the <code>inference-server</code> README. You can add a new page to the <code>docs/models</code> directory that describes your model and how to use it. You can use the existing model documentation as a guide for how to structure your documentation.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Create a new branch that describes your changes (i.e. <code>line-counter-docs</code>). Push your changes to the branch on your fork and then submit a pull request to this repository.</p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> </ol> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#code-quality","title":"\ud83e\uddf9 Code quality","text":"<p>We provide two handy commands inside the <code>Makefile</code>, namely:</p> <ul> <li><code>make style</code> to format the code</li> <li><code>make check_code_quality</code> to check code quality (PEP8 basically)</li> </ul>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"cookbooks/","title":"Cookbooks","text":"Inference Cookbooks <p>Read our getting started guides that show how to solve specific problems using Inference.</p> <p></p> <p></p> <p></p>"},{"location":"models/","title":"Models","text":"<p>Roboflow Inference enables you to deploy computer vision models faster than ever.</p> <p>With a <code>pip install inference</code> and <code>inference server start</code>, you can start a server to run a fine-tuned model on images, videos, and streams.</p> <p>Inference supports running object detection, classification, instance segmentation, and foundation models (i.e. SAM, CLIP).</p> <p>You can train and deploy your own custom model or use one of the 50,000+ fine-tuned models shared by the Roboflow Universe community.</p> <p>You can run Inference on an edge device like an NVIDIA Jetson, or on cloud computing platforms like AWS, GCP, and Azure.</p> <p>Get started with our \"Run your first model\" guide</p> <p>Here is an example of a model running on a video using Inference:</p>"},{"location":"models/#features","title":"\ud83d\udcbb Features","text":"<p>Inference provides a scalable method through which you can use computer vision models.</p> <p>Inference is backed by:</p> <ul> <li> <p>A server, so you don\u2019t have to reinvent the wheel when it comes to serving your model to disperate parts of your application.</p> </li> <li> <p>Standard APIs for computer vision tasks, so switching out the model weights and architecture can be done independently of your application code.</p> </li> <li> <p>Model architecture implementations, which implement the tensor parsing glue between images and predictions for supervised models that you've fine-tuned to perform custom tasks.</p> </li> <li> <p>A model registry, so your code can be independent from your model weights &amp; you don't have to re-build and re-deploy every time you want to iterate on your model weights.</p> </li> <li> <p>Data management integrations, so you can collect more images of edge cases to improve your dataset &amp; model the more it sees in the wild.</p> </li> </ul> <p>And more!</p>"},{"location":"models/#install-pip-vs-docker","title":"\ud83d\udccc Install pip vs Docker:","text":"<ul> <li>pip: Installs <code>inference</code> into your Python environment. Lightweight, good for Python-centric projects.</li> <li>Docker: Packages <code>inference</code> with its environment. Ensures consistency across setups; ideal for scalable deployments.</li> </ul>"},{"location":"models/#install","title":"\ud83d\udcbb install","text":""},{"location":"models/#with-onnx-cpu-runtime","title":"With ONNX CPU Runtime:","text":"<p>For CPU powered inference:</p> <pre><code>pip install inference\n</code></pre> <p>or</p> <pre><code>pip install inference-cpu\n</code></pre>"},{"location":"models/#with-onnx-gpu-runtime","title":"With ONNX GPU Runtime:","text":"<p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"models/#without-onnx-runtime","title":"Without ONNX Runtime:","text":"<p>Roboflow Inference uses Onnxruntime as its core inference engine. Onnxruntime provides an array of different execution providers that can optimize inference on differnt target devices. If you decide to install onnxruntime on your own, install inference with:</p> <pre><code>pip install inference-core\n</code></pre> <p>Alternatively, you can take advantage of some advanced execution providers using one of our published docker images.</p>"},{"location":"models/#extras","title":"Extras:","text":"<p>Some functionality requires extra dependencies. These can be installed by specifying the desired extras during installation of Roboflow Inference. e.x. <code>pip install inference[extra]</code></p> extra description <code>clip</code> Ability to use the core <code>CLIP</code> model (by OpenAI) <code>gaze</code> Ability to use the core <code>Gaze</code> model <code>http</code> Ability to run the http interface <code>sam</code> Ability to run the core <code>Segment Anything</code> model (by Meta AI) <code>doctr</code> Ability to use the core <code>doctr</code> model (by Mindee) <p>Note: Both CLIP and Segment Anything require PyTorch to run. These are included in their respective dependencies however PyTorch installs can be highly environment dependent. See the official PyTorch install page for instructions specific to your enviornment.</p> <p>Example install with CLIP dependencies:</p> <pre><code>pip install inference[clip]\n</code></pre>"},{"location":"models/#docker","title":"\ud83d\udc0b docker","text":"<p>You can learn more about Roboflow Inference Docker Image build, pull and run in our documentation.</p> <ul> <li>Run on x86 CPU:</li> </ul> <pre><code>docker run -it --net=host roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <ul> <li>Run on NVIDIA GPU:</li> </ul> <pre><code>docker run -it --network=host --gpus=all roboflow/roboflow-inference-server-gpu:latest\n</code></pre> \ud83d\udc49 more docker run options  - Run on arm64 CPU:  <pre><code>docker run -p 9001:9001 roboflow/roboflow-inference-server-arm-cpu:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `4.x`:  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `5.x`:  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre> <p></p>"},{"location":"models/#cli","title":"\ud83d\udcdf CLI","text":"<p>To use the CLI you will need python 3.7 or higher. To ensure you have the correct version of python, run <code>python --version</code> in your terminal. To install python, follow the instructions here.</p> <p>After you have python installed, install the pypi package <code>inference-cli</code> or <code>inference</code>:</p> <pre><code>pip install inference-cli\n</code></pre> <p>From there you can run the inference server. See Docker quickstart via CLI for more information.</p> <pre><code>inference server start\n</code></pre> <p>CLI supports also stopping the server via: <pre><code>inference server stop\n</code></pre></p> <p>To use the CLI to make inferences, first find your project ID and model version number in Roboflow.</p> <p>See more detailed documentation on HTTP Inference quickstart via CLI.</p> <pre><code>inference infer {image_path} \\\n--project-id {project_id} \\\n--model-version {model_version} \\\n--api-key {api_key}\n</code></pre>"},{"location":"models/#enterprise-license","title":"Enterprise License","text":"<p>With a Roboflow Inference Enterprise License, you can access additional Inference features, including:</p> <ul> <li>Server cluster deployment</li> <li>Device management</li> <li>Active learning</li> <li>YOLOv5 and YOLOv8 model sub-license</li> </ul> <p>To learn more, contact the Roboflow team.</p>"},{"location":"models/#more-roboflow-open-source-projects","title":"More Roboflow Open Source Projects","text":"Project Description supervision General-purpose utilities for use in computer vision projects, from predictions filtering and display to object tracking to model evaluation. Autodistill Automatically label images for use in training computer vision models. Inference (this project) An easy-to-use, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. Notebooks Tutorials for computer vision tasks, from training state-of-the-art models to tracking objects to counting objects in a zone. Collect Automated, intelligent data collection powered by CLIP."},{"location":"yolov8/","title":"Yolov8","text":""},{"location":"docs/reference/nav/","title":"Nav","text":"<ul> <li>inference<ul> <li>core<ul> <li>active_learning<ul> <li>accounting</li> <li>batching</li> <li>cache_operations</li> <li>configuration</li> <li>core</li> <li>entities</li> <li>middlewares</li> <li>post_processing</li> <li>samplers<ul> <li>close_to_threshold</li> <li>contains_classes</li> <li>number_of_detections</li> <li>random</li> </ul> </li> <li>utils</li> </ul> </li> <li>cache<ul> <li>base</li> <li>memory</li> <li>model_artifacts</li> <li>redis</li> <li>serializers</li> </ul> </li> <li>constants</li> <li>devices<ul> <li>utils</li> </ul> </li> <li>entities<ul> <li>common</li> <li>requests<ul> <li>clip</li> <li>cogvlm</li> <li>doctr</li> <li>dynamic_class_base</li> <li>gaze</li> <li>groundingdino</li> <li>inference</li> <li>sam</li> <li>server_state</li> <li>workflows</li> <li>yolo_world</li> </ul> </li> <li>responses<ul> <li>clip</li> <li>cogvlm</li> <li>doctr</li> <li>gaze</li> <li>groundingdino</li> <li>inference</li> <li>notebooks</li> <li>sam</li> <li>server_state</li> <li>workflows</li> </ul> </li> <li>types</li> </ul> </li> <li>env</li> <li>exceptions</li> <li>interfaces<ul> <li>base</li> <li>camera<ul> <li>camera</li> <li>entities</li> <li>exceptions</li> <li>utils</li> <li>video_source</li> </ul> </li> <li>http<ul> <li>http_api</li> <li>orjson_utils</li> </ul> </li> <li>stream<ul> <li>entities</li> <li>inference_pipeline</li> <li>model_handlers<ul> <li>roboflow_models</li> <li>workflows</li> <li>yolo_world</li> </ul> </li> <li>sinks</li> <li>stream</li> <li>utils</li> <li>watchdog</li> </ul> </li> <li>udp<ul> <li>udp_stream</li> </ul> </li> </ul> </li> <li>logger</li> <li>managers<ul> <li>active_learning</li> <li>base</li> <li>decorators<ul> <li>base</li> <li>fixed_size_cache</li> <li>locked_load</li> <li>logger</li> </ul> </li> <li>entities</li> <li>metrics</li> <li>pingback</li> <li>stub_loader</li> </ul> </li> <li>models<ul> <li>base</li> <li>classification_base</li> <li>defaults</li> <li>instance_segmentation_base</li> <li>keypoints_detection_base</li> <li>object_detection_base</li> <li>roboflow</li> <li>stubs</li> <li>types</li> <li>utils<ul> <li>batching</li> <li>keypoints</li> <li>validate</li> </ul> </li> </ul> </li> <li>nms</li> <li>registries<ul> <li>base</li> <li>roboflow</li> </ul> </li> <li>roboflow_api</li> <li>usage</li> <li>utils<ul> <li>drawing</li> <li>environment</li> <li>file_system</li> <li>function</li> <li>hash</li> <li>image_utils</li> <li>notebooks</li> <li>onnx</li> <li>postprocess</li> <li>preprocess</li> <li>requests</li> <li>roboflow</li> <li>s3</li> <li>url_utils</li> <li>visualisation</li> </ul> </li> <li>version</li> </ul> </li> <li>enterprise<ul> <li>device_manager<ul> <li>container_service</li> <li>device_manager</li> <li>helpers</li> <li>metrics_service</li> </ul> </li> <li>parallel<ul> <li>dispatch_manager</li> <li>entrypoint</li> <li>infer</li> <li>parallel_http_api</li> <li>parallel_http_config</li> <li>tasks</li> <li>utils</li> </ul> </li> <li>stream_management<ul> <li>api<ul> <li>app</li> <li>entities</li> <li>errors</li> <li>stream_manager_client</li> </ul> </li> <li>manager<ul> <li>app</li> <li>communication</li> <li>entities</li> <li>errors</li> <li>inference_pipeline_manager</li> <li>serialisation</li> <li>tcp_server</li> </ul> </li> </ul> </li> <li>workflows<ul> <li>complier<ul> <li>core</li> <li>entities</li> <li>execution_engine</li> <li>flow_coordinator</li> <li>graph_parser</li> <li>runtime_input_validator</li> <li>steps_executors<ul> <li>active_learning_middlewares</li> <li>auxiliary</li> <li>constants</li> <li>models</li> <li>types</li> <li>utils</li> </ul> </li> <li>utils</li> <li>validator</li> </ul> </li> <li>constants</li> <li>entities<ul> <li>base</li> <li>inputs</li> <li>outputs</li> <li>steps</li> <li>validators</li> <li>workflows_specification</li> </ul> </li> <li>errors</li> </ul> </li> </ul> </li> <li>models<ul> <li>aliases</li> <li>clip<ul> <li>clip_model</li> </ul> </li> <li>cogvlm<ul> <li>cogvlm</li> </ul> </li> <li>doctr<ul> <li>doctr_model</li> </ul> </li> <li>gaze<ul> <li>gaze</li> <li>l2cs</li> </ul> </li> <li>grounding_dino<ul> <li>grounding_dino</li> </ul> </li> <li>sam<ul> <li>segment_anything</li> </ul> </li> <li>utils</li> <li>vit<ul> <li>vit_classification</li> </ul> </li> <li>yolact<ul> <li>yolact_instance_segmentation</li> </ul> </li> <li>yolo_world<ul> <li>yolo_world</li> </ul> </li> <li>yolonas<ul> <li>yolonas_object_detection</li> </ul> </li> <li>yolov5<ul> <li>yolov5_instance_segmentation</li> <li>yolov5_object_detection</li> </ul> </li> <li>yolov7<ul> <li>yolov7_instance_segmentation</li> </ul> </li> <li>yolov8<ul> <li>yolov8_classification</li> <li>yolov8_instance_segmentation</li> <li>yolov8_keypoints_detection</li> <li>yolov8_object_detection</li> </ul> </li> <li>yolov9<ul> <li>yolov9_object_detection</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/reference/inference/core/constants/","title":"constants","text":""},{"location":"docs/reference/inference/core/env/","title":"env","text":""},{"location":"docs/reference/inference/core/exceptions/","title":"exceptions","text":""},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.ContentTypeInvalid","title":"<code>ContentTypeInvalid</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the content type is invalid.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class ContentTypeInvalid(Exception):\n\"\"\"Raised when the content type is invalid.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.ContentTypeMissing","title":"<code>ContentTypeMissing</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the content type is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class ContentTypeMissing(Exception):\n\"\"\"Raised when the content type is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.EngineIgnitionFailure","title":"<code>EngineIgnitionFailure</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the engine fails to ignite.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class EngineIgnitionFailure(Exception):\n\"\"\"Raised when the engine fails to ignite.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InferenceModelNotFound","title":"<code>InferenceModelNotFound</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the inference model is not found.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InferenceModelNotFound(Exception):\n\"\"\"Raised when the inference model is not found.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InvalidEnvironmentVariableError","title":"<code>InvalidEnvironmentVariableError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when an environment variable is invalid.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidEnvironmentVariableError(Exception):\n\"\"\"Raised when an environment variable is invalid.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InvalidMaskDecodeArgument","title":"<code>InvalidMaskDecodeArgument</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when an invalid argument is provided for mask decoding.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidMaskDecodeArgument(Exception):\n\"\"\"Raised when an invalid argument is provided for mask decoding.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InvalidNumpyInput","title":"<code>InvalidNumpyInput</code>","text":"<p>             Bases: <code>InputImageLoadError</code></p> <p>Raised when the input is an invalid NumPy array.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidNumpyInput(InputImageLoadError):\n\"\"\"Raised when the input is an invalid NumPy array.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the API key is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n\"\"\"Raised when the API key is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.MissingServiceSecretError","title":"<code>MissingServiceSecretError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the service secret is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class MissingServiceSecretError(Exception):\n\"\"\"Raised when the service secret is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.OnnxProviderNotAvailable","title":"<code>OnnxProviderNotAvailable</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the ONNX provider is not available.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class OnnxProviderNotAvailable(Exception):\n\"\"\"Raised when the ONNX provider is not available.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.WorkspaceLoadError","title":"<code>WorkspaceLoadError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when there is an error loading the workspace.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class WorkspaceLoadError(Exception):\n\"\"\"Raised when there is an error loading the workspace.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/logger/","title":"logger","text":""},{"location":"docs/reference/inference/core/nms/","title":"nms","text":""},{"location":"docs/reference/inference/core/nms/#inference.core.nms.non_max_suppression_fast","title":"<code>non_max_suppression_fast(boxes, overlapThresh)</code>","text":"<p>Applies non-maximum suppression to bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>ndarray</code> <p>Array of bounding boxes with confidence scores.</p> required <code>overlapThresh</code> <code>float</code> <p>Overlap threshold for suppression.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of bounding boxes after non-maximum suppression.</p> Source code in <code>inference/core/nms.py</code> <pre><code>def non_max_suppression_fast(boxes, overlapThresh):\n\"\"\"Applies non-maximum suppression to bounding boxes.\n\n    Args:\n        boxes (np.ndarray): Array of bounding boxes with confidence scores.\n        overlapThresh (float): Overlap threshold for suppression.\n\n    Returns:\n        list: List of bounding boxes after non-maximum suppression.\n    \"\"\"\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n    # initialize the list of picked indexes\n    pick = []\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    conf = boxes[:, 4]\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    idxs = np.argsort(conf)\n    # keep looping while some indexes still remain in the indexes\n    # list\n    while len(idxs) &gt; 0:\n        # grab the last index in the indexes list and add the\n        # index value to the list of picked indexes\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n        # find the largest (x, y) coordinates for the start of\n        # the bounding box and the smallest (x, y) coordinates\n        # for the end of the bounding box\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n        # compute the width and height of the bounding box\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n        # compute the ratio of overlap\n        overlap = (w * h) / area[idxs[:last]]\n        # delete all indexes from the index list that have\n        idxs = np.delete(\n            idxs, np.concatenate(([last], np.where(overlap &gt; overlapThresh)[0]))\n        )\n    # return only the bounding boxes that were picked using the\n    # integer data type\n    return boxes[pick].astype(\"float\")\n</code></pre>"},{"location":"docs/reference/inference/core/nms/#inference.core.nms.w_np_non_max_suppression","title":"<code>w_np_non_max_suppression(prediction, conf_thresh=0.25, iou_thresh=0.45, class_agnostic=False, max_detections=300, max_candidate_detections=3000, timeout_seconds=None, num_masks=0, box_format='xywh')</code>","text":"<p>Applies non-maximum suppression to predictions.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>ndarray</code> <p>Array of predictions. Format for single prediction is [bbox x 4, max_class_confidence, (confidence) x num_of_classes, additional_element x num_masks]</p> required <code>conf_thresh</code> <code>float</code> <p>Confidence threshold. Defaults to 0.25.</p> <code>0.25</code> <code>iou_thresh</code> <code>float</code> <p>IOU threshold. Defaults to 0.45.</p> <code>0.45</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to ignore class labels. Defaults to False.</p> <code>False</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections. Defaults to 300.</p> <code>300</code> <code>max_candidate_detections</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>3000</code> <code>timeout_seconds</code> <code>Optional[int]</code> <p>Timeout in seconds. Defaults to None.</p> <code>None</code> <code>num_masks</code> <code>int</code> <p>Number of masks. Defaults to 0.</p> <code>0</code> <code>box_format</code> <code>str</code> <p>Format of bounding boxes. Either 'xywh' or 'xyxy'. Defaults to 'xywh'.</p> <code>'xywh'</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of filtered predictions after non-maximum suppression. Format of a single result is: [bbox x 4, max_class_confidence, max_class_confidence, id_of_class_with_max_confidence, additional_element x num_masks]</p> Source code in <code>inference/core/nms.py</code> <pre><code>def w_np_non_max_suppression(\n    prediction,\n    conf_thresh: float = 0.25,\n    iou_thresh: float = 0.45,\n    class_agnostic: bool = False,\n    max_detections: int = 300,\n    max_candidate_detections: int = 3000,\n    timeout_seconds: Optional[int] = None,\n    num_masks: int = 0,\n    box_format: str = \"xywh\",\n):\n\"\"\"Applies non-maximum suppression to predictions.\n\n    Args:\n        prediction (np.ndarray): Array of predictions. Format for single prediction is\n            [bbox x 4, max_class_confidence, (confidence) x num_of_classes, additional_element x num_masks]\n        conf_thresh (float, optional): Confidence threshold. Defaults to 0.25.\n        iou_thresh (float, optional): IOU threshold. Defaults to 0.45.\n        class_agnostic (bool, optional): Whether to ignore class labels. Defaults to False.\n        max_detections (int, optional): Maximum number of detections. Defaults to 300.\n        max_candidate_detections (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        timeout_seconds (Optional[int], optional): Timeout in seconds. Defaults to None.\n        num_masks (int, optional): Number of masks. Defaults to 0.\n        box_format (str, optional): Format of bounding boxes. Either 'xywh' or 'xyxy'. Defaults to 'xywh'.\n\n    Returns:\n        list: List of filtered predictions after non-maximum suppression. Format of a single result is:\n            [bbox x 4, max_class_confidence, max_class_confidence, id_of_class_with_max_confidence,\n            additional_element x num_masks]\n    \"\"\"\n    num_classes = prediction.shape[2] - 5 - num_masks\n\n    np_box_corner = np.zeros(prediction.shape)\n    if box_format == \"xywh\":\n        np_box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n        np_box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n        np_box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n        np_box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n        prediction[:, :, :4] = np_box_corner[:, :, :4]\n    elif box_format == \"xyxy\":\n        pass\n    else:\n        raise ValueError(\n            \"box_format must be either 'xywh' or 'xyxy', got {}\".format(box_format)\n        )\n\n    batch_predictions = []\n    for np_image_i, np_image_pred in enumerate(prediction):\n        filtered_predictions = []\n        np_conf_mask = (np_image_pred[:, 4] &gt;= conf_thresh).squeeze()\n\n        np_image_pred = np_image_pred[np_conf_mask]\n        cls_confs = np_image_pred[:, 5 : num_classes + 5]\n        if (\n            np_image_pred.shape[0] == 0\n            or np_image_pred.shape[1] == 0\n            or cls_confs.shape[1] == 0\n        ):\n            batch_predictions.append(filtered_predictions)\n            continue\n\n        np_class_conf = np.max(cls_confs, 1)\n        np_class_pred = np.argmax(np_image_pred[:, 5 : num_classes + 5], 1)\n        np_class_conf = np.expand_dims(np_class_conf, axis=1)\n        np_class_pred = np.expand_dims(np_class_pred, axis=1)\n        np_mask_pred = np_image_pred[:, 5 + num_classes :]\n        np_detections = np.append(\n            np.append(\n                np.append(np_image_pred[:, :5], np_class_conf, axis=1),\n                np_class_pred,\n                axis=1,\n            ),\n            np_mask_pred,\n            axis=1,\n        )\n\n        np_unique_labels = np.unique(np_detections[:, 6])\n\n        if class_agnostic:\n            np_detections_class = sorted(\n                np_detections, key=lambda row: row[4], reverse=True\n            )\n            filtered_predictions.extend(\n                non_max_suppression_fast(np.array(np_detections_class), iou_thresh)\n            )\n        else:\n            for c in np_unique_labels:\n                np_detections_class = np_detections[np_detections[:, 6] == c]\n                np_detections_class = sorted(\n                    np_detections_class, key=lambda row: row[4], reverse=True\n                )\n                filtered_predictions.extend(\n                    non_max_suppression_fast(np.array(np_detections_class), iou_thresh)\n                )\n        filtered_predictions = sorted(\n            filtered_predictions, key=lambda row: row[4], reverse=True\n        )\n        batch_predictions.append(filtered_predictions[:max_detections])\n    return batch_predictions\n</code></pre>"},{"location":"docs/reference/inference/core/roboflow_api/","title":"roboflow_api","text":""},{"location":"docs/reference/inference/core/usage/","title":"usage","text":""},{"location":"docs/reference/inference/core/usage/#inference.core.usage.trackUsage","title":"<code>trackUsage(endpoint, actor, n=1)</code>","text":"<p>Tracks the usage of an endpoint by an actor.</p> <p>This function increments the usage count for a given endpoint by an actor. It also handles initialization if the count does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint being accessed.</p> required <code>actor</code> <code>str</code> <p>The actor accessing the endpoint.</p> required <code>n</code> <code>int</code> <p>The number of times the endpoint was accessed. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>None</code> <p>This function does not return anything but updates the memcache client.</p> Source code in <code>inference/core/usage.py</code> <pre><code>def trackUsage(endpoint, actor, n=1):\n\"\"\"Tracks the usage of an endpoint by an actor.\n\n    This function increments the usage count for a given endpoint by an actor.\n    It also handles initialization if the count does not exist.\n\n    Args:\n        endpoint (str): The endpoint being accessed.\n        actor (str): The actor accessing the endpoint.\n        n (int, optional): The number of times the endpoint was accessed. Defaults to 1.\n\n    Returns:\n        None: This function does not return anything but updates the memcache client.\n    \"\"\"\n    # count an inference\n    try:\n        job = endpoint + \"endpoint:::actor\" + actor\n        current_infers = memcache_client.incr(job, n)\n        if current_infers is None:  # not yet set; initialize at 1\n            memcache_client.set(job, n)\n            current_infers = n\n\n            # store key\n            job_keys = memcache_client.get(\"JOB_KEYS\")\n            if job_keys is None:\n                memcache_client.add(\"JOB_KEYS\", json.dumps([job]))\n            else:\n                decoded = json.loads(job_keys)\n                decoded.append(job)\n                decoded = list(set(decoded))\n                memcache_client.set(\"JOB_KEYS\", json.dumps(decoded))\n\n            actor_keys = memcache_client.get(\"ACTOR_KEYS\")\n            if actor_keys is None:\n                ak = {}\n                ak[actor] = n\n                memcache_client.add(\"ACTOR_KEYS\", json.dumps(ak))\n            else:\n                decoded = json.loads(actor_keys)\n                if actor in actor_keys:\n                    actor_keys[actor] += n\n                else:\n                    actor_keys[actor] = n\n                memcache_client.set(\"ACTOR_KEYS\", json.dumps(actor_keys))\n\n    except Exception as e:\n        logger.debug(\"WARNING: there was an error in counting this inference\")\n        logger.debug(e)\n</code></pre>"},{"location":"docs/reference/inference/core/version/","title":"version","text":""},{"location":"docs/reference/inference/core/active_learning/accounting/","title":"accounting","text":""},{"location":"docs/reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.get_images_in_labeling_jobs_of_specific_batch","title":"<code>get_images_in_labeling_jobs_of_specific_batch(all_labeling_jobs, batch_id)</code>","text":"<p>Get the number of images in labeling jobs of a specific batch.</p> <p>Parameters:</p> Name Type Description Default <code>all_labeling_jobs</code> <code>List[dict]</code> <p>All labeling jobs.</p> required <code>batch_id</code> <code>str</code> <p>ID of the batch.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of images in labeling jobs of the batch.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def get_images_in_labeling_jobs_of_specific_batch(\n    all_labeling_jobs: List[dict],\n    batch_id: str,\n) -&gt; int:\n\"\"\"Get the number of images in labeling jobs of a specific batch.\n\n    Args:\n        all_labeling_jobs: All labeling jobs.\n        batch_id: ID of the batch.\n\n    Returns:\n        The number of images in labeling jobs of the batch.\n\n    \"\"\"\n\n    matching_jobs = []\n    for labeling_job in all_labeling_jobs:\n        if batch_id in labeling_job[\"sourceBatch\"]:\n            matching_jobs.append(labeling_job)\n    return sum(job[\"numImages\"] for job in matching_jobs)\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.get_matching_labeling_batch","title":"<code>get_matching_labeling_batch(all_labeling_batches, batch_name)</code>","text":"<p>Get the matching labeling batch.</p> <p>Parameters:</p> Name Type Description Default <code>all_labeling_batches</code> <code>List[dict]</code> <p>All labeling batches.</p> required <code>batch_name</code> <code>str</code> <p>Name of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>The matching labeling batch if found, None otherwise.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def get_matching_labeling_batch(\n    all_labeling_batches: List[dict],\n    batch_name: str,\n) -&gt; Optional[dict]:\n\"\"\"Get the matching labeling batch.\n\n    Args:\n        all_labeling_batches: All labeling batches.\n        batch_name: Name of the batch.\n\n    Returns:\n        The matching labeling batch if found, None otherwise.\n\n    \"\"\"\n    matching_batch = None\n    for labeling_batch in all_labeling_batches:\n        if labeling_batch[\"name\"] == batch_name:\n            matching_batch = labeling_batch\n            break\n    return matching_batch\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.image_can_be_submitted_to_batch","title":"<code>image_can_be_submitted_to_batch(batch_name, workspace_id, dataset_id, max_batch_images, api_key)</code>","text":"<p>Check if an image can be submitted to a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_name</code> <code>str</code> <p>Name of the batch.</p> required <code>workspace_id</code> <code>WorkspaceID</code> <p>ID of the workspace.</p> required <code>dataset_id</code> <code>DatasetID</code> <p>ID of the dataset.</p> required <code>max_batch_images</code> <code>Optional[int]</code> <p>Maximum number of images allowed in the batch.</p> required <code>api_key</code> <code>str</code> <p>API key to use for the request.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the image can be submitted to the batch, False otherwise.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def image_can_be_submitted_to_batch(\n    batch_name: str,\n    workspace_id: WorkspaceID,\n    dataset_id: DatasetID,\n    max_batch_images: Optional[int],\n    api_key: str,\n) -&gt; bool:\n\"\"\"Check if an image can be submitted to a batch.\n\n    Args:\n        batch_name: Name of the batch.\n        workspace_id: ID of the workspace.\n        dataset_id: ID of the dataset.\n        max_batch_images: Maximum number of images allowed in the batch.\n        api_key: API key to use for the request.\n\n    Returns:\n        True if the image can be submitted to the batch, False otherwise.\n    \"\"\"\n    if max_batch_images is None:\n        return True\n    labeling_batches = get_roboflow_labeling_batches(\n        api_key=api_key,\n        workspace_id=workspace_id,\n        dataset_id=dataset_id,\n    )\n    matching_labeling_batch = get_matching_labeling_batch(\n        all_labeling_batches=labeling_batches[\"batches\"],\n        batch_name=batch_name,\n    )\n    if matching_labeling_batch is None:\n        return max_batch_images &gt; 0\n    batch_images_under_labeling = 0\n    if matching_labeling_batch[\"numJobs\"] &gt; 0:\n        labeling_jobs = get_roboflow_labeling_jobs(\n            api_key=api_key, workspace_id=workspace_id, dataset_id=dataset_id\n        )\n        batch_images_under_labeling = get_images_in_labeling_jobs_of_specific_batch(\n            all_labeling_jobs=labeling_jobs[\"jobs\"],\n            batch_id=matching_labeling_batch[\"id\"],\n        )\n    total_batch_images = matching_labeling_batch[\"images\"] + batch_images_under_labeling\n    return max_batch_images &gt; total_batch_images\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/batching/","title":"batching","text":""},{"location":"docs/reference/inference/core/active_learning/cache_operations/","title":"cache_operations","text":""},{"location":"docs/reference/inference/core/active_learning/configuration/","title":"configuration","text":""},{"location":"docs/reference/inference/core/active_learning/configuration/#inference.core.active_learning.configuration.predictions_incompatible_with_dataset","title":"<code>predictions_incompatible_with_dataset(model_type, dataset_type)</code>","text":"<p>The incompatibility occurs when we mix classification with detection - as detection-based predictions are partially compatible (for instance - for key-points detection we may register bboxes from object detection and manually provide key-points annotations)</p> Source code in <code>inference/core/active_learning/configuration.py</code> <pre><code>def predictions_incompatible_with_dataset(\n    model_type: str,\n    dataset_type: str,\n) -&gt; bool:\n\"\"\"\n    The incompatibility occurs when we mix classification with detection - as detection-based\n    predictions are partially compatible (for instance - for key-points detection we may register bboxes\n    from object detection and manually provide key-points annotations)\n    \"\"\"\n    model_is_classifier = CLASSIFICATION_TASK in model_type\n    dataset_is_of_type_classification = CLASSIFICATION_TASK in dataset_type\n    return model_is_classifier != dataset_is_of_type_classification\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/core/","title":"core","text":""},{"location":"docs/reference/inference/core/active_learning/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/active_learning/middlewares/","title":"middlewares","text":""},{"location":"docs/reference/inference/core/active_learning/post_processing/","title":"post_processing","text":""},{"location":"docs/reference/inference/core/active_learning/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/close_to_threshold/","title":"close_to_threshold","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/contains_classes/","title":"contains_classes","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/number_of_detections/","title":"number_of_detections","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/random/","title":"random","text":""},{"location":"docs/reference/inference/core/cache/base/","title":"base","text":""},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache","title":"<code>BaseCache</code>","text":"<p>BaseCache is an abstract base class that defines the interface for a cache.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>class BaseCache:\n\"\"\"\n    BaseCache is an abstract base class that defines the interface for a cache.\n    \"\"\"\n\n    def get(self, key: str):\n\"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def set(self, key: str, value: str, expire: float = None):\n\"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zadd(self, key: str, value: str, score: float, expire: float = None):\n\"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n\"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting index of the range. Defaults to -1.\n            stop (int, optional): The ending index of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        start: Optional[int] = -1,\n        stop: Optional[int] = float(\"inf\"),\n    ):\n\"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def acquire_lock(self, key: str, expire: float = None) -&gt; Any:\n        raise NotImplementedError()\n\n    @contextmanager\n    def lock(self, key: str, expire: float = None) -&gt; Any:\n        logger.debug(f\"Acquiring lock at cache key: {key}\")\n        l = self.acquire_lock(key, expire=expire)\n        try:\n            yield l\n        finally:\n            logger.debug(f\"Releasing lock at cache key: {key}\")\n            l.release()\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n\"\"\"\n        Caches a numpy array.\n\n        Args:\n            key (str): The key to store the value.\n            value (Any): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_numpy(self, key: str) -&gt; Any:\n\"\"\"\n        Retrieves a numpy array from the cache.\n\n        Args:\n            key (str): The key of the value to retrieve.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def get(self, key: str):\n\"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.get_numpy","title":"<code>get_numpy(key)</code>","text":"<p>Retrieves a numpy array from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the value to retrieve.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def get_numpy(self, key: str) -&gt; Any:\n\"\"\"\n    Retrieves a numpy array from the cache.\n\n    Args:\n        key (str): The key of the value to retrieve.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n\"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.set_numpy","title":"<code>set_numpy(key, value, expire=None)</code>","text":"<p>Caches a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def set_numpy(self, key: str, value: Any, expire: float = None):\n\"\"\"\n    Caches a numpy array.\n\n    Args:\n        key (str): The key to store the value.\n        value (Any): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zadd(self, key: str, value: str, score: float, expire: float = None):\n\"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting index of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending index of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n\"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting index of the range. Defaults to -1.\n        stop (int, optional): The ending index of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zremrangebyscore","title":"<code>zremrangebyscore(key, start=-1, stop=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> <code>-1</code> <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> <code>float('inf')</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    start: Optional[int] = -1,\n    stop: Optional[int] = float(\"inf\"),\n):\n\"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/","title":"memory","text":""},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache","title":"<code>MemoryCache</code>","text":"<p>             Bases: <code>BaseCache</code></p> <p>MemoryCache is an in-memory cache that implements the BaseCache interface.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>dict</code> <p>A dictionary to store the cache values.</p> <code>expires</code> <code>dict</code> <p>A dictionary to store the expiration times of the cache values.</p> <code>zexpires</code> <code>dict</code> <p>A dictionary to store the expiration times of the sorted set values.</p> <code>_expire_thread</code> <code>Thread</code> <p>A thread that runs the _expire method.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>class MemoryCache(BaseCache):\n\"\"\"\n    MemoryCache is an in-memory cache that implements the BaseCache interface.\n\n    Attributes:\n        cache (dict): A dictionary to store the cache values.\n        expires (dict): A dictionary to store the expiration times of the cache values.\n        zexpires (dict): A dictionary to store the expiration times of the sorted set values.\n        _expire_thread (threading.Thread): A thread that runs the _expire method.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n\"\"\"\n        Initializes a new instance of the MemoryCache class.\n        \"\"\"\n        self.cache = dict()\n        self.expires = dict()\n        self.zexpires = dict()\n\n        self._expire_thread = threading.Thread(target=self._expire)\n        self._expire_thread.daemon = True\n        self._expire_thread.start()\n\n    def _expire(self):\n\"\"\"\n        Removes the expired keys from the cache and zexpires dictionaries.\n\n        This method runs in an infinite loop and sleeps for MEMORY_CACHE_EXPIRE_INTERVAL seconds between each iteration.\n        \"\"\"\n        while True:\n            now = time.time()\n            keys_to_delete = []\n            for k, v in self.expires.copy().items():\n                if v &lt; now:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                del self.cache[k]\n                del self.expires[k]\n            keys_to_delete = []\n            for k, v in self.zexpires.copy().items():\n                if v &lt; now:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                del self.cache[k[0]][k[1]]\n                del self.zexpires[k]\n            while time.time() - now &lt; MEMORY_CACHE_EXPIRE_INTERVAL:\n                time.sleep(0.1)\n\n    def get(self, key: str):\n\"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Returns:\n            str: The value associated with the key, or None if the key does not exist or is expired.\n        \"\"\"\n        if key in self.expires:\n            if self.expires[key] &lt; time.time():\n                del self.cache[key]\n                del self.expires[key]\n                return None\n        return self.cache.get(key)\n\n    def set(self, key: str, value: str, expire: float = None):\n\"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        self.cache[key] = value\n        if expire:\n            self.expires[key] = expire + time.time()\n\n    def zadd(self, key: str, value: Any, score: float, expire: float = None):\n\"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        if not key in self.cache:\n            self.cache[key] = dict()\n        self.cache[key][score] = value\n        if expire:\n            self.zexpires[(key, score)] = expire + time.time()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n\"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting score of the range. Defaults to -1.\n            stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Returns:\n            list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n        \"\"\"\n        if not key in self.cache:\n            return []\n        keys = sorted([k for k in self.cache[key].keys() if min &lt;= k &lt;= max])\n        if withscores:\n            return [(self.cache[key][k], k) for k in keys]\n        else:\n            return [self.cache[key][k] for k in keys]\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n    ):\n\"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Returns:\n            int: The number of members removed from the sorted set.\n        \"\"\"\n        res = self.zrangebyscore(key, min=min, max=max, withscores=True)\n        keys_to_delete = [k[1] for k in res]\n        for k in keys_to_delete:\n            del self.cache[key][k]\n        return len(keys_to_delete)\n\n    def acquire_lock(self, key: str, expire=None) -&gt; Any:\n        lock: Optional[Lock] = self.get(key)\n        if lock is None:\n            lock = Lock()\n            self.set(key, lock, expire=expire)\n        if expire is None:\n            expire = -1\n        acquired = lock.acquire(timeout=expire)\n        if not acquired:\n            raise TimeoutError()\n        # refresh the lock\n        self.set(key, lock, expire=expire)\n        return lock\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        return self.set(key, value, expire=expire)\n\n    def get_numpy(self, key: str):\n        return self.get(key)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new instance of the MemoryCache class.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"\n    Initializes a new instance of the MemoryCache class.\n    \"\"\"\n    self.cache = dict()\n    self.expires = dict()\n    self.zexpires = dict()\n\n    self._expire_thread = threading.Thread(target=self._expire)\n    self._expire_thread.daemon = True\n    self._expire_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The value associated with the key, or None if the key does not exist or is expired.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def get(self, key: str):\n\"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Returns:\n        str: The value associated with the key, or None if the key does not exist or is expired.\n    \"\"\"\n    if key in self.expires:\n        if self.expires[key] &lt; time.time():\n            del self.cache[key]\n            del self.expires[key]\n            return None\n    return self.cache.get(key)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n\"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    self.cache[key] = value\n    if expire:\n        self.expires[key] = expire + time.time()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zadd(self, key: str, value: Any, score: float, expire: float = None):\n\"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    if not key in self.cache:\n        self.cache[key] = dict()\n    self.cache[key][score] = value\n    if expire:\n        self.zexpires[(key, score)] = expire + time.time()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending score of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values (or value-score pairs if withscores is True) in the specified score range.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n\"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting score of the range. Defaults to -1.\n        stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Returns:\n        list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n    \"\"\"\n    if not key in self.cache:\n        return []\n    keys = sorted([k for k in self.cache[key].keys() if min &lt;= k &lt;= max])\n    if withscores:\n        return [(self.cache[key][k], k) for k in keys]\n    else:\n        return [self.cache[key][k] for k in keys]\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zremrangebyscore","title":"<code>zremrangebyscore(key, min=-1, max=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of members removed from the sorted set.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n):\n\"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Returns:\n        int: The number of members removed from the sorted set.\n    \"\"\"\n    res = self.zrangebyscore(key, min=min, max=max, withscores=True)\n    keys_to_delete = [k[1] for k in res]\n    for k in keys_to_delete:\n        del self.cache[key][k]\n    return len(keys_to_delete)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/model_artifacts/","title":"model_artifacts","text":""},{"location":"docs/reference/inference/core/cache/redis/","title":"redis","text":""},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache","title":"<code>RedisCache</code>","text":"<p>             Bases: <code>BaseCache</code></p> <p>MemoryCache is an in-memory cache that implements the BaseCache interface.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>dict</code> <p>A dictionary to store the cache values.</p> <code>expires</code> <code>dict</code> <p>A dictionary to store the expiration times of the cache values.</p> <code>zexpires</code> <code>dict</code> <p>A dictionary to store the expiration times of the sorted set values.</p> <code>_expire_thread</code> <code>Thread</code> <p>A thread that runs the _expire method.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>class RedisCache(BaseCache):\n\"\"\"\n    MemoryCache is an in-memory cache that implements the BaseCache interface.\n\n    Attributes:\n        cache (dict): A dictionary to store the cache values.\n        expires (dict): A dictionary to store the expiration times of the cache values.\n        zexpires (dict): A dictionary to store the expiration times of the sorted set values.\n        _expire_thread (threading.Thread): A thread that runs the _expire method.\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        ssl: bool = False,\n        timeout: float = 2.0,\n    ) -&gt; None:\n\"\"\"\n        Initializes a new instance of the MemoryCache class.\n        \"\"\"\n        self.client = redis.Redis(\n            host=host,\n            port=port,\n            db=db,\n            decode_responses=False,\n            ssl=ssl,\n            socket_timeout=timeout,\n            socket_connect_timeout=timeout,\n        )\n        logger.debug(\"Attempting to diagnose Redis connection...\")\n        self.client.ping()\n        logger.debug(\"Redis connection established.\")\n        self.zexpires = dict()\n\n        self._expire_thread = threading.Thread(target=self._expire, daemon=True)\n        self._expire_thread.start()\n\n    def _expire(self):\n\"\"\"\n        Removes the expired keys from the cache and zexpires dictionaries.\n\n        This method runs in an infinite loop and sleeps for MEMORY_CACHE_EXPIRE_INTERVAL seconds between each iteration.\n        \"\"\"\n        while True:\n            logger.debug(\"Redis cleaner thread starts cleaning...\")\n            now = time.time()\n            for k, v in copy(list(self.zexpires.items())):\n                if v &lt; now:\n                    tolerance_factor = 1e-14  # floating point accuracy\n                    self.zremrangebyscore(\n                        k[0], k[1] - tolerance_factor, k[1] + tolerance_factor\n                    )\n                    del self.zexpires[k]\n            logger.debug(\"Redis cleaner finished task.\")\n            sleep_time = MEMORY_CACHE_EXPIRE_INTERVAL - (time.time() - now)\n            time.sleep(max(sleep_time, 0))\n\n    def get(self, key: str):\n\"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Returns:\n            str: The value associated with the key, or None if the key does not exist or is expired.\n        \"\"\"\n        item = self.client.get(key)\n        if item is not None:\n            try:\n                return json.loads(item)\n            except (TypeError, ValueError):\n                return item\n\n    def set(self, key: str, value: str, expire: float = None):\n\"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        if not isinstance(value, bytes):\n            value = json.dumps(value)\n        self.client.set(key, value, ex=expire)\n\n    def zadd(self, key: str, value: Any, score: float, expire: float = None):\n\"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        # serializable_value = self.ensure_serializable(value)\n        value = json.dumps(value)\n        self.client.zadd(key, {value: score})\n        if expire:\n            self.zexpires[(key, score)] = expire + time.time()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n\"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting score of the range. Defaults to -1.\n            stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Returns:\n            list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n        \"\"\"\n        res = self.client.zrangebyscore(key, min, max, withscores=withscores)\n        if withscores:\n            return [(json.loads(x), y) for x, y in res]\n        else:\n            return [json.loads(x) for x in res]\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n    ):\n\"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Returns:\n            int: The number of members removed from the sorted set.\n        \"\"\"\n        return self.client.zremrangebyscore(key, min, max)\n\n    def ensure_serializable(self, value: Any):\n        if isinstance(value, dict):\n            for k, v in value.items():\n                if isinstance(v, Exception):\n                    value[k] = str(v)\n                elif inspect.isclass(v) and isinstance(v, InferenceResponseImage):\n                    value[k] = v.dict()\n        return value\n\n    def acquire_lock(self, key: str, expire=None) -&gt; Any:\n        l = self.client.lock(key, blocking=True, timeout=expire)\n        acquired = l.acquire(blocking_timeout=expire)\n        if not acquired:\n            raise TimeoutError(\"Couldn't get lock\")\n        # refresh the lock\n        if expire is not None:\n            l.extend(expire)\n        return l\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        serialized_value = pickle.dumps(value)\n        self.set(key, serialized_value, expire=expire)\n\n    def get_numpy(self, key: str) -&gt; Any:\n        serialized_value = self.get(key)\n        if serialized_value is not None:\n            return pickle.loads(serialized_value)\n        else:\n            return None\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.__init__","title":"<code>__init__(host='localhost', port=6379, db=0, ssl=False, timeout=2.0)</code>","text":"<p>Initializes a new instance of the MemoryCache class.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def __init__(\n    self,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    ssl: bool = False,\n    timeout: float = 2.0,\n) -&gt; None:\n\"\"\"\n    Initializes a new instance of the MemoryCache class.\n    \"\"\"\n    self.client = redis.Redis(\n        host=host,\n        port=port,\n        db=db,\n        decode_responses=False,\n        ssl=ssl,\n        socket_timeout=timeout,\n        socket_connect_timeout=timeout,\n    )\n    logger.debug(\"Attempting to diagnose Redis connection...\")\n    self.client.ping()\n    logger.debug(\"Redis connection established.\")\n    self.zexpires = dict()\n\n    self._expire_thread = threading.Thread(target=self._expire, daemon=True)\n    self._expire_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The value associated with the key, or None if the key does not exist or is expired.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def get(self, key: str):\n\"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Returns:\n        str: The value associated with the key, or None if the key does not exist or is expired.\n    \"\"\"\n    item = self.client.get(key)\n    if item is not None:\n        try:\n            return json.loads(item)\n        except (TypeError, ValueError):\n            return item\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n\"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    if not isinstance(value, bytes):\n        value = json.dumps(value)\n    self.client.set(key, value, ex=expire)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zadd(self, key: str, value: Any, score: float, expire: float = None):\n\"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    # serializable_value = self.ensure_serializable(value)\n    value = json.dumps(value)\n    self.client.zadd(key, {value: score})\n    if expire:\n        self.zexpires[(key, score)] = expire + time.time()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending score of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values (or value-score pairs if withscores is True) in the specified score range.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n\"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting score of the range. Defaults to -1.\n        stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Returns:\n        list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n    \"\"\"\n    res = self.client.zrangebyscore(key, min, max, withscores=withscores)\n    if withscores:\n        return [(json.loads(x), y) for x, y in res]\n    else:\n        return [json.loads(x) for x in res]\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zremrangebyscore","title":"<code>zremrangebyscore(key, min=-1, max=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of members removed from the sorted set.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n):\n\"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Returns:\n        int: The number of members removed from the sorted set.\n    \"\"\"\n    return self.client.zremrangebyscore(key, min, max)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/serializers/","title":"serializers","text":""},{"location":"docs/reference/inference/core/devices/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_cpu_id","title":"<code>get_cpu_id()</code>","text":"<p>Fetches the CPU ID based on the operating system.</p> <p>Attempts to get the CPU ID for Windows, Linux, and MacOS. In case of any error or an unsupported OS, returns None.</p> <p>Returns:</p> Type Description <p>Optional[str]: CPU ID string if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_cpu_id():\n\"\"\"Fetches the CPU ID based on the operating system.\n\n    Attempts to get the CPU ID for Windows, Linux, and MacOS.\n    In case of any error or an unsupported OS, returns None.\n\n    Returns:\n        Optional[str]: CPU ID string if available, None otherwise.\n    \"\"\"\n    try:\n        if platform.system() == \"Windows\":\n            return os.popen(\"wmic cpu get ProcessorId\").read().strip()\n        elif platform.system() == \"Linux\":\n            return (\n                open(\"/proc/cpuinfo\").read().split(\"processor\")[0].split(\":\")[1].strip()\n            )\n        elif platform.system() == \"Darwin\":\n            import subprocess\n\n            return (\n                subprocess.check_output([\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"])\n                .strip()\n                .decode()\n            )\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_device_hostname","title":"<code>get_device_hostname()</code>","text":"<p>Fetches the device's hostname.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The device's hostname.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_device_hostname():\n\"\"\"Fetches the device's hostname.\n\n    Returns:\n        str: The device's hostname.\n    \"\"\"\n    return platform.node()\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_gpu_id","title":"<code>get_gpu_id()</code>","text":"<p>Fetches the GPU ID if a GPU is present.</p> <p>Tries to import and use the <code>GPUtil</code> module to retrieve the GPU information.</p> <p>Returns:</p> Type Description <p>Optional[int]: GPU ID if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_gpu_id():\n\"\"\"Fetches the GPU ID if a GPU is present.\n\n    Tries to import and use the `GPUtil` module to retrieve the GPU information.\n\n    Returns:\n        Optional[int]: GPU ID if available, None otherwise.\n    \"\"\"\n    try:\n        import GPUtil\n\n        GPUs = GPUtil.getGPUs()\n        if GPUs:\n            return GPUs[0].id\n    except ImportError:\n        return None\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_inference_server_id","title":"<code>get_inference_server_id()</code>","text":"<p>Fetches a unique device ID.</p> <p>Tries to get the GPU ID first, then falls back to CPU ID. If the application is running inside Docker, the Docker container ID is appended to the hostname.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A unique string representing the device. If unable to determine, returns \"UNKNOWN\".</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_inference_server_id():\n\"\"\"Fetches a unique device ID.\n\n    Tries to get the GPU ID first, then falls back to CPU ID.\n    If the application is running inside Docker, the Docker container ID is appended to the hostname.\n\n    Returns:\n        str: A unique string representing the device. If unable to determine, returns \"UNKNOWN\".\n    \"\"\"\n    try:\n        if INFERENCE_SERVER_ID is not None:\n            return INFERENCE_SERVER_ID\n        id = random_string(6)\n        gpu_id = get_gpu_id()\n        if gpu_id is not None:\n            return f\"{id}-GPU-{gpu_id}\"\n        jetson_id = get_jetson_id()\n        if jetson_id is not None:\n            return f\"{id}-JETSON-{jetson_id}\"\n        return id\n    except Exception as e:\n        return \"UNKNOWN\"\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_jetson_id","title":"<code>get_jetson_id()</code>","text":"<p>Fetches the Jetson device's serial number.</p> <p>Attempts to read the serial number from the device tree. In case of any error, returns None.</p> <p>Returns:</p> Type Description <p>Optional[str]: Jetson device serial number if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_jetson_id():\n\"\"\"Fetches the Jetson device's serial number.\n\n    Attempts to read the serial number from the device tree.\n    In case of any error, returns None.\n\n    Returns:\n        Optional[str]: Jetson device serial number if available, None otherwise.\n    \"\"\"\n    try:\n        # Fetch the device's serial number\n        if not os.path.exists(\"/proc/device-tree/serial-number\"):\n            return None\n        serial_number = os.popen(\"cat /proc/device-tree/serial-number\").read().strip()\n        if serial_number == \"\":\n            return None\n        return serial_number\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.is_running_in_docker","title":"<code>is_running_in_docker()</code>","text":"<p>Checks if the current process is running inside a Docker container.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if running inside a Docker container, False otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def is_running_in_docker():\n\"\"\"Checks if the current process is running inside a Docker container.\n\n    Returns:\n        bool: True if running inside a Docker container, False otherwise.\n    \"\"\"\n    return os.path.exists(\"/.dockerenv\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/common/","title":"common","text":""},{"location":"docs/reference/inference/core/entities/types/","title":"types","text":""},{"location":"docs/reference/inference/core/entities/requests/clip/","title":"clip","text":""},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipCompareRequest","title":"<code>ClipCompareRequest</code>","text":"<p>             Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>subject</code> <code>Union[InferenceRequestImage, str]</code> <p>The type of image data provided, one of 'url' or 'base64'.</p> <code>subject_type</code> <code>str</code> <p>The type of subject, one of 'image' or 'text'.</p> <code>prompt</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]</code> <p>The prompt for comparison.</p> <code>prompt_type</code> <code>str</code> <p>The type of prompt, one of 'image' or 'text'.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipCompareRequest(ClipInferenceRequest):\n\"\"\"Request for CLIP comparison.\n\n    Attributes:\n        subject (Union[InferenceRequestImage, str]): The type of image data provided, one of 'url' or 'base64'.\n        subject_type (str): The type of subject, one of 'image' or 'text'.\n        prompt (Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]): The prompt for comparison.\n        prompt_type (str): The type of prompt, one of 'image' or 'text'.\n    \"\"\"\n\n    subject: Union[InferenceRequestImage, str] = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url' or 'base64'\",\n    )\n    subject_type: str = Field(\n        default=\"image\",\n        examples=[\"image\"],\n        description=\"The type of subject, one of 'image' or 'text'\",\n    )\n    prompt: Union[\n        List[InferenceRequestImage],\n        InferenceRequestImage,\n        str,\n        List[str],\n        Dict[str, Union[InferenceRequestImage, str]],\n    ]\n    prompt_type: str = Field(\n        default=\"text\",\n        examples=[\"text\"],\n        description=\"The type of prompt, one of 'image' or 'text'\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipImageEmbeddingRequest","title":"<code>ClipImageEmbeddingRequest</code>","text":"<p>             Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP image embedding.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) to be embedded.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipImageEmbeddingRequest(ClipInferenceRequest):\n\"\"\"Request for CLIP image embedding.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) to be embedded.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipInferenceRequest","title":"<code>ClipInferenceRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>Request for CLIP inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>clip_version_id</code> <code>Optional[str]</code> <p>The version ID of CLIP to be used for this request.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipInferenceRequest(BaseRequest):\n\"\"\"Request for CLIP inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        clip_version_id (Optional[str]): The version ID of CLIP to be used for this request.\n    \"\"\"\n\n    clip_version_id: Optional[str] = Field(\n        default=CLIP_VERSION_ID,\n        examples=[\"ViT-B-16\"],\n        description=\"The version ID of CLIP to be used for this request. Must be one of RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, ViT-B-32, ViT-L-14-336px, and ViT-L-14.\",\n    )\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"clip_version_id\") is None:\n            return None\n        return f\"clip/{values['clip_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipTextEmbeddingRequest","title":"<code>ClipTextEmbeddingRequest</code>","text":"<p>             Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP text embedding.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Union[List[str], str]</code> <p>A string or list of strings.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipTextEmbeddingRequest(ClipInferenceRequest):\n\"\"\"Request for CLIP text embedding.\n\n    Attributes:\n        text (Union[List[str], str]): A string or list of strings.\n    \"\"\"\n\n    text: Union[List[str], str] = Field(\n        examples=[\"The quick brown fox jumps over the lazy dog\"],\n        description=\"A string or list of strings\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/cogvlm/","title":"cogvlm","text":""},{"location":"docs/reference/inference/core/entities/requests/cogvlm/#inference.core.entities.requests.cogvlm.CogVLMInferenceRequest","title":"<code>CogVLMInferenceRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>Request for CogVLM inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>cog_version_id</code> <code>Optional[str]</code> <p>The version ID of CLIP to be used for this request.</p> Source code in <code>inference/core/entities/requests/cogvlm.py</code> <pre><code>class CogVLMInferenceRequest(BaseRequest):\n\"\"\"Request for CogVLM inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        cog_version_id (Optional[str]): The version ID of CLIP to be used for this request.\n    \"\"\"\n\n    cogvlm_version_id: Optional[str] = Field(\n        default=COGVLM_VERSION_ID,\n        examples=[\"cogvlm-chat-hf\"],\n        description=\"The version ID of CogVLM to be used for this request. See the huggingface model repo at THUDM.\",\n    )\n    model_id: Optional[str] = Field(None)\n    image: InferenceRequestImage = Field(\n        description=\"Image for CogVLM to look at. Use prompt to specify what you want it to do with the image.\"\n    )\n    prompt: str = Field(\n        description=\"Text to be passed to CogVLM. Use to prompt it to describe an image or provide only text to chat with the model.\",\n        examples=[\"Describe this image.\"],\n    )\n    history: Optional[List[Tuple[str, str]]] = Field(\n        None,\n        description=\"Optional chat history, formatted as a list of 2-tuples where the first entry is the user prompt\"\n        \" and the second entry is the generated model response\",\n    )\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"cogvlm_version_id\") is None:\n            return None\n        return f\"cogvlm/{values['cogvlm_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/doctr/","title":"doctr","text":""},{"location":"docs/reference/inference/core/entities/requests/doctr/#inference.core.entities.requests.doctr.DoctrOCRInferenceRequest","title":"<code>DoctrOCRInferenceRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>DocTR inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> Source code in <code>inference/core/entities/requests/doctr.py</code> <pre><code>class DoctrOCRInferenceRequest(BaseRequest):\n\"\"\"\n    DocTR inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    doctr_version_id: Optional[str] = \"default\"\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"doctr_version_id\") is None:\n            return None\n        return f\"doctr/{values['doctr_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/dynamic_class_base/","title":"dynamic_class_base","text":""},{"location":"docs/reference/inference/core/entities/requests/dynamic_class_base/#inference.core.entities.requests.dynamic_class_base.DynamicClassBaseInferenceRequest","title":"<code>DynamicClassBaseInferenceRequest</code>","text":"<p>             Bases: <code>CVInferenceRequest</code></p> <p>Request for zero-shot object detection models (with dynamic class lists).</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/dynamic_class_base.py</code> <pre><code>class DynamicClassBaseInferenceRequest(CVInferenceRequest):\n\"\"\"Request for zero-shot object detection models (with dynamic class lists).\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    model_id: Optional[str] = Field(None)\n    text: List[str] = Field(\n        examples=[[\"person\", \"dog\", \"cat\"]],\n        description=\"A list of strings\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/gaze/","title":"gaze","text":""},{"location":"docs/reference/inference/core/entities/requests/gaze/#inference.core.entities.requests.gaze.GazeDetectionInferenceRequest","title":"<code>GazeDetectionInferenceRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>Request for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>gaze_version_id</code> <code>Optional[str]</code> <p>The version ID of Gaze to be used for this request.</p> <code>do_run_face_detection</code> <code>Optional[bool]</code> <p>If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection.</p> <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> Source code in <code>inference/core/entities/requests/gaze.py</code> <pre><code>class GazeDetectionInferenceRequest(BaseRequest):\n\"\"\"Request for gaze detection inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        gaze_version_id (Optional[str]): The version ID of Gaze to be used for this request.\n        do_run_face_detection (Optional[bool]): If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection.\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n    \"\"\"\n\n    gaze_version_id: Optional[str] = Field(\n        default=GAZE_VERSION_ID,\n        examples=[\"l2cs\"],\n        description=\"The version ID of Gaze to be used for this request. Must be one of l2cs.\",\n    )\n\n    do_run_face_detection: Optional[bool] = Field(\n        default=True,\n        examples=[False],\n        description=\"If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection\",\n    )\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"gaze_version_id\") is None:\n            return None\n        return f\"gaze/{values['gaze_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/groundingdino/","title":"groundingdino","text":""},{"location":"docs/reference/inference/core/entities/requests/groundingdino/#inference.core.entities.requests.groundingdino.GroundingDINOInferenceRequest","title":"<code>GroundingDINOInferenceRequest</code>","text":"<p>             Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Grounding DINO zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/groundingdino.py</code> <pre><code>class GroundingDINOInferenceRequest(DynamicClassBaseInferenceRequest):\n\"\"\"Request for Grounding DINO zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    box_threshold: Optional[float] = 0.5\n    grounding_dino_version_id: Optional[str] = \"default\"\n    text_threshold: Optional[float] = 0.5\n    class_agnostic_nms: Optional[bool] = CLASS_AGNOSTIC_NMS\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/","title":"inference","text":""},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.BaseRequest","title":"<code>BaseRequest</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base request for inference.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str_</code> <p>A unique request identifier.</p> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key that will be passed to the model during initialization for artifact retrieval.</p> <code>start</code> <code>Optional[float]</code> <p>start time of request</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class BaseRequest(BaseModel):\n\"\"\"Base request for inference.\n\n    Attributes:\n        id (str_): A unique request identifier.\n        api_key (Optional[str]): Roboflow API Key that will be passed to the model during initialization for artifact retrieval.\n        start (Optional[float]): start time of request\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        kwargs[\"id\"] = str(uuid4())\n        super().__init__(**kwargs)\n\n    model_config = ConfigDict(protected_namespaces=())\n    id: str\n    api_key: Optional[str] = ApiKey\n    start: Optional[float] = None\n    source: Optional[str] = None\n    source_info: Optional[str] = None\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.CVInferenceRequest","title":"<code>CVInferenceRequest</code>","text":"<p>             Bases: <code>InferenceRequest</code></p> <p>Computer Vision inference request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> <code>disable_preproc_auto_orient</code> <code>Optional[bool]</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_contrast</code> <code>Optional[bool]</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_grayscale</code> <code>Optional[bool]</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_static_crop</code> <code>Optional[bool]</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class CVInferenceRequest(InferenceRequest):\n\"\"\"Computer Vision inference request.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n        disable_preproc_auto_orient (Optional[bool]): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (Optional[bool]): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (Optional[bool]): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (Optional[bool]): If true, the static crop preprocessing step is disabled for this call. Default is False.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    disable_preproc_auto_orient: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the auto orient preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_contrast: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the auto contrast preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_grayscale: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the grayscale preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_static_crop: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the static crop preprocessing step is disabled for this call.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.ClassificationInferenceRequest","title":"<code>ClassificationInferenceRequest</code>","text":"<p>             Bases: <code>CVInferenceRequest</code></p> <p>Classification inference request.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class ClassificationInferenceRequest(CVInferenceRequest):\n\"\"\"Classification inference request.\n\n    Attributes:\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    confidence: Optional[float] = Field(\n        default=0.4,\n        examples=[0.5],\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n    disable_active_learning: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n    )\n    active_learning_target_dataset: Optional[str] = Field(\n        default=None,\n        examples=[\"my_dataset\"],\n        description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InferenceRequest","title":"<code>InferenceRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>Base request for inference.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> <code>model_type</code> <code>Optional[str]</code> <p>The type of the model, usually referring to what task the model performs.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InferenceRequest(BaseRequest):\n\"\"\"Base request for inference.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n        model_type (Optional[str]): The type of the model, usually referring to what task the model performs.\n    \"\"\"\n\n    model_id: Optional[str] = ModelID\n    model_type: Optional[str] = ModelType\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InferenceRequestImage","title":"<code>InferenceRequestImage</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Image data for inference request.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of image data provided, one of 'url', 'base64', or 'numpy'.</p> <code>value</code> <code>Optional[Any]</code> <p>Image data corresponding to the image type.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InferenceRequestImage(BaseModel):\n\"\"\"Image data for inference request.\n\n    Attributes:\n        type (str): The type of image data provided, one of 'url', 'base64', or 'numpy'.\n        value (Optional[Any]): Image data corresponding to the image type.\n    \"\"\"\n\n    type: str = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url', 'base64', or 'numpy'\",\n    )\n    value: Optional[Any] = Field(\n        None,\n        examples=[\"http://www.example-image-url.com\"],\n        description=\"Image data corresponding to the image type, if type = 'url' then value is a string containing the url of an image, else if type = 'base64' then value is a string containing base64 encoded image data, else if type = 'numpy' then value is binary numpy data serialized using pickle.dumps(); array should 3 dimensions, channels last, with values in the range [0,255].\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InstanceSegmentationInferenceRequest","title":"<code>InstanceSegmentationInferenceRequest</code>","text":"<p>             Bases: <code>ObjectDetectionInferenceRequest</code></p> <p>Instance Segmentation inference request.</p> <p>Attributes:</p> Name Type Description <code>mask_decode_mode</code> <code>Optional[str]</code> <p>The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.</p> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>The amount to tradeoff between 0='fast' and 1='accurate'.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InstanceSegmentationInferenceRequest(ObjectDetectionInferenceRequest):\n\"\"\"Instance Segmentation inference request.\n\n    Attributes:\n        mask_decode_mode (Optional[str]): The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.\n        tradeoff_factor (Optional[float]): The amount to tradeoff between 0='fast' and 1='accurate'.\n    \"\"\"\n\n    mask_decode_mode: Optional[str] = Field(\n        default=\"accurate\",\n        examples=[\"accurate\"],\n        description=\"The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'\",\n    )\n    tradeoff_factor: Optional[float] = Field(\n        default=0.0,\n        examples=[0.5],\n        description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.ObjectDetectionInferenceRequest","title":"<code>ObjectDetectionInferenceRequest</code>","text":"<p>             Bases: <code>CVInferenceRequest</code></p> <p>Object Detection inference request.</p> <p>Attributes:</p> Name Type Description <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>If true, NMS is applied to all detections at once, if false, NMS is applied per class.</p> <code>class_filter</code> <code>Optional[List[str]]</code> <p>If provided, only predictions for the listed classes will be returned.</p> <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>fix_batch_size</code> <code>Optional[bool]</code> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p> <code>iou_threshold</code> <code>Optional[float]</code> <p>The IoU threshold that must be met for a box pair to be considered duplicate during NMS.</p> <code>max_detections</code> <code>Optional[int]</code> <p>The maximum number of detections that will be returned.</p> <code>max_candidates</code> <code>Optional[int]</code> <p>The maximum number of candidate detections passed to NMS.</p> <code>visualization_labels</code> <code>Optional[bool]</code> <p>If true, labels will be rendered on prediction visualizations.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class ObjectDetectionInferenceRequest(CVInferenceRequest):\n\"\"\"Object Detection inference request.\n\n    Attributes:\n        class_agnostic_nms (Optional[bool]): If true, NMS is applied to all detections at once, if false, NMS is applied per class.\n        class_filter (Optional[List[str]]): If provided, only predictions for the listed classes will be returned.\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        fix_batch_size (Optional[bool]): If true, the batch size will be fixed to the maximum batch size configured for this server.\n        iou_threshold (Optional[float]): The IoU threshold that must be met for a box pair to be considered duplicate during NMS.\n        max_detections (Optional[int]): The maximum number of detections that will be returned.\n        max_candidates (Optional[int]): The maximum number of candidate detections passed to NMS.\n        visualization_labels (Optional[bool]): If true, labels will be rendered on prediction visualizations.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    class_agnostic_nms: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, NMS is applied to all detections at once, if false, NMS is applied per class\",\n    )\n    class_filter: Optional[List[str]] = Field(\n        default=None,\n        examples=[[\"class-1\", \"class-2\", \"class-n\"]],\n        description=\"If provided, only predictions for the listed classes will be returned\",\n    )\n    confidence: Optional[float] = Field(\n        default=0.4,\n        examples=[0.5],\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    fix_batch_size: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the batch size will be fixed to the maximum batch size configured for this server\",\n    )\n    iou_threshold: Optional[float] = Field(\n        default=0.3,\n        examples=[0.5],\n        description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n    )\n    max_detections: Optional[int] = Field(\n        default=300,\n        examples=[300],\n        description=\"The maximum number of detections that will be returned\",\n    )\n    max_candidates: Optional[int] = Field(\n        default=3000,\n        description=\"The maximum number of candidate detections passed to NMS\",\n    )\n    visualization_labels: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, labels will be rendered on prediction visualizations\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n    disable_active_learning: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n    )\n    active_learning_target_dataset: Optional[str] = Field(\n        default=None,\n        examples=[\"my_dataset\"],\n        description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.request_from_type","title":"<code>request_from_type(model_type, request_dict)</code>","text":"<p>Uses original request id</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>def request_from_type(model_type, request_dict):\n\"\"\"Uses original request id\"\"\"\n    if model_type == \"classification\":\n        request = ClassificationInferenceRequest(**request_dict)\n    elif model_type == \"instance-segmentation\":\n        request = InstanceSegmentationInferenceRequest(**request_dict)\n    elif model_type == \"object-detection\":\n        request = ObjectDetectionInferenceRequest(**request_dict)\n    else:\n        raise ValueError(f\"Uknown task type {model_type}\")\n    request.id = request_dict.get(\"id\")\n    return request\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam/","title":"sam","text":""},{"location":"docs/reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamEmbeddingRequest","title":"<code>SamEmbeddingRequest</code>","text":"<p>             Bases: <code>SamInferenceRequest</code></p> <p>SAM embedding request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be embedded.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be embedded used to cache the embedding.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response. Must be one of json or binary.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamEmbeddingRequest(SamInferenceRequest):\n\"\"\"SAM embedding request.\n\n    Attributes:\n        image (Optional[inference.core.entities.requests.inference.InferenceRequestImage]): The image to be embedded.\n        image_id (Optional[str]): The ID of the image to be embedded used to cache the embedding.\n        format (Optional[str]): The format of the response. Must be one of json or binary.\n    \"\"\"\n\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be embedded\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be embedded used to cache the embedding.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, embedding is returned as a binary numpy array.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamInferenceRequest","title":"<code>SamInferenceRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>SAM inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>sam_version_id</code> <code>Optional[str]</code> <p>The version ID of SAM to be used for this request.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamInferenceRequest(BaseRequest):\n\"\"\"SAM inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        sam_version_id (Optional[str]): The version ID of SAM to be used for this request.\n    \"\"\"\n\n    sam_version_id: Optional[str] = Field(\n        default=SAM_VERSION_ID,\n        examples=[\"vit_h\"],\n        description=\"The version ID of SAM to be used for this request. Must be one of vit_h, vit_l, or vit_b.\",\n    )\n\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"sam_version_id\") is None:\n            return None\n        return f\"sam/{values['sam_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamSegmentationRequest","title":"<code>SamSegmentationRequest</code>","text":"<p>             Bases: <code>SamInferenceRequest</code></p> <p>SAM segmentation request.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Optional[Union[List[List[List[List[float]]]], Any]]</code> <p>The embeddings to be decoded.</p> <code>embeddings_format</code> <code>Optional[str]</code> <p>The format of the embeddings.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response.</p> <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be segmented.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be segmented used to retrieve cached embeddings.</p> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Whether or not the request includes a mask input.</p> <code>mask_input</code> <code>Optional[Union[List[List[List[float]]], Any]]</code> <p>The set of output masks.</p> <code>mask_input_format</code> <code>Optional[str]</code> <p>The format of the mask input.</p> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>The original size of the image used to generate the embeddings.</p> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>The coordinates of the interactive points used during decoding.</p> <code>point_labels</code> <code>Optional[List[float]]</code> <p>The labels of the interactive points used during decoding.</p> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Whether or not to use the mask input cache.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamSegmentationRequest(SamInferenceRequest):\n\"\"\"SAM segmentation request.\n\n    Attributes:\n        embeddings (Optional[Union[List[List[List[List[float]]]], Any]]): The embeddings to be decoded.\n        embeddings_format (Optional[str]): The format of the embeddings.\n        format (Optional[str]): The format of the response.\n        image (Optional[InferenceRequestImage]): The image to be segmented.\n        image_id (Optional[str]): The ID of the image to be segmented used to retrieve cached embeddings.\n        has_mask_input (Optional[bool]): Whether or not the request includes a mask input.\n        mask_input (Optional[Union[List[List[List[float]]], Any]]): The set of output masks.\n        mask_input_format (Optional[str]): The format of the mask input.\n        orig_im_size (Optional[List[int]]): The original size of the image used to generate the embeddings.\n        point_coords (Optional[List[List[float]]]): The coordinates of the interactive points used during decoding.\n        point_labels (Optional[List[float]]): The labels of the interactive points used during decoding.\n        use_mask_input_cache (Optional[bool]): Whether or not to use the mask input cache.\n    \"\"\"\n\n    embeddings: Optional[Union[List[List[List[List[float]]]], Any]] = Field(\n        None,\n        examples=[\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\"],\n        description=\"The embeddings to be decoded. The dimensions of the embeddings are 1 x 256 x 64 x 64. If embeddings is not provided, image must be provided.\",\n    )\n    embeddings_format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the embeddings. Must be one of json or binary. If binary, embeddings are expected to be a binary numpy array.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, masks are returned as binary numpy arrays. If json, masks are converted to polygons, then returned as json.\",\n    )\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be segmented. Only required if embeddings are not provided.\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be segmented used to retrieve cached embeddings. If an embedding is cached, it will be used instead of generating a new embedding. If no embedding is cached, a new embedding will be generated and cached.\",\n    )\n    has_mask_input: Optional[bool] = Field(\n        default=False,\n        examples=[True],\n        description=\"Whether or not the request includes a mask input. If true, the mask input must be provided.\",\n    )\n    mask_input: Optional[Union[List[List[List[float]]], Any]] = Field(\n        default=None,\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256. This is the same as the output, low resolution mask from the previous inference.\",\n    )\n    mask_input_format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the mask input. Must be one of json or binary. If binary, mask input is expected to be a binary numpy array.\",\n    )\n    orig_im_size: Optional[List[int]] = Field(\n        default=None,\n        examples=[[640, 320]],\n        description=\"The original size of the image used to generate the embeddings. This is only required if the image is not provided.\",\n    )\n    point_coords: Optional[List[List[float]]] = Field(\n        default=[[0.0, 0.0]],\n        examples=[[[10.0, 10.0]]],\n        description=\"The coordinates of the interactive points used during decoding. Each point (x,y pair) corresponds to a label in point_labels.\",\n    )\n    point_labels: Optional[List[float]] = Field(\n        default=[-1],\n        examples=[[1]],\n        description=\"The labels of the interactive points used during decoding. A 1 represents a positive point (part of the object to be segmented). A -1 represents a negative point (not part of the object to be segmented). Each label corresponds to a point in point_coords.\",\n    )\n    use_mask_input_cache: Optional[bool] = Field(\n        default=True,\n        examples=[True],\n        description=\"Whether or not to use the mask input cache. If true, the mask input cache will be used if it exists. If false, the mask input cache will not be used.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/server_state/","title":"server_state","text":""},{"location":"docs/reference/inference/core/entities/requests/server_state/#inference.core.entities.requests.server_state.AddModelRequest","title":"<code>AddModelRequest</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request to add a model to the inference server.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> <code>model_type</code> <code>Optional[str]</code> <p>The type of the model, usually referring to what task the model performs.</p> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key that will be passed to the model during initialization for artifact retrieval.</p> Source code in <code>inference/core/entities/requests/server_state.py</code> <pre><code>class AddModelRequest(BaseModel):\n\"\"\"Request to add a model to the inference server.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n        model_type (Optional[str]): The type of the model, usually referring to what task the model performs.\n        api_key (Optional[str]): Roboflow API Key that will be passed to the model during initialization for artifact retrieval.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    model_id: str = ModelID\n    model_type: Optional[str] = ModelType\n    api_key: Optional[str] = ApiKey\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/server_state/#inference.core.entities.requests.server_state.ClearModelRequest","title":"<code>ClearModelRequest</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request to clear a model from the inference server.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> Source code in <code>inference/core/entities/requests/server_state.py</code> <pre><code>class ClearModelRequest(BaseModel):\n\"\"\"Request to clear a model from the inference server.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    model_id: str = ModelID\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/entities/requests/yolo_world/","title":"yolo_world","text":""},{"location":"docs/reference/inference/core/entities/requests/yolo_world/#inference.core.entities.requests.yolo_world.YOLOWorldInferenceRequest","title":"<code>YOLOWorldInferenceRequest</code>","text":"<p>             Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Grounding DINO zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/yolo_world.py</code> <pre><code>class YOLOWorldInferenceRequest(DynamicClassBaseInferenceRequest):\n\"\"\"Request for Grounding DINO zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    yolo_world_version_id: Optional[str] = \"l\"\n    confidence: Optional[float] = DEFAULT_CONFIDENCE\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/clip/","title":"clip","text":""},{"location":"docs/reference/inference/core/entities/responses/clip/#inference.core.entities.responses.clip.ClipCompareResponse","title":"<code>ClipCompareResponse</code>","text":"<p>             Bases: <code>InferenceResponse</code></p> <p>Response for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>similarity</code> <code>Union[List[float], Dict[str, float]]</code> <p>Similarity scores.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the similarity scores including preprocessing.</p> Source code in <code>inference/core/entities/responses/clip.py</code> <pre><code>class ClipCompareResponse(InferenceResponse):\n\"\"\"Response for CLIP comparison.\n\n    Attributes:\n        similarity (Union[List[float], Dict[str, float]]): Similarity scores.\n        time (float): The time in seconds it took to produce the similarity scores including preprocessing.\n    \"\"\"\n\n    similarity: Union[List[float], Dict[str, float]]\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the similarity scores including preprocessing\",\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/clip/#inference.core.entities.responses.clip.ClipEmbeddingResponse","title":"<code>ClipEmbeddingResponse</code>","text":"<p>             Bases: <code>InferenceResponse</code></p> <p>Response for CLIP embedding.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>List[List[float]]</code> <p>A list of embeddings, each embedding is a list of floats.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/clip.py</code> <pre><code>class ClipEmbeddingResponse(InferenceResponse):\n\"\"\"Response for CLIP embedding.\n\n    Attributes:\n        embeddings (List[List[float]]): A list of embeddings, each embedding is a list of floats.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: List[List[float]] = Field(\n        examples=[\"[[0.12, 0.23, 0.34, ..., 0.43]]\"],\n        description=\"A list of embeddings, each embedding is a list of floats\",\n    )\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/cogvlm/","title":"cogvlm","text":""},{"location":"docs/reference/inference/core/entities/responses/doctr/","title":"doctr","text":""},{"location":"docs/reference/inference/core/entities/responses/doctr/#inference.core.entities.responses.doctr.DoctrOCRInferenceResponse","title":"<code>DoctrOCRInferenceResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>DocTR Inference response.</p> <p>Attributes:</p> Name Type Description <code>result</code> <code>str</code> <p>The result from OCR.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the segmentation including preprocessing.</p> Source code in <code>inference/core/entities/responses/doctr.py</code> <pre><code>class DoctrOCRInferenceResponse(BaseModel):\n\"\"\"\n    DocTR Inference response.\n\n    Attributes:\n        result (str): The result from OCR.\n        time: The time in seconds it took to produce the segmentation including preprocessing.\n    \"\"\"\n\n    result: str = Field(description=\"The result from OCR.\")\n    time: float = Field(\n        description=\"The time in seconds it took to produce the segmentation including preprocessing.\"\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/gaze/","title":"gaze","text":""},{"location":"docs/reference/inference/core/entities/responses/gaze/#inference.core.entities.responses.gaze.GazeDetectionInferenceResponse","title":"<code>GazeDetectionInferenceResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[GazeDetectionPrediction]</code> <p>List of gaze detection predictions.</p> <code>time</code> <code>float</code> <p>The processing time (second).</p> Source code in <code>inference/core/entities/responses/gaze.py</code> <pre><code>class GazeDetectionInferenceResponse(BaseModel):\n\"\"\"Response for gaze detection inference.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.gaze.GazeDetectionPrediction]): List of gaze detection predictions.\n        time (float): The processing time (second).\n    \"\"\"\n\n    predictions: List[GazeDetectionPrediction]\n\n    time: float = Field(description=\"The processing time (second)\")\n    time_face_det: Optional[float] = Field(\n        None, description=\"The face detection time (second)\"\n    )\n    time_gaze_det: Optional[float] = Field(\n        None, description=\"The gaze detection time (second)\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/gaze/#inference.core.entities.responses.gaze.GazeDetectionPrediction","title":"<code>GazeDetectionPrediction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Gaze Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>face</code> <code>FaceDetectionPrediction</code> <p>The face prediction.</p> <code>yaw</code> <code>float</code> <p>Yaw (radian) of the detected face.</p> <code>pitch</code> <code>float</code> <p>Pitch (radian) of the detected face.</p> Source code in <code>inference/core/entities/responses/gaze.py</code> <pre><code>class GazeDetectionPrediction(BaseModel):\n\"\"\"Gaze Detection prediction.\n\n    Attributes:\n        face (inference.core.entities.responses.inference.FaceDetectionPrediction): The face prediction.\n        yaw (float): Yaw (radian) of the detected face.\n        pitch (float): Pitch (radian) of the detected face.\n    \"\"\"\n\n    face: FaceDetectionPrediction\n\n    yaw: float = Field(description=\"Yaw (radian) of the detected face\")\n    pitch: float = Field(description=\"Pitch (radian) of the detected face\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/groundingdino/","title":"groundingdino","text":""},{"location":"docs/reference/inference/core/entities/responses/inference/","title":"inference","text":""},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ClassificationInferenceResponse","title":"<code>ClassificationInferenceResponse</code>","text":"<p>             Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Classification inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[ClassificationPrediction]</code> <p>List of classification predictions.</p> <code>top</code> <code>str</code> <p>The top predicted class label.</p> <code>confidence</code> <code>float</code> <p>The confidence of the top predicted class label.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ClassificationInferenceResponse(CvInferenceResponse, WithVisualizationResponse):\n\"\"\"Classification inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.ClassificationPrediction]): List of classification predictions.\n        top (str): The top predicted class label.\n        confidence (float): The confidence of the top predicted class label.\n    \"\"\"\n\n    predictions: List[ClassificationPrediction]\n    top: str = Field(description=\"The top predicted class label\")\n    confidence: float = Field(\n        description=\"The confidence of the top predicted class label\"\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ClassificationPrediction","title":"<code>ClassificationPrediction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Classification prediction.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_id</code> <code>int</code> <p>Numeric ID associated with the class label.</p> <code>confidence</code> <code>float</code> <p>The class label confidence as a fraction between 0 and 1.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ClassificationPrediction(BaseModel):\n\"\"\"Classification prediction.\n\n    Attributes:\n        class_name (str): The predicted class label.\n        class_id (int): Numeric ID associated with the class label.\n        confidence (float): The class label confidence as a fraction between 0 and 1.\n    \"\"\"\n\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n    class_id: int = Field(description=\"Numeric ID associated with the class label\")\n    confidence: float = Field(\n        description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.CvInferenceResponse","title":"<code>CvInferenceResponse</code>","text":"<p>             Bases: <code>InferenceResponse</code></p> <p>Computer Vision inference response.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceResponseImage], InferenceResponseImage]</code> <p>Image(s) used in inference.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class CvInferenceResponse(InferenceResponse):\n\"\"\"Computer Vision inference response.\n\n    Attributes:\n        image (Union[List[inference.core.entities.responses.inference.InferenceResponseImage], inference.core.entities.responses.inference.InferenceResponseImage]): Image(s) used in inference.\n    \"\"\"\n\n    image: Union[List[InferenceResponseImage], InferenceResponseImage]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.FaceDetectionPrediction","title":"<code>FaceDetectionPrediction</code>","text":"<p>             Bases: <code>ObjectDetectionPrediction</code></p> <p>Face Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>fixed value \"face\".</p> <code>landmarks</code> <code>Union[List[Point], List[Point3D]]</code> <p>The detected face landmarks.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class FaceDetectionPrediction(ObjectDetectionPrediction):\n\"\"\"Face Detection prediction.\n\n    Attributes:\n        class_name (str): fixed value \"face\".\n        landmarks (Union[List[inference.core.entities.responses.inference.Point], List[inference.core.entities.responses.inference.Point3D]]): The detected face landmarks.\n    \"\"\"\n\n    class_id: Optional[int] = Field(\n        description=\"The class id of the prediction\", default=0\n    )\n    class_name: str = Field(\n        alias=\"class\", default=\"face\", description=\"The predicted class label\"\n    )\n    landmarks: Union[List[Point], List[Point3D]]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InferenceResponse","title":"<code>InferenceResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base inference response.</p> <p>Attributes:</p> Name Type Description <code>frame_id</code> <code>Optional[int]</code> <p>The frame id of the image used in inference if the input was a video.</p> <code>time</code> <code>Optional[float]</code> <p>The time in seconds it took to produce the predictions including image preprocessing.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InferenceResponse(BaseModel):\n\"\"\"Base inference response.\n\n    Attributes:\n        frame_id (Optional[int]): The frame id of the image used in inference if the input was a video.\n        time (Optional[float]): The time in seconds it took to produce the predictions including image preprocessing.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    frame_id: Optional[int] = Field(\n        default=None,\n        description=\"The frame id of the image used in inference if the input was a video\",\n    )\n    time: Optional[float] = Field(\n        default=None,\n        description=\"The time in seconds it took to produce the predictions including image preprocessing\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InferenceResponseImage","title":"<code>InferenceResponseImage</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Inference response image information.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>The original width of the image used in inference.</p> <code>height</code> <code>int</code> <p>The original height of the image used in inference.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InferenceResponseImage(BaseModel):\n\"\"\"Inference response image information.\n\n    Attributes:\n        width (int): The original width of the image used in inference.\n        height (int): The original height of the image used in inference.\n    \"\"\"\n\n    width: int = Field(description=\"The original width of the image used in inference\")\n    height: int = Field(\n        description=\"The original height of the image used in inference\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InstanceSegmentationInferenceResponse","title":"<code>InstanceSegmentationInferenceResponse</code>","text":"<p>             Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Instance Segmentation inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[InstanceSegmentationPrediction]</code> <p>List of instance segmentation predictions.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InstanceSegmentationInferenceResponse(\n    CvInferenceResponse, WithVisualizationResponse\n):\n\"\"\"Instance Segmentation inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.InstanceSegmentationPrediction]): List of instance segmentation predictions.\n    \"\"\"\n\n    predictions: List[InstanceSegmentationPrediction]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InstanceSegmentationPrediction","title":"<code>InstanceSegmentationPrediction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Instance Segmentation prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The center x-axis pixel coordinate of the prediction.</p> <code>y</code> <code>float</code> <p>The center y-axis pixel coordinate of the prediction.</p> <code>width</code> <code>float</code> <p>The width of the prediction bounding box in number of pixels.</p> <code>height</code> <code>float</code> <p>The height of the prediction bounding box in number of pixels.</p> <code>confidence</code> <code>float</code> <p>The detection confidence as a fraction between 0 and 1.</p> <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_confidence</code> <code>Union[float, None]</code> <p>The class label confidence as a fraction between 0 and 1.</p> <code>points</code> <code>List[Point]</code> <p>The list of points that make up the instance polygon.</p> <code>class_id</code> <code>int</code> <p>int = Field(description=\"The class id of the prediction\")</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InstanceSegmentationPrediction(BaseModel):\n\"\"\"Instance Segmentation prediction.\n\n    Attributes:\n        x (float): The center x-axis pixel coordinate of the prediction.\n        y (float): The center y-axis pixel coordinate of the prediction.\n        width (float): The width of the prediction bounding box in number of pixels.\n        height (float): The height of the prediction bounding box in number of pixels.\n        confidence (float): The detection confidence as a fraction between 0 and 1.\n        class_name (str): The predicted class label.\n        class_confidence (Union[float, None]): The class label confidence as a fraction between 0 and 1.\n        points (List[Point]): The list of points that make up the instance polygon.\n        class_id: int = Field(description=\"The class id of the prediction\")\n    \"\"\"\n\n    x: float = Field(description=\"The center x-axis pixel coordinate of the prediction\")\n    y: float = Field(description=\"The center y-axis pixel coordinate of the prediction\")\n    width: float = Field(\n        description=\"The width of the prediction bounding box in number of pixels\"\n    )\n    height: float = Field(\n        description=\"The height of the prediction bounding box in number of pixels\"\n    )\n    confidence: float = Field(\n        description=\"The detection confidence as a fraction between 0 and 1\"\n    )\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n\n    class_confidence: Union[float, None] = Field(\n        None, description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    points: List[Point] = Field(\n        description=\"The list of points that make up the instance polygon\"\n    )\n    class_id: int = Field(description=\"The class id of the prediction\")\n    detection_id: str = Field(\n        description=\"Unique identifier of detection\",\n        default_factory=lambda: str(uuid4()),\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.MultiLabelClassificationInferenceResponse","title":"<code>MultiLabelClassificationInferenceResponse</code>","text":"<p>             Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Multi-label Classification inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>Dict[str, MultiLabelClassificationPrediction]</code> <p>Dictionary of multi-label classification predictions.</p> <code>predicted_classes</code> <code>List[str]</code> <p>The list of predicted classes.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class MultiLabelClassificationInferenceResponse(\n    CvInferenceResponse, WithVisualizationResponse\n):\n\"\"\"Multi-label Classification inference response.\n\n    Attributes:\n        predictions (Dict[str, inference.core.entities.responses.inference.MultiLabelClassificationPrediction]): Dictionary of multi-label classification predictions.\n        predicted_classes (List[str]): The list of predicted classes.\n    \"\"\"\n\n    predictions: Dict[str, MultiLabelClassificationPrediction]\n    predicted_classes: List[str] = Field(description=\"The list of predicted classes\")\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.MultiLabelClassificationPrediction","title":"<code>MultiLabelClassificationPrediction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Multi-label Classification prediction.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>float</code> <p>The class label confidence as a fraction between 0 and 1.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class MultiLabelClassificationPrediction(BaseModel):\n\"\"\"Multi-label Classification prediction.\n\n    Attributes:\n        confidence (float): The class label confidence as a fraction between 0 and 1.\n    \"\"\"\n\n    confidence: float = Field(\n        description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ObjectDetectionInferenceResponse","title":"<code>ObjectDetectionInferenceResponse</code>","text":"<p>             Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Object Detection inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[ObjectDetectionPrediction]</code> <p>List of object detection predictions.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ObjectDetectionInferenceResponse(CvInferenceResponse, WithVisualizationResponse):\n\"\"\"Object Detection inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.ObjectDetectionPrediction]): List of object detection predictions.\n    \"\"\"\n\n    predictions: List[ObjectDetectionPrediction]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ObjectDetectionPrediction","title":"<code>ObjectDetectionPrediction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Object Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The center x-axis pixel coordinate of the prediction.</p> <code>y</code> <code>float</code> <p>The center y-axis pixel coordinate of the prediction.</p> <code>width</code> <code>float</code> <p>The width of the prediction bounding box in number of pixels.</p> <code>height</code> <code>float</code> <p>The height of the prediction bounding box in number of pixels.</p> <code>confidence</code> <code>float</code> <p>The detection confidence as a fraction between 0 and 1.</p> <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_confidence</code> <code>Union[float, None]</code> <p>The class label confidence as a fraction between 0 and 1.</p> <code>class_id</code> <code>int</code> <p>The class id of the prediction</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ObjectDetectionPrediction(BaseModel):\n\"\"\"Object Detection prediction.\n\n    Attributes:\n        x (float): The center x-axis pixel coordinate of the prediction.\n        y (float): The center y-axis pixel coordinate of the prediction.\n        width (float): The width of the prediction bounding box in number of pixels.\n        height (float): The height of the prediction bounding box in number of pixels.\n        confidence (float): The detection confidence as a fraction between 0 and 1.\n        class_name (str): The predicted class label.\n        class_confidence (Union[float, None]): The class label confidence as a fraction between 0 and 1.\n        class_id (int): The class id of the prediction\n    \"\"\"\n\n    x: float = Field(description=\"The center x-axis pixel coordinate of the prediction\")\n    y: float = Field(description=\"The center y-axis pixel coordinate of the prediction\")\n    width: float = Field(\n        description=\"The width of the prediction bounding box in number of pixels\"\n    )\n    height: float = Field(\n        description=\"The height of the prediction bounding box in number of pixels\"\n    )\n    confidence: float = Field(\n        description=\"The detection confidence as a fraction between 0 and 1\"\n    )\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n\n    class_confidence: Union[float, None] = Field(\n        None, description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    class_id: int = Field(description=\"The class id of the prediction\")\n    tracker_id: Optional[int] = Field(\n        description=\"The tracker id of the prediction if tracking is enabled\",\n        default=None,\n    )\n    detection_id: str = Field(\n        description=\"Unique identifier of detection\",\n        default_factory=lambda: str(uuid4()),\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.Point","title":"<code>Point</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Point coordinates.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The x-axis pixel coordinate of the point.</p> <code>y</code> <code>float</code> <p>The y-axis pixel coordinate of the point.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class Point(BaseModel):\n\"\"\"Point coordinates.\n\n    Attributes:\n        x (float): The x-axis pixel coordinate of the point.\n        y (float): The y-axis pixel coordinate of the point.\n    \"\"\"\n\n    x: float = Field(description=\"The x-axis pixel coordinate of the point\")\n    y: float = Field(description=\"The y-axis pixel coordinate of the point\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.Point3D","title":"<code>Point3D</code>","text":"<p>             Bases: <code>Point</code></p> <p>3D Point coordinates.</p> <p>Attributes:</p> Name Type Description <code>z</code> <code>float</code> <p>The z-axis pixel coordinate of the point.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class Point3D(Point):\n\"\"\"3D Point coordinates.\n\n    Attributes:\n        z (float): The z-axis pixel coordinate of the point.\n    \"\"\"\n\n    z: float = Field(description=\"The z-axis pixel coordinate of the point\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.WithVisualizationResponse","title":"<code>WithVisualizationResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response with visualization.</p> <p>Attributes:</p> Name Type Description <code>visualization</code> <code>Optional[Any]</code> <p>Base64 encoded string containing prediction visualization image data.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class WithVisualizationResponse(BaseModel):\n\"\"\"Response with visualization.\n\n    Attributes:\n        visualization (Optional[Any]): Base64 encoded string containing prediction visualization image data.\n    \"\"\"\n\n    visualization: Optional[Any] = Field(\n        default=None,\n        description=\"Base64 encoded string containing prediction visualization image data\",\n    )\n\n    @field_serializer(\"visualization\", when_used=\"json\")\n    def serialize_visualisation(self, visualization: Optional[Any]) -&gt; Optional[str]:\n        if visualization is None:\n            return None\n        return base64.b64encode(visualization).decode(\"utf-8\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/notebooks/","title":"notebooks","text":""},{"location":"docs/reference/inference/core/entities/responses/notebooks/#inference.core.entities.responses.notebooks.NotebookStartResponse","title":"<code>NotebookStartResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response model for notebook start request</p> Source code in <code>inference/core/entities/responses/notebooks.py</code> <pre><code>class NotebookStartResponse(BaseModel):\n\"\"\"Response model for notebook start request\"\"\"\n\n    success: str = Field(..., description=\"Status of the request\")\n    message: str = Field(..., description=\"Message of the request\", optional=True)\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/sam/","title":"sam","text":""},{"location":"docs/reference/inference/core/entities/responses/sam/#inference.core.entities.responses.sam.SamEmbeddingResponse","title":"<code>SamEmbeddingResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>SAM embedding response.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Union[List[List[List[List[float]]]], Any]</code> <p>The SAM embedding.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam.py</code> <pre><code>class SamEmbeddingResponse(BaseModel):\n\"\"\"SAM embedding response.\n\n    Attributes:\n        embeddings (Union[List[List[List[List[float]]]], Any]): The SAM embedding.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: Union[List[List[List[List[float]]]], Any] = Field(\n        examples=[\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\"],\n        description=\"If request format is json, embeddings is a series of nested lists representing the SAM embedding. If request format is binary, embeddings is a binary numpy array. The dimensions of the embedding are 1 x 256 x 64 x 64.\",\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/sam/#inference.core.entities.responses.sam.SamSegmentationResponse","title":"<code>SamSegmentationResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>SAM segmentation response.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output masks.</p> <code>low_res_masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output low-resolution masks.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the segmentation including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam.py</code> <pre><code>class SamSegmentationResponse(BaseModel):\n\"\"\"SAM segmentation response.\n\n    Attributes:\n        masks (Union[List[List[List[int]]], Any]): The set of output masks.\n        low_res_masks (Union[List[List[List[int]]], Any]): The set of output low-resolution masks.\n        time (float): The time in seconds it took to produce the segmentation including preprocessing.\n    \"\"\"\n\n    masks: Union[List[List[List[int]]], Any] = Field(\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are the same as the dimensions of the input image.\",\n    )\n    low_res_masks: Union[List[List[List[int]]], Any] = Field(\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256\",\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the segmentation including preprocessing\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/server_state/","title":"server_state","text":""},{"location":"docs/reference/inference/core/entities/responses/server_state/#inference.core.entities.responses.server_state.ServerVersionInfo","title":"<code>ServerVersionInfo</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Server version information.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Server name.</p> <code>version</code> <code>str</code> <p>Server version.</p> <code>uuid</code> <code>str</code> <p>Server UUID.</p> Source code in <code>inference/core/entities/responses/server_state.py</code> <pre><code>class ServerVersionInfo(BaseModel):\n\"\"\"Server version information.\n\n    Attributes:\n        name (str): Server name.\n        version (str): Server version.\n        uuid (str): Server UUID.\n    \"\"\"\n\n    name: str = Field(examples=[\"Roboflow Inference Server\"])\n    version: str = Field(examples=[\"0.0.1\"])\n    uuid: str = Field(examples=[\"9c18c6f4-2266-41fb-8a0f-c12ae28f6fbe\"])\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/interfaces/base/","title":"base","text":""},{"location":"docs/reference/inference/core/interfaces/base/#inference.core.interfaces.base.BaseInterface","title":"<code>BaseInterface</code>","text":"<p>Base interface class which accepts a model manager on initialization</p> Source code in <code>inference/core/interfaces/base.py</code> <pre><code>class BaseInterface:\n\"\"\"Base interface class which accepts a model manager on initialization\"\"\"\n\n    def __init__(self, model_manager: ModelManager) -&gt; None:\n        self.model_manager = model_manager\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/","title":"camera","text":""},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream","title":"<code>WebcamStream</code>","text":"<p>Class to handle webcam streaming using a separate thread.</p> <p>Attributes:</p> Name Type Description <code>stream_id</code> <code>int</code> <p>The ID of the webcam stream.</p> <code>frame_id</code> <code>int</code> <p>A counter for the current frame.</p> <code>vcap</code> <code>VideoCapture</code> <p>OpenCV video capture object.</p> <code>width</code> <code>int</code> <p>The width of the video frame.</p> <code>height</code> <code>int</code> <p>The height of the video frame.</p> <code>fps_input_stream</code> <code>int</code> <p>Frames per second of the input stream.</p> <code>grabbed</code> <code>bool</code> <p>A flag indicating if a frame was successfully grabbed.</p> <code>frame</code> <code>array</code> <p>The current frame as a NumPy array.</p> <code>pil_image</code> <code>Image</code> <p>The current frame as a PIL image.</p> <code>stopped</code> <code>bool</code> <p>A flag indicating if the stream is stopped.</p> <code>t</code> <code>Thread</code> <p>The thread used to update the stream.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>class WebcamStream:\n\"\"\"Class to handle webcam streaming using a separate thread.\n\n    Attributes:\n        stream_id (int): The ID of the webcam stream.\n        frame_id (int): A counter for the current frame.\n        vcap (VideoCapture): OpenCV video capture object.\n        width (int): The width of the video frame.\n        height (int): The height of the video frame.\n        fps_input_stream (int): Frames per second of the input stream.\n        grabbed (bool): A flag indicating if a frame was successfully grabbed.\n        frame (array): The current frame as a NumPy array.\n        pil_image (Image): The current frame as a PIL image.\n        stopped (bool): A flag indicating if the stream is stopped.\n        t (Thread): The thread used to update the stream.\n    \"\"\"\n\n    def __init__(self, stream_id=0, enforce_fps=False):\n\"\"\"Initialize the webcam stream.\n\n        Args:\n            stream_id (int, optional): The ID of the webcam stream. Defaults to 0.\n        \"\"\"\n        self.stream_id = stream_id\n        self.enforce_fps = enforce_fps\n        self.frame_id = 0\n        self.vcap = cv2.VideoCapture(self.stream_id)\n\n        for key in os.environ:\n            if key.startswith(\"CV2_CAP_PROP\"):\n                opencv_prop = key[4:]\n                opencv_constant = getattr(cv2, opencv_prop, None)\n                if opencv_constant is not None:\n                    value = int(os.getenv(key))\n                    self.vcap.set(opencv_constant, value)\n                    logger.info(f\"set {opencv_prop} to {value}\")\n                else:\n                    logger.warn(f\"Property {opencv_prop} not found in cv2\")\n\n        self.width = int(self.vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.height = int(self.vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        self.file_mode = self.vcap.get(cv2.CAP_PROP_FRAME_COUNT) &gt; 0\n        if self.enforce_fps and not self.file_mode:\n            logger.warn(\n                \"Ignoring enforce_fps flag for this stream. It is not compatible with streams and will cause the process to crash\"\n            )\n            self.enforce_fps = False\n        self.max_fps = None\n        if self.vcap.isOpened() is False:\n            logger.debug(\"[Exiting]: Error accessing webcam stream.\")\n            exit(0)\n        self.fps_input_stream = int(self.vcap.get(cv2.CAP_PROP_FPS))\n        logger.debug(\n            \"FPS of webcam hardware/input stream: {}\".format(self.fps_input_stream)\n        )\n        self.grabbed, self.frame = self.vcap.read()\n        self.pil_image = Image.fromarray(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB))\n        if self.grabbed is False:\n            logger.debug(\"[Exiting] No more frames to read\")\n            exit(0)\n        self.stopped = True\n        self.t = Thread(target=self.update, args=())\n        self.t.daemon = True\n\n    def start(self):\n\"\"\"Start the thread for reading frames.\"\"\"\n        self.stopped = False\n        self.t.start()\n\n    def update(self):\n\"\"\"Update the frame by reading from the webcam.\"\"\"\n        frame_id = 0\n        next_frame_time = 0\n        t0 = time.perf_counter()\n        while True:\n            t1 = time.perf_counter()\n            if self.stopped is True:\n                break\n\n            self.grabbed = self.vcap.grab()\n            if self.grabbed is False:\n                logger.debug(\"[Exiting] No more frames to read\")\n                self.stopped = True\n                break\n            frame_id += 1\n            # We can't retrieve each frame on nano and other lower powered devices quickly enough to keep up with the stream.\n            # By default, we will only retrieve frames when we'll be ready process them (determined by self.max_fps).\n            if t1 &gt; next_frame_time:\n                ret, frame = self.vcap.retrieve()\n                if frame is None:\n                    logger.debug(\"[Exiting] Frame not available for read\")\n                    self.stopped = True\n                    break\n                logger.debug(\n                    f\"retrieved frame {frame_id}, effective FPS: {frame_id / (t1 - t0):.2f}\"\n                )\n                self.frame_id = frame_id\n                self.frame = frame\n                while self.file_mode and self.enforce_fps and self.max_fps is None:\n                    # sleep until we have processed the first frame and we know what our FPS should be\n                    time.sleep(0.01)\n                if self.max_fps is None:\n                    self.max_fps = 30\n                next_frame_time = t1 + (1 / self.max_fps) + 0.02\n            if self.file_mode:\n                t2 = time.perf_counter()\n                if self.enforce_fps:\n                    # when enforce_fps is true, grab video frames 1:1 with inference speed\n                    time_to_sleep = next_frame_time - t2\n                else:\n                    # otherwise, grab at native FPS of the video file\n                    time_to_sleep = (1 / self.fps_input_stream) - (t2 - t1)\n                if time_to_sleep &gt; 0:\n                    time.sleep(time_to_sleep)\n        self.vcap.release()\n\n    def read_opencv(self):\n\"\"\"Read the current frame using OpenCV.\n\n        Returns:\n            array, int: The current frame as a NumPy array, and the frame ID.\n        \"\"\"\n        return self.frame, self.frame_id\n\n    def stop(self):\n\"\"\"Stop the webcam stream.\"\"\"\n        self.stopped = True\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.__init__","title":"<code>__init__(stream_id=0, enforce_fps=False)</code>","text":"<p>Initialize the webcam stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_id</code> <code>int</code> <p>The ID of the webcam stream. Defaults to 0.</p> <code>0</code> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def __init__(self, stream_id=0, enforce_fps=False):\n\"\"\"Initialize the webcam stream.\n\n    Args:\n        stream_id (int, optional): The ID of the webcam stream. Defaults to 0.\n    \"\"\"\n    self.stream_id = stream_id\n    self.enforce_fps = enforce_fps\n    self.frame_id = 0\n    self.vcap = cv2.VideoCapture(self.stream_id)\n\n    for key in os.environ:\n        if key.startswith(\"CV2_CAP_PROP\"):\n            opencv_prop = key[4:]\n            opencv_constant = getattr(cv2, opencv_prop, None)\n            if opencv_constant is not None:\n                value = int(os.getenv(key))\n                self.vcap.set(opencv_constant, value)\n                logger.info(f\"set {opencv_prop} to {value}\")\n            else:\n                logger.warn(f\"Property {opencv_prop} not found in cv2\")\n\n    self.width = int(self.vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.height = int(self.vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.file_mode = self.vcap.get(cv2.CAP_PROP_FRAME_COUNT) &gt; 0\n    if self.enforce_fps and not self.file_mode:\n        logger.warn(\n            \"Ignoring enforce_fps flag for this stream. It is not compatible with streams and will cause the process to crash\"\n        )\n        self.enforce_fps = False\n    self.max_fps = None\n    if self.vcap.isOpened() is False:\n        logger.debug(\"[Exiting]: Error accessing webcam stream.\")\n        exit(0)\n    self.fps_input_stream = int(self.vcap.get(cv2.CAP_PROP_FPS))\n    logger.debug(\n        \"FPS of webcam hardware/input stream: {}\".format(self.fps_input_stream)\n    )\n    self.grabbed, self.frame = self.vcap.read()\n    self.pil_image = Image.fromarray(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB))\n    if self.grabbed is False:\n        logger.debug(\"[Exiting] No more frames to read\")\n        exit(0)\n    self.stopped = True\n    self.t = Thread(target=self.update, args=())\n    self.t.daemon = True\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.read_opencv","title":"<code>read_opencv()</code>","text":"<p>Read the current frame using OpenCV.</p> <p>Returns:</p> Type Description <p>array, int: The current frame as a NumPy array, and the frame ID.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def read_opencv(self):\n\"\"\"Read the current frame using OpenCV.\n\n    Returns:\n        array, int: The current frame as a NumPy array, and the frame ID.\n    \"\"\"\n    return self.frame, self.frame_id\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.start","title":"<code>start()</code>","text":"<p>Start the thread for reading frames.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def start(self):\n\"\"\"Start the thread for reading frames.\"\"\"\n    self.stopped = False\n    self.t.start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.stop","title":"<code>stop()</code>","text":"<p>Stop the webcam stream.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def stop(self):\n\"\"\"Stop the webcam stream.\"\"\"\n    self.stopped = True\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.update","title":"<code>update()</code>","text":"<p>Update the frame by reading from the webcam.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def update(self):\n\"\"\"Update the frame by reading from the webcam.\"\"\"\n    frame_id = 0\n    next_frame_time = 0\n    t0 = time.perf_counter()\n    while True:\n        t1 = time.perf_counter()\n        if self.stopped is True:\n            break\n\n        self.grabbed = self.vcap.grab()\n        if self.grabbed is False:\n            logger.debug(\"[Exiting] No more frames to read\")\n            self.stopped = True\n            break\n        frame_id += 1\n        # We can't retrieve each frame on nano and other lower powered devices quickly enough to keep up with the stream.\n        # By default, we will only retrieve frames when we'll be ready process them (determined by self.max_fps).\n        if t1 &gt; next_frame_time:\n            ret, frame = self.vcap.retrieve()\n            if frame is None:\n                logger.debug(\"[Exiting] Frame not available for read\")\n                self.stopped = True\n                break\n            logger.debug(\n                f\"retrieved frame {frame_id}, effective FPS: {frame_id / (t1 - t0):.2f}\"\n            )\n            self.frame_id = frame_id\n            self.frame = frame\n            while self.file_mode and self.enforce_fps and self.max_fps is None:\n                # sleep until we have processed the first frame and we know what our FPS should be\n                time.sleep(0.01)\n            if self.max_fps is None:\n                self.max_fps = 30\n            next_frame_time = t1 + (1 / self.max_fps) + 0.02\n        if self.file_mode:\n            t2 = time.perf_counter()\n            if self.enforce_fps:\n                # when enforce_fps is true, grab video frames 1:1 with inference speed\n                time_to_sleep = next_frame_time - t2\n            else:\n                # otherwise, grab at native FPS of the video file\n                time_to_sleep = (1 / self.fps_input_stream) - (t2 - t1)\n            if time_to_sleep &gt; 0:\n                time.sleep(time_to_sleep)\n    self.vcap.release()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.StatusUpdate","title":"<code>StatusUpdate</code>  <code>dataclass</code>","text":"<p>Represents a status update event in the system.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>datetime</code> <p>The timestamp when the status update was created.</p> <code>severity</code> <code>UpdateSeverity</code> <p>The severity level of the update.</p> <code>event_type</code> <code>str</code> <p>A string representing the type of the event.</p> <code>payload</code> <code>dict</code> <p>A dictionary containing data relevant to the update.</p> <code>context</code> <code>str</code> <p>A string providing additional context about the update.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass StatusUpdate:\n\"\"\"Represents a status update event in the system.\n\n    Attributes:\n        timestamp (datetime): The timestamp when the status update was created.\n        severity (UpdateSeverity): The severity level of the update.\n        event_type (str): A string representing the type of the event.\n        payload (dict): A dictionary containing data relevant to the update.\n        context (str): A string providing additional context about the update.\n    \"\"\"\n\n    timestamp: datetime\n    severity: UpdateSeverity\n    event_type: str\n    payload: dict\n    context: str\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.UpdateSeverity","title":"<code>UpdateSeverity</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enumeration for defining different levels of update severity.</p> <p>Attributes:</p> Name Type Description <code>DEBUG</code> <code>int</code> <p>A debugging severity level.</p> <code>INFO</code> <code>int</code> <p>An informational severity level.</p> <code>WARNING</code> <code>int</code> <p>A warning severity level.</p> <code>ERROR</code> <code>int</code> <p>An error severity level.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>class UpdateSeverity(Enum):\n\"\"\"Enumeration for defining different levels of update severity.\n\n    Attributes:\n        DEBUG (int): A debugging severity level.\n        INFO (int): An informational severity level.\n        WARNING (int): A warning severity level.\n        ERROR (int): An error severity level.\n    \"\"\"\n\n    DEBUG = logging.DEBUG\n    INFO = logging.INFO\n    WARNING = logging.WARNING\n    ERROR = logging.ERROR\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.VideoFrame","title":"<code>VideoFrame</code>  <code>dataclass</code>","text":"<p>Represents a single frame of video data.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>ndarray</code> <p>The image data of the frame as a NumPy array.</p> <code>frame_id</code> <code>FrameID</code> <p>A unique identifier for the frame.</p> <code>frame_timestamp</code> <code>FrameTimestamp</code> <p>The timestamp when the frame was captured.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass VideoFrame:\n\"\"\"Represents a single frame of video data.\n\n    Attributes:\n        image (np.ndarray): The image data of the frame as a NumPy array.\n        frame_id (FrameID): A unique identifier for the frame.\n        frame_timestamp (FrameTimestamp): The timestamp when the frame was captured.\n    \"\"\"\n\n    image: np.ndarray\n    frame_id: FrameID\n    frame_timestamp: FrameTimestamp\n    source_id: Optional[int] = None\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/exceptions/","title":"exceptions","text":""},{"location":"docs/reference/inference/core/interfaces/camera/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.RateLimiter","title":"<code>RateLimiter</code>","text":"<p>Implements rate upper-bound rate limiting by ensuring estimate_next_tick_delay() to be at min 1 / desired_fps, not letting the client obeying outcomes to exceed assumed rate.</p> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>class RateLimiter:\n\"\"\"\n    Implements rate upper-bound rate limiting by ensuring estimate_next_tick_delay()\n    to be at min 1 / desired_fps, not letting the client obeying outcomes to exceed\n    assumed rate.\n    \"\"\"\n\n    def __init__(self, desired_fps: Union[float, int]):\n        self._desired_fps = max(desired_fps, MINIMAL_FPS)\n        self._last_tick: Optional[float] = None\n\n    def tick(self) -&gt; None:\n        self._last_tick = time.monotonic()\n\n    def estimate_next_action_delay(self) -&gt; float:\n        if self._last_tick is None:\n            return 0.0\n        desired_delay = 1 / self._desired_fps\n        time_since_last_tick = time.monotonic() - self._last_tick\n        return max(desired_delay - time_since_last_tick, 0.0)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.VideoSourcesManager","title":"<code>VideoSourcesManager</code>","text":"<p>This class should be treated as internal building block of stream multiplexer - not for external use.</p> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>class VideoSourcesManager:\n\"\"\"\n    This class should be treated as internal building block of stream multiplexer - not for external use.\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        video_sources: VideoSources,\n        should_stop: Callable[[], bool],\n        on_reconnection_error: Callable[[Optional[int], SourceConnectionError], None],\n    ) -&gt; \"VideoSourcesManager\":\n        return cls(\n            video_sources=video_sources,\n            should_stop=should_stop,\n            on_reconnection_error=on_reconnection_error,\n        )\n\n    def __init__(\n        self,\n        video_sources: VideoSources,\n        should_stop: Callable[[], bool],\n        on_reconnection_error: Callable[[Optional[int], SourceConnectionError], None],\n    ):\n        self._video_sources = video_sources\n        self._reconnection_threads: Dict[int, Thread] = {}\n        self._external_should_stop = should_stop\n        self._on_reconnection_error = on_reconnection_error\n        self._enforce_stop: Dict[int, bool] = {}\n        self._ended_sources: Set[int] = set()\n        self._threads_to_join: Set[int] = set()\n        self._last_batch_yielded_time = datetime.now()\n\n    def retrieve_frames_from_sources(\n        self,\n        batch_collection_timeout: Optional[float],\n    ) -&gt; Optional[List[VideoFrame]]:\n        batch_frames = []\n        if batch_collection_timeout is not None:\n            batch_timeout_moment = self._last_batch_yielded_time + timedelta(\n                seconds=batch_collection_timeout\n            )\n        else:\n            batch_timeout_moment = None\n        for source_ord, (source, source_should_reconnect) in enumerate(\n            zip(self._video_sources.all_sources, self._video_sources.allow_reconnection)\n        ):\n            if self._external_should_stop():\n                self.join_all_reconnection_threads(include_not_finished=True)\n                return None\n            if self._is_source_inactive(source_ord=source_ord):\n                continue\n            batch_time_left = (\n                None\n                if batch_timeout_moment is None\n                else max((batch_timeout_moment - datetime.now()).total_seconds(), 0.0)\n            )\n            try:\n                frame = source.read_frame(timeout=batch_time_left)\n                if frame is not None:\n                    batch_frames.append(frame)\n            except EndOfStreamError:\n                self._register_end_of_stream(source_ord=source_ord)\n        self.join_all_reconnection_threads()\n        self._last_batch_yielded_time = datetime.now()\n        return batch_frames\n\n    def all_sources_ended(self) -&gt; bool:\n        return len(self._ended_sources) &gt;= len(self._video_sources.all_sources)\n\n    def join_all_reconnection_threads(self, include_not_finished: bool = False) -&gt; None:\n        for source_ord in copy(self._threads_to_join):\n            self._purge_reconnection_thread(source_ord=source_ord)\n        if not include_not_finished:\n            return None\n        for source_ord in list(self._reconnection_threads.keys()):\n            self._purge_reconnection_thread(source_ord=source_ord)\n\n    def _is_source_inactive(self, source_ord: int) -&gt; bool:\n        return (\n            source_ord in self._ended_sources\n            or source_ord in self._reconnection_threads\n        )\n\n    def _register_end_of_stream(self, source_ord: int) -&gt; None:\n        source_should_reconnect = self._video_sources.allow_reconnection[source_ord]\n        if source_should_reconnect:\n            self._reconnect_source(source_ord=source_ord)\n        else:\n            self._ended_sources.add(source_ord)\n\n    def _reconnect_source(self, source_ord: int) -&gt; None:\n        if source_ord in self._reconnection_threads:\n            return None\n        self._reconnection_threads[source_ord] = Thread(\n            target=_attempt_reconnect,\n            args=(\n                self._video_sources.all_sources[source_ord],\n                partial(self._should_stop, source_ord=source_ord),\n                self._on_reconnection_error,\n                partial(self._register_thread_to_join, source_ord=source_ord),\n                partial(self._register_reconnection_fatal_error, source_ord=source_ord),\n            ),\n        )\n        self._reconnection_threads[source_ord].start()\n\n    def _register_reconnection_fatal_error(self, source_ord: int) -&gt; None:\n        self._register_thread_to_join(source_ord=source_ord)\n        self._ended_sources.add(source_ord)\n\n    def _register_thread_to_join(self, source_ord: int) -&gt; None:\n        self._threads_to_join.add(source_ord)\n\n    def _purge_reconnection_thread(self, source_ord: int) -&gt; None:\n        if source_ord not in self._reconnection_threads:\n            return None\n        self._enforce_stop[source_ord] = True\n        self._reconnection_threads[source_ord].join()\n        del self._reconnection_threads[source_ord]\n        self._enforce_stop[source_ord] = False\n        if source_ord in self._threads_to_join:\n            self._threads_to_join.remove(source_ord)\n\n    def _should_stop(self, source_ord: int) -&gt; bool:\n        if self._external_should_stop():\n            return True\n        return self._enforce_stop.get(source_ord, False)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.get_video_frames_generator","title":"<code>get_video_frames_generator(video, max_fps=None, limiter_strategy=None)</code>","text":"<p>Util function to create a frames generator from <code>VideoSource</code> with possibility to limit FPS of consumed frames and dictate what to do if frames are produced to fast.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Union[VideoSource, str, int]</code> <p>Either instance of VideoSource or video reference accepted by VideoSource.init(...)</p> required <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>value of maximum FPS rate of generated frames - can be used to limit generation frequency</p> <code>None</code> <code>limiter_strategy</code> <code>Optional[FPSLimiterStrategy]</code> <p>strategy used to deal with frames decoding exceeding limit of <code>max_fps</code>. By default - for files, in the interest of processing all frames - generation will be awaited, for streams - frames will be dropped on the floor.</p> <code>None</code> <p>Returns: generator of <code>VideoFrame</code></p> Example <pre><code>from inference.core.interfaces.camera.utils import get_video_frames_generator\n\nfor frame in get_video_frames_generator(\n    video=\"./some.mp4\",\n    max_fps=50,\n):\n     pass\n</code></pre> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>def get_video_frames_generator(\n    video: Union[VideoSource, str, int],\n    max_fps: Optional[Union[float, int]] = None,\n    limiter_strategy: Optional[FPSLimiterStrategy] = None,\n) -&gt; Generator[VideoFrame, None, None]:\n\"\"\"\n    Util function to create a frames generator from `VideoSource` with possibility to\n    limit FPS of consumed frames and dictate what to do if frames are produced to fast.\n\n    Args:\n        video (Union[VideoSource, str, int]): Either instance of VideoSource or video reference accepted\n            by VideoSource.init(...)\n        max_fps (Optional[Union[float, int]]): value of maximum FPS rate of generated frames - can be used to limit\n            generation frequency\n        limiter_strategy (Optional[FPSLimiterStrategy]): strategy used to deal with frames decoding exceeding\n            limit of `max_fps`. By default - for files, in the interest of processing all frames -\n            generation will be awaited, for streams - frames will be dropped on the floor.\n    Returns: generator of `VideoFrame`\n\n    Example:\n        ```python\n        from inference.core.interfaces.camera.utils import get_video_frames_generator\n\n        for frame in get_video_frames_generator(\n            video=\"./some.mp4\",\n            max_fps=50,\n        ):\n             pass\n        ```\n    \"\"\"\n    is_managed_source = False\n    if issubclass(type(video), str) or issubclass(type(video), int):\n        video = VideoSource.init(\n            video_reference=video,\n        )\n        video.start()\n        is_managed_source = True\n    if max_fps is None:\n        yield from video\n        if is_managed_source:\n            video.terminate(purge_frames_buffer=True)\n        return None\n    limiter_strategy = resolve_limiter_strategy(\n        explicitly_defined_strategy=limiter_strategy,\n        source_properties=video.describe_source().source_properties,\n    )\n    yield from limit_frame_rate(\n        frames_generator=video, max_fps=max_fps, strategy=limiter_strategy\n    )\n    if is_managed_source:\n        video.terminate(purge_frames_buffer=True)\n    return None\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.multiplex_videos","title":"<code>multiplex_videos(videos, max_fps=None, limiter_strategy=None, batch_collection_timeout=None, force_stream_reconnection=True, should_stop=never_stop, on_reconnection_error=log_error)</code>","text":"<p>Function that is supposed to provide a generator over frames from multiple video sources. It is capable to initialise <code>VideoSource</code> from references to video files or streams and grab frames from all the sources - each running individual decoding on separate thread. In each cycle it attempts to grab frames from all sources (and wait at max <code>batch_collection_timeout</code> for whole batch to be collected). If frame from specific source cannot be collected in that time - it is simply not included in returned list. If after batch collection list of frames is empty - new collection start immediately. Collection does not account for sources that lost connectivity (example: streams that went offline). If that does not happen and stream has large latency - without reasonable <code>batch_collection_timeout</code> it will slow down processing - so please set it up in PROD solutions. In case of video streams (not video files) - given that <code>force_stream_reconnection=True</code> function will attempt to re-connect to disconnected source using background thread, not impairing batch frames collection and that source is not going to block frames retrieval even if infinite <code>batch_collection_timeout=None</code> is set. Similarly, when processing files - video file that is shorter than other passed into processing will not block the whole flow after End Of Stream (EOS).</p> <p>All sources must be accessible on start - if that's not the case - logic function raises <code>SourceConnectionError</code> and closes all video sources it opened on it own. Disconnections at later stages are handled by re-connection mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>List[Union[VideoSource, str, int]]</code> <p>List with references to video sources. Elements can be pre-initialised <code>VideoSource</code> instances, str with stream URI or file location or int representing camera device attached to the PC/server running the code.</p> required <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Upper-bound of processing speed - to be used when one wants at max <code>max_fps</code> video frames per second to be yielded from all sources by the generator.</p> <code>None</code> <code>limiter_strategy</code> <code>Optional[FPSLimiterStrategy]</code> <p>strategy used to deal with frames decoding exceeding limit of <code>max_fps</code>. For video files, in the interest of processing all frames - we recommend WAIT mode,  for streams - frames should be dropped on the floor with DROP strategy. Not setting the strategy equals  using automatic mode - WAIT if all sources are files and DROP otherwise</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>maximum await time to get batch of predictions from all sources. <code>None</code> means infinite timeout.</p> <code>None</code> <code>force_stream_reconnection</code> <code>bool</code> <p>Flag to decide on reconnection to streams (files are never re-connected)</p> <code>True</code> <code>should_stop</code> <code>Callable[[], bool]</code> <p>external stop signal that is periodically checked - to denote that video consumption stopped - make the function to return True</p> <code>never_stop</code> <code>on_reconnection_error</code> <code>Callable[[Optional[int], SourceConnectionError], None]</code> <p>Function that will be called whenever source cannot re-connect after disconnection. First parameter is source_id, second is connection error instance.</p> <code>log_error</code> <p>Returns Generator[List[VideoFrame], None, None]: allowing to iterate through frames from multiple video sources.</p> <p>Raises:</p> Type Description <code>SourceConnectionError</code> <p>when one or more source is not reachable at start of generation</p> Example <pre><code>from inference.core.interfaces.camera.utils import multiplex_videos\n\nfor frames in multiplex_videos(videos=[\"./some.mp4\", \"./other.mp4\"]):\n     for frame in frames:\n        pass  # do something with frame\n</code></pre> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>def multiplex_videos(\n    videos: List[Union[VideoSource, str, int]],\n    max_fps: Optional[Union[float, int]] = None,\n    limiter_strategy: Optional[FPSLimiterStrategy] = None,\n    batch_collection_timeout: Optional[float] = None,\n    force_stream_reconnection: bool = True,\n    should_stop: Callable[[], bool] = never_stop,\n    on_reconnection_error: Callable[\n        [Optional[int], SourceConnectionError], None\n    ] = log_error,\n) -&gt; Generator[List[VideoFrame], None, None]:\n\"\"\"\n    Function that is supposed to provide a generator over frames from multiple video sources. It is capable to\n    initialise `VideoSource` from references to video files or streams and grab frames from all the sources -\n    each running individual decoding on separate thread. In each cycle it attempts to grab frames from all sources\n    (and wait at max `batch_collection_timeout` for whole batch to be collected). If frame from specific source\n    cannot be collected in that time - it is simply not included in returned list. If after batch collection list of\n    frames is empty - new collection start immediately. Collection does not account for\n    sources that lost connectivity (example: streams that went offline). If that does not happen and stream has\n    large latency - without reasonable `batch_collection_timeout` it will slow down processing - so please\n    set it up in PROD solutions. In case of video streams (not video files) - given that\n    `force_stream_reconnection=True` function will attempt to re-connect to disconnected source using background thread,\n    not impairing batch frames collection and that source is not going to block frames retrieval even if infinite\n    `batch_collection_timeout=None` is set. Similarly, when processing files - video file that is shorter than other\n    passed into processing will not block the whole flow after End Of Stream (EOS).\n\n    All sources must be accessible on start - if that's not the case - logic function raises `SourceConnectionError`\n    and closes all video sources it opened on it own. Disconnections at later stages are handled by re-connection\n    mechanism.\n\n    Args:\n        videos (List[Union[VideoSource, str, int]]): List with references to video sources. Elements can be\n            pre-initialised `VideoSource` instances, str with stream URI or file location or int representing\n            camera device attached to the PC/server running the code.\n        max_fps (Optional[Union[float, int]]): Upper-bound of processing speed - to be used when one wants at max\n            `max_fps` video frames per second to be yielded from all sources by the generator.\n        limiter_strategy (Optional[FPSLimiterStrategy]): strategy used to deal with frames decoding exceeding\n            limit of `max_fps`. For video files, in the interest of processing all frames - we recommend WAIT mode,\n             for streams - frames should be dropped on the floor with DROP strategy. Not setting the strategy equals\n             using automatic mode - WAIT if all sources are files and DROP otherwise\n        batch_collection_timeout (Optional[float]): maximum await time to get batch of predictions from all sources.\n            `None` means infinite timeout.\n        force_stream_reconnection (bool): Flag to decide on reconnection to streams (files are never re-connected)\n        should_stop (Callable[[], bool]): external stop signal that is periodically checked - to denote that\n            video consumption stopped - make the function to return True\n        on_reconnection_error (Callable[[Optional[int], SourceConnectionError], None]): Function that will be\n            called whenever source cannot re-connect after disconnection. First parameter is source_id, second\n            is connection error instance.\n\n    Returns Generator[List[VideoFrame], None, None]: allowing to iterate through frames from multiple video sources.\n\n    Raises:\n        SourceConnectionError: when one or more source is not reachable at start of generation\n\n    Example:\n        ```python\n        from inference.core.interfaces.camera.utils import multiplex_videos\n\n        for frames in multiplex_videos(videos=[\"./some.mp4\", \"./other.mp4\"]):\n             for frame in frames:\n                pass  # do something with frame\n        ```\n    \"\"\"\n    video_sources = _prepare_video_sources(\n        videos=videos, force_stream_reconnection=force_stream_reconnection\n    )\n    if any(rule is None for rule in video_sources.allow_reconnection):\n        logger.warning(\"Could not connect to all sources.\")\n        return None\n    generator = _multiplex_videos(\n        video_sources=video_sources,\n        batch_collection_timeout=batch_collection_timeout,\n        should_stop=should_stop,\n        on_reconnection_error=on_reconnection_error,\n    )\n    if max_fps is None:\n        yield from generator\n        return None\n    max_fps = max_fps / len(videos)\n    if limiter_strategy is None:\n        limiter_strategy = negotiate_rate_limiter_strategy_for_multiple_sources(\n            video_sources=video_sources.all_sources,\n        )\n    yield from limit_frame_rate(\n        frames_generator=generator, max_fps=max_fps, strategy=limiter_strategy\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/","title":"video_source","text":""},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoConsumer","title":"<code>VideoConsumer</code>","text":"<p>This class should be consumed as part of internal implementation. It provides abstraction around stream consumption strategies.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>class VideoConsumer:\n\"\"\"\n    This class should be consumed as part of internal implementation.\n    It provides abstraction around stream consumption strategies.\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        buffer_filling_strategy: Optional[BufferFillingStrategy],\n        adaptive_mode_stream_pace_tolerance: float,\n        adaptive_mode_reader_pace_tolerance: float,\n        minimum_adaptive_mode_samples: int,\n        maximum_adaptive_frames_dropped_in_row: int,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n    ) -&gt; \"VideoConsumer\":\n        minimum_adaptive_mode_samples = max(minimum_adaptive_mode_samples, 2)\n        reader_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        stream_consumption_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        decoding_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        return cls(\n            buffer_filling_strategy=buffer_filling_strategy,\n            adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n            adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n            minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n            maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n            status_update_handlers=status_update_handlers,\n            reader_pace_monitor=reader_pace_monitor,\n            stream_consumption_pace_monitor=stream_consumption_pace_monitor,\n            decoding_pace_monitor=decoding_pace_monitor,\n        )\n\n    def __init__(\n        self,\n        buffer_filling_strategy: Optional[BufferFillingStrategy],\n        adaptive_mode_stream_pace_tolerance: float,\n        adaptive_mode_reader_pace_tolerance: float,\n        minimum_adaptive_mode_samples: int,\n        maximum_adaptive_frames_dropped_in_row: int,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        reader_pace_monitor: sv.FPSMonitor,\n        stream_consumption_pace_monitor: sv.FPSMonitor,\n        decoding_pace_monitor: sv.FPSMonitor,\n    ):\n        self._buffer_filling_strategy = buffer_filling_strategy\n        self._frame_counter = 0\n        self._adaptive_mode_stream_pace_tolerance = adaptive_mode_stream_pace_tolerance\n        self._adaptive_mode_reader_pace_tolerance = adaptive_mode_reader_pace_tolerance\n        self._minimum_adaptive_mode_samples = minimum_adaptive_mode_samples\n        self._maximum_adaptive_frames_dropped_in_row = (\n            maximum_adaptive_frames_dropped_in_row\n        )\n        self._adaptive_frames_dropped_in_row = 0\n        self._reader_pace_monitor = reader_pace_monitor\n        self._stream_consumption_pace_monitor = stream_consumption_pace_monitor\n        self._decoding_pace_monitor = decoding_pace_monitor\n        self._status_update_handlers = status_update_handlers\n\n    @property\n    def buffer_filling_strategy(self) -&gt; Optional[BufferFillingStrategy]:\n        return self._buffer_filling_strategy\n\n    def reset(self, source_properties: SourceProperties) -&gt; None:\n        if source_properties.is_file:\n            self._set_file_mode_buffering_strategies()\n        else:\n            self._set_stream_mode_buffering_strategies()\n        self._reader_pace_monitor.reset()\n        self.reset_stream_consumption_pace()\n        self._decoding_pace_monitor.reset()\n        self._adaptive_frames_dropped_in_row = 0\n\n    def reset_stream_consumption_pace(self) -&gt; None:\n        self._stream_consumption_pace_monitor.reset()\n\n    def notify_frame_consumed(self) -&gt; None:\n        self._reader_pace_monitor.tick()\n\n    def consume_frame(\n        self,\n        video: cv2.VideoCapture,\n        declared_source_fps: Optional[float],\n        buffer: Queue,\n        frames_buffering_allowed: bool,\n        source_id: Optional[int] = None,\n    ) -&gt; bool:\n        frame_timestamp = datetime.now()\n        success = video.grab()\n        self._stream_consumption_pace_monitor.tick()\n        if not success:\n            return False\n        self._frame_counter += 1\n        send_video_source_status_update(\n            severity=UpdateSeverity.DEBUG,\n            event_type=FRAME_CAPTURED_EVENT,\n            payload={\n                \"frame_timestamp\": frame_timestamp,\n                \"frame_id\": self._frame_counter,\n                \"source_id\": source_id,\n            },\n            status_update_handlers=self._status_update_handlers,\n        )\n        return self._consume_stream_frame(\n            video=video,\n            declared_source_fps=declared_source_fps,\n            frame_timestamp=frame_timestamp,\n            buffer=buffer,\n            frames_buffering_allowed=frames_buffering_allowed,\n            source_id=source_id,\n        )\n\n    def _set_file_mode_buffering_strategies(self) -&gt; None:\n        if self._buffer_filling_strategy is None:\n            self._buffer_filling_strategy = BufferFillingStrategy.WAIT\n\n    def _set_stream_mode_buffering_strategies(self) -&gt; None:\n        if self._buffer_filling_strategy is None:\n            self._buffer_filling_strategy = BufferFillingStrategy.ADAPTIVE_DROP_OLDEST\n\n    def _consume_stream_frame(\n        self,\n        video: cv2.VideoCapture,\n        declared_source_fps: Optional[float],\n        frame_timestamp: datetime,\n        buffer: Queue,\n        frames_buffering_allowed: bool,\n        source_id: Optional[int],\n    ) -&gt; bool:\n\"\"\"\n        Returns: boolean flag with success status\n        \"\"\"\n        if not frames_buffering_allowed:\n            send_frame_drop_update(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                cause=\"Buffering not allowed at the moment\",\n                status_update_handlers=self._status_update_handlers,\n                source_id=source_id,\n            )\n            return True\n        if self._frame_should_be_adaptively_dropped(\n            declared_source_fps=declared_source_fps\n        ):\n            self._adaptive_frames_dropped_in_row += 1\n            send_frame_drop_update(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                cause=\"ADAPTIVE strategy\",\n                status_update_handlers=self._status_update_handlers,\n                source_id=source_id,\n            )\n            return True\n        self._adaptive_frames_dropped_in_row = 0\n        if (\n            not buffer.full()\n            or self._buffer_filling_strategy is BufferFillingStrategy.WAIT\n        ):\n            return decode_video_frame_to_buffer(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                video=video,\n                buffer=buffer,\n                decoding_pace_monitor=self._decoding_pace_monitor,\n                source_id=source_id,\n            )\n        if self._buffer_filling_strategy in DROP_OLDEST_STRATEGIES:\n            return self._process_stream_frame_dropping_oldest(\n                frame_timestamp=frame_timestamp,\n                video=video,\n                buffer=buffer,\n                source_id=source_id,\n            )\n        send_frame_drop_update(\n            frame_timestamp=frame_timestamp,\n            frame_id=self._frame_counter,\n            cause=\"DROP_LATEST strategy\",\n            status_update_handlers=self._status_update_handlers,\n            source_id=source_id,\n        )\n        return True\n\n    def _frame_should_be_adaptively_dropped(\n        self, declared_source_fps: Optional[float]\n    ) -&gt; bool:\n        if self._buffer_filling_strategy not in ADAPTIVE_STRATEGIES:\n            return False\n        if (\n            self._adaptive_frames_dropped_in_row\n            &gt;= self._maximum_adaptive_frames_dropped_in_row\n        ):\n            return False\n        if (\n            len(self._stream_consumption_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ):\n            # not enough observations\n            return False\n        stream_consumption_pace = self._stream_consumption_pace_monitor()\n        announced_stream_fps = stream_consumption_pace\n        if declared_source_fps is not None and declared_source_fps &gt; 0:\n            announced_stream_fps = declared_source_fps\n        if (\n            announced_stream_fps - stream_consumption_pace\n            &gt; self._adaptive_mode_stream_pace_tolerance\n        ):\n            # cannot keep up with stream emission\n            return True\n        if (\n            len(self._reader_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ) or (\n            len(self._decoding_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ):\n            # not enough observations\n            return False\n        actual_reader_pace = get_fps_if_tick_happens_now(\n            fps_monitor=self._reader_pace_monitor\n        )\n        decoding_pace = self._decoding_pace_monitor()\n        if (\n            decoding_pace - actual_reader_pace\n            &gt; self._adaptive_mode_reader_pace_tolerance\n        ):\n            # we are too fast for the reader - time to save compute on decoding\n            return True\n        return False\n\n    def _process_stream_frame_dropping_oldest(\n        self,\n        frame_timestamp: datetime,\n        video: cv2.VideoCapture,\n        buffer: Queue,\n        source_id: Optional[int],\n    ) -&gt; bool:\n        drop_single_frame_from_buffer(\n            buffer=buffer,\n            cause=\"DROP_OLDEST strategy\",\n            status_update_handlers=self._status_update_handlers,\n        )\n        return decode_video_frame_to_buffer(\n            frame_timestamp=frame_timestamp,\n            frame_id=self._frame_counter,\n            video=video,\n            buffer=buffer,\n            decoding_pace_monitor=self._decoding_pace_monitor,\n            source_id=source_id,\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource","title":"<code>VideoSource</code>","text":"Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>class VideoSource:\n    @classmethod\n    def init(\n        cls,\n        video_reference: Union[str, int],\n        buffer_size: int = DEFAULT_BUFFER_SIZE,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        adaptive_mode_stream_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE,\n        adaptive_mode_reader_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE,\n        minimum_adaptive_mode_samples: int = DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES,\n        maximum_adaptive_frames_dropped_in_row: int = DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        source_id: Optional[int] = None,\n    ):\n\"\"\"\n        This class is meant to represent abstraction over video sources - both video files and\n        on-line streams that are possible to be consumed and used by other components of `inference`\n        library.\n\n        Before digging into details of the class behaviour, it is advised to familiarise with the following\n        concepts and implementation assumptions:\n\n        1. Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by\n            its processing capabilities. If processing is faster than the frame rate of video, operations\n            may be executed in a time shorter than the time of video playback. In the opposite case - consumer\n            may freely decode and process frames in its own pace, without risk for failures due to temporal\n            dependencies of processing - this is classical offline processing example.\n        2. Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -\n            in other words - this is on-line processing example. Consumer being faster than incoming stream\n            frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.\n            Slow consumer, however, may not be able to process everything on time and to keep up with the pace\n            of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of\n            sync with the stream causing decoding failures or unpredictable behavior.\n\n        To fit those two types of video sources, `VideoSource` introduces the concept of buffered decoding of\n        video stream (like at the YouTube - player buffers some frames that are soon to be displayed).\n        The way on how buffer is filled and consumed dictates the behavior of `VideoSource`.\n\n        Starting from `BufferFillingStrategy` - we have 3 basic options:\n        * WAIT: in case of slow video consumption, when buffer is full - `VideoSource` will wait for\n        the empty spot in buffer before next frame will be processed - this is suitable in cases when\n        we want to ensure EACH FRAME of the video to be processed\n        * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped -\n        this is suitable for cases when we want to process the most recent frames possible\n        * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when\n        it is expected to have processing performance drops, but we would like to consume portions of\n        video that are locally smooth - but this is probably the least common use-case.\n\n        On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST,\n        which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion\n        of that mode will be described later.\n\n        Naturally, decoded frames must also be consumed. `VideoSource` provides a handy interface for reading\n        a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via\n        `BufferConsumptionStrategy`:\n        * LAZY - consume all the frames from decoding buffer one-by-one\n        * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent\n\n        In consequence - there are various combinations of `BufferFillingStrategy` and `BufferConsumptionStrategy`.\n        The most popular would be:\n        * `BufferFillingStrategy.WAIT` and `BufferConsumptionStrategy.LAZY` - to always decode and process each and\n            every frame of the source (useful while processing video files - and default behaviour enforced by\n            `inference` if there is no explicit configuration)\n        * `BufferFillingStrategy.DROP_OLDEST` and `BufferConsumptionStrategy.EAGER` - to always process the most\n            recent frames of source (useful while processing video streams when low latency [real-time experience]\n            is required - ADAPTIVE version of this is default for streams)\n\n        ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume\n        video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing\n        against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as `DROP_OLDEST`\n        and `DROP_LATEST` strategies, but there are two more conditions that may influence frame drop:\n        * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that\n        MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing\n        deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner\n        * consumption rate - in resource constraints environment, not only decoding is problematic from the performance\n        perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources\n        for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently\n        than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just\n        grabbed and dropped on the floor.\n        ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases.\n        Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source,\n        reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients,\n        with reasonable defaults being set.\n\n        `VideoSource` emits events regarding its activity - which can be intercepted by custom handlers. Take\n        into account that they are always executed in context of thread invoking them (and should be fast to complete,\n        otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.\n\n        `VideoSource` implementation is naturally multithreading, with different thread decoding video and different\n        one consuming it and manipulating source state. Implementation of user interface is thread-safe, although\n        stream it is meant to be consumed by a single thread only.\n\n        ENV variables involved:\n        * VIDEO_SOURCE_BUFFER_SIZE - default: 64\n        * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1\n        * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0\n        * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10\n        * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n\n        Args:\n            video_reference (Union[str, int]): Either str with file or stream reference, or int representing device ID\n            buffer_size (int): size of decoding buffer\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers for status updates\n            buffer_filling_strategy (Optional[BufferFillingStrategy]): Settings for buffer filling strategy - if not\n                given - automatic choice regarding source type will be applied\n            buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Settings for buffer consumption strategy,\n                if not given - automatic choice regarding source type will be applied\n            adaptive_mode_stream_pace_tolerance (float): Maximum deviation between frames grabbing pace and stream pace\n                that will not trigger adaptive mode frame drop\n            adaptive_mode_reader_pace_tolerance (float): Maximum deviation between decoding pace and stream consumption\n                pace that will not trigger adaptive mode frame drop\n            minimum_adaptive_mode_samples (int): Minimal number of frames to be used to establish actual pace of\n                processing, before adaptive mode can drop any frame\n            maximum_adaptive_frames_dropped_in_row (int): Maximum number of frames dropped in row due to application of\n                adaptive strategy\n            video_source_properties (Optional[dict[str, float]]): Optional dictionary with video source properties\n                corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.\n            source_id (Optional[int]): Optional identifier of video source - mainly useful to recognise specific source\n                when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised\n                to keep it unique within all sources in use.\n\n        Returns: Instance of `VideoSource` class\n        \"\"\"\n        frames_buffer = Queue(maxsize=buffer_size)\n        if status_update_handlers is None:\n            status_update_handlers = []\n        video_consumer = VideoConsumer.init(\n            buffer_filling_strategy=buffer_filling_strategy,\n            adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n            adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n            minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n            maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n            status_update_handlers=status_update_handlers,\n        )\n        return cls(\n            stream_reference=video_reference,\n            frames_buffer=frames_buffer,\n            status_update_handlers=status_update_handlers,\n            buffer_consumption_strategy=buffer_consumption_strategy,\n            video_consumer=video_consumer,\n            video_source_properties=video_source_properties,\n            source_id=source_id,\n        )\n\n    def __init__(\n        self,\n        stream_reference: Union[str, int],\n        frames_buffer: Queue,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        buffer_consumption_strategy: Optional[BufferConsumptionStrategy],\n        video_consumer: \"VideoConsumer\",\n        video_source_properties: Optional[Dict[str, float]],\n        source_id: Optional[int],\n    ):\n        self._stream_reference = stream_reference\n        self._video: Optional[cv2.VideoCapture] = None\n        self._source_properties: Optional[SourceProperties] = None\n        self._frames_buffer = frames_buffer\n        self._status_update_handlers = status_update_handlers\n        self._buffer_consumption_strategy = buffer_consumption_strategy\n        self._video_consumer = video_consumer\n        self._state = StreamState.NOT_STARTED\n        self._playback_allowed = Event()\n        self._frames_buffering_allowed = True\n        self._stream_consumption_thread: Optional[Thread] = None\n        self._state_change_lock = Lock()\n        self._video_source_properties = video_source_properties or {}\n        self._source_id = source_id\n\n    @property\n    def source_id(self) -&gt; Optional[int]:\n        return self._source_id\n\n    @lock_state_transition\n    def restart(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n\"\"\"\n        Method to restart source consumption. Eligible to be used in states:\n        [MUTED, RUNNING, PAUSED, ENDED, ERROR].\n        End state:\n        * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n        * ERROR - if it was not possible to connect with source\n\n        Thread safe - only one transition of states possible at the time.\n\n        Args:\n            wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n                completion of this operation.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n            * SourceConnectionError: if source cannot be connected\n        \"\"\"\n        if self._state not in RESTART_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not RESTART stream in state: {self._state}\"\n            )\n        self._restart(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n\n    @lock_state_transition\n    def start(self) -&gt; None:\n\"\"\"\n        Method to be used to start source consumption. Eligible to be used in states:\n        [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)]\n        End state:\n        * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n        * ERROR - if it was not possible to connect with source\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n            * SourceConnectionError: if source cannot be connected\n        \"\"\"\n        if self._state not in START_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not START stream in state: {self._state}\"\n            )\n        self._start()\n\n    @lock_state_transition\n    def terminate(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n\"\"\"\n        Method to be used to terminate source consumption. Eligible to be used in states:\n        [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)]\n        End state:\n        * ENDED - indicating success of the process\n        * ERROR - if error with processing occurred\n\n        Must be used to properly dispose resources at the end.\n\n        Thread safe - only one transition of states possible at the time.\n\n        Args:\n            wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n                completion of this operation.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in TERMINATE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not TERMINATE stream in state: {self._state}\"\n            )\n        self._terminate(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n\n    @lock_state_transition\n    def pause(self) -&gt; None:\n\"\"\"\n        Method to be used to pause source consumption. During pause - no new frames are consumed.\n        Used on on-line streams for too long may cause stream disconnection.\n        Eligible to be used in states:\n        [RUNNING]\n        End state:\n        * PAUSED\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in PAUSE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not PAUSE stream in state: {self._state}\"\n            )\n        self._pause()\n\n    @lock_state_transition\n    def mute(self) -&gt; None:\n\"\"\"\n        Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where\n        frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing\n        intermediate frames to be dropped. May be also used against files, although arguably less useful.\n        Eligible to be used in states:\n        [RUNNING]\n        End state:\n        * MUTED\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in MUTE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not MUTE stream in state: {self._state}\"\n            )\n        self._mute()\n\n    @lock_state_transition\n    def resume(self) -&gt; None:\n\"\"\"\n        Method to recover from pause or mute into running state.\n        [PAUSED, MUTED]\n        End state:\n        * RUNNING\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in RESUME_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not RESUME stream in state: {self._state}\"\n            )\n        self._resume()\n\n    def get_state(self) -&gt; StreamState:\n\"\"\"\n        Method to get current state of the `VideoSource`\n\n        Returns: StreamState\n        \"\"\"\n        return self._state\n\n    def frame_ready(self) -&gt; bool:\n\"\"\"\n        Method to check if decoded frame is ready for consumer\n\n        Returns: boolean flag indicating frame readiness\n        \"\"\"\n        return not self._frames_buffer.empty()\n\n    def read_frame(self, timeout: Optional[float] = None) -&gt; Optional[VideoFrame]:\n\"\"\"\n        Method to be used by the consumer to get decoded source frame.\n\n        Returns: VideoFrame object with decoded frame and its metadata.\n        Throws:\n            * EndOfStreamError: when trying to get the frame from closed source.\n        \"\"\"\n        video_frame: Optional[Union[VideoFrame, str]] = get_from_queue(\n            queue=self._frames_buffer,\n            on_successful_read=self._video_consumer.notify_frame_consumed,\n            timeout=timeout,\n            purge=self._buffer_consumption_strategy is BufferConsumptionStrategy.EAGER,\n        )\n        if video_frame == POISON_PILL:\n            raise EndOfStreamError(\n                \"Attempted to retrieve frame from stream that already ended.\"\n            )\n        if video_frame is not None:\n            send_video_source_status_update(\n                severity=UpdateSeverity.DEBUG,\n                event_type=FRAME_CONSUMED_EVENT,\n                payload={\n                    \"frame_timestamp\": video_frame.frame_timestamp,\n                    \"frame_id\": video_frame.frame_id,\n                    \"source_id\": video_frame.source_id,\n                },\n                status_update_handlers=self._status_update_handlers,\n            )\n        return video_frame\n\n    def describe_source(self) -&gt; SourceMetadata:\n        return SourceMetadata(\n            source_properties=self._source_properties,\n            source_reference=self._stream_reference,\n            buffer_size=self._frames_buffer.maxsize,\n            state=self._state,\n            buffer_filling_strategy=self._video_consumer.buffer_filling_strategy,\n            buffer_consumption_strategy=self._buffer_consumption_strategy,\n            source_id=self._source_id,\n        )\n\n    def _restart(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        self._terminate(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n        self._change_state(target_state=StreamState.RESTARTING)\n        self._playback_allowed = Event()\n        self._frames_buffering_allowed = True\n        self._video: Optional[cv2.VideoCapture] = None\n        self._source_properties: Optional[SourceProperties] = None\n        self._start()\n\n    def _start(self) -&gt; None:\n        self._change_state(target_state=StreamState.INITIALISING)\n        self._video = cv2.VideoCapture(self._stream_reference)\n        if not self._video.isOpened():\n            self._change_state(target_state=StreamState.ERROR)\n            raise SourceConnectionError(\n                f\"Cannot connect to video source under reference: {self._stream_reference}\"\n            )\n        initialize_source_properties(\n            video=self._video, properties=self._video_source_properties\n        )\n        self._source_properties = discover_source_properties(stream=self._video)\n        self._video_consumer.reset(source_properties=self._source_properties)\n        if self._source_properties.is_file:\n            self._set_file_mode_consumption_strategies()\n        else:\n            self._set_stream_mode_consumption_strategies()\n        self._playback_allowed.set()\n        self._stream_consumption_thread = Thread(target=self._consume_video)\n        self._stream_consumption_thread.start()\n\n    def _terminate(\n        self, wait_on_frames_consumption: bool, purge_frames_buffer: bool\n    ) -&gt; None:\n        if self._state in RESUME_ELIGIBLE_STATES:\n            self._resume()\n        previous_state = self._state\n        self._change_state(target_state=StreamState.TERMINATING)\n        if purge_frames_buffer:\n            _ = get_from_queue(queue=self._frames_buffer, timeout=0.0, purge=True)\n        if self._stream_consumption_thread is not None:\n            self._stream_consumption_thread.join()\n        if wait_on_frames_consumption:\n            self._frames_buffer.join()\n        if previous_state is not StreamState.ERROR:\n            self._change_state(target_state=StreamState.ENDED)\n\n    def _pause(self) -&gt; None:\n        self._playback_allowed.clear()\n        self._change_state(target_state=StreamState.PAUSED)\n\n    def _mute(self) -&gt; None:\n        self._frames_buffering_allowed = False\n        self._change_state(target_state=StreamState.MUTED)\n\n    def _resume(self) -&gt; None:\n        previous_state = self._state\n        self._change_state(target_state=StreamState.RUNNING)\n        if previous_state is StreamState.PAUSED:\n            self._video_consumer.reset_stream_consumption_pace()\n            self._playback_allowed.set()\n        if previous_state is StreamState.MUTED:\n            self._frames_buffering_allowed = True\n\n    def _set_file_mode_consumption_strategies(self) -&gt; None:\n        if self._buffer_consumption_strategy is None:\n            self._buffer_consumption_strategy = BufferConsumptionStrategy.LAZY\n\n    def _set_stream_mode_consumption_strategies(self) -&gt; None:\n        if self._buffer_consumption_strategy is None:\n            self._buffer_consumption_strategy = BufferConsumptionStrategy.EAGER\n\n    def _consume_video(self) -&gt; None:\n        send_video_source_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=VIDEO_CONSUMPTION_STARTED_EVENT,\n            status_update_handlers=self._status_update_handlers,\n            payload={\"source_id\": self._source_id},\n        )\n        logger.info(f\"Video consumption started\")\n        try:\n            if self._state is not StreamState.TERMINATING:\n                self._change_state(target_state=StreamState.RUNNING)\n            declared_source_fps = None\n            if self._source_properties is not None:\n                declared_source_fps = self._source_properties.fps\n            while self._video.isOpened():\n                if self._state is StreamState.TERMINATING:\n                    break\n                self._playback_allowed.wait()\n                success = self._video_consumer.consume_frame(\n                    video=self._video,\n                    declared_source_fps=declared_source_fps,\n                    buffer=self._frames_buffer,\n                    frames_buffering_allowed=self._frames_buffering_allowed,\n                    source_id=self._source_id,\n                )\n                if not success:\n                    break\n            self._frames_buffer.put(POISON_PILL)\n            self._video.release()\n            self._change_state(target_state=StreamState.ENDED)\n            send_video_source_status_update(\n                severity=UpdateSeverity.INFO,\n                event_type=VIDEO_CONSUMPTION_FINISHED_EVENT,\n                status_update_handlers=self._status_update_handlers,\n                payload={\"source_id\": self._source_id},\n            )\n            logger.info(f\"Video consumption finished\")\n        except Exception as error:\n            self._change_state(target_state=StreamState.ERROR)\n            payload = {\n                \"source_id\": self._source_id,\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"stream_consumer_thread\",\n            }\n            send_video_source_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=SOURCE_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.exception(\"Encountered error in video consumption thread\")\n\n    def _change_state(self, target_state: StreamState) -&gt; None:\n        payload = {\n            \"previous_state\": self._state,\n            \"new_state\": target_state,\n            \"source_id\": self._source_id,\n        }\n        self._state = target_state\n        send_video_source_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=SOURCE_STATE_UPDATE_EVENT,\n            payload=payload,\n            status_update_handlers=self._status_update_handlers,\n        )\n\n    def __iter__(self) -&gt; \"VideoSource\":\n        return self\n\n    def __next__(self) -&gt; VideoFrame:\n\"\"\"\n        Method allowing to use `VideoSource` convenient to read frames\n\n        Returns: VideoFrame\n\n        Example:\n            ```python\n            source = VideoSource.init(video_reference=\"./some.mp4\")\n            source.start()\n\n            for frame in source:\n                 pass\n            ```\n        \"\"\"\n        try:\n            return self.read_frame()\n        except EndOfStreamError:\n            raise StopIteration()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.__next__","title":"<code>__next__()</code>","text":"<p>Method allowing to use <code>VideoSource</code> convenient to read frames</p> <p>Returns: VideoFrame</p> Example <pre><code>source = VideoSource.init(video_reference=\"./some.mp4\")\nsource.start()\n\nfor frame in source:\n     pass\n</code></pre> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def __next__(self) -&gt; VideoFrame:\n\"\"\"\n    Method allowing to use `VideoSource` convenient to read frames\n\n    Returns: VideoFrame\n\n    Example:\n        ```python\n        source = VideoSource.init(video_reference=\"./some.mp4\")\n        source.start()\n\n        for frame in source:\n             pass\n        ```\n    \"\"\"\n    try:\n        return self.read_frame()\n    except EndOfStreamError:\n        raise StopIteration()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.frame_ready","title":"<code>frame_ready()</code>","text":"<p>Method to check if decoded frame is ready for consumer</p> <p>Returns: boolean flag indicating frame readiness</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def frame_ready(self) -&gt; bool:\n\"\"\"\n    Method to check if decoded frame is ready for consumer\n\n    Returns: boolean flag indicating frame readiness\n    \"\"\"\n    return not self._frames_buffer.empty()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.get_state","title":"<code>get_state()</code>","text":"<p>Method to get current state of the <code>VideoSource</code></p> <p>Returns: StreamState</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def get_state(self) -&gt; StreamState:\n\"\"\"\n    Method to get current state of the `VideoSource`\n\n    Returns: StreamState\n    \"\"\"\n    return self._state\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.init","title":"<code>init(video_reference, buffer_size=DEFAULT_BUFFER_SIZE, status_update_handlers=None, buffer_filling_strategy=None, buffer_consumption_strategy=None, adaptive_mode_stream_pace_tolerance=DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE, adaptive_mode_reader_pace_tolerance=DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE, minimum_adaptive_mode_samples=DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES, maximum_adaptive_frames_dropped_in_row=DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW, video_source_properties=None, source_id=None)</code>  <code>classmethod</code>","text":"<p>This class is meant to represent abstraction over video sources - both video files and on-line streams that are possible to be consumed and used by other components of <code>inference</code> library.</p> <p>Before digging into details of the class behaviour, it is advised to familiarise with the following concepts and implementation assumptions:</p> <ol> <li>Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by     its processing capabilities. If processing is faster than the frame rate of video, operations     may be executed in a time shorter than the time of video playback. In the opposite case - consumer     may freely decode and process frames in its own pace, without risk for failures due to temporal     dependencies of processing - this is classical offline processing example.</li> <li>Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -     in other words - this is on-line processing example. Consumer being faster than incoming stream     frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.     Slow consumer, however, may not be able to process everything on time and to keep up with the pace     of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of     sync with the stream causing decoding failures or unpredictable behavior.</li> </ol> <p>To fit those two types of video sources, <code>VideoSource</code> introduces the concept of buffered decoding of video stream (like at the YouTube - player buffers some frames that are soon to be displayed). The way on how buffer is filled and consumed dictates the behavior of <code>VideoSource</code>.</p> <p>Starting from <code>BufferFillingStrategy</code> - we have 3 basic options: * WAIT: in case of slow video consumption, when buffer is full - <code>VideoSource</code> will wait for the empty spot in buffer before next frame will be processed - this is suitable in cases when we want to ensure EACH FRAME of the video to be processed * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped - this is suitable for cases when we want to process the most recent frames possible * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when it is expected to have processing performance drops, but we would like to consume portions of video that are locally smooth - but this is probably the least common use-case.</p> <p>On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST, which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion of that mode will be described later.</p> <p>Naturally, decoded frames must also be consumed. <code>VideoSource</code> provides a handy interface for reading a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via <code>BufferConsumptionStrategy</code>: * LAZY - consume all the frames from decoding buffer one-by-one * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent</p> <p>In consequence - there are various combinations of <code>BufferFillingStrategy</code> and <code>BufferConsumptionStrategy</code>. The most popular would be: * <code>BufferFillingStrategy.WAIT</code> and <code>BufferConsumptionStrategy.LAZY</code> - to always decode and process each and     every frame of the source (useful while processing video files - and default behaviour enforced by     <code>inference</code> if there is no explicit configuration) * <code>BufferFillingStrategy.DROP_OLDEST</code> and <code>BufferConsumptionStrategy.EAGER</code> - to always process the most     recent frames of source (useful while processing video streams when low latency [real-time experience]     is required - ADAPTIVE version of this is default for streams)</p> <p>ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as <code>DROP_OLDEST</code> and <code>DROP_LATEST</code> strategies, but there are two more conditions that may influence frame drop: * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner * consumption rate - in resource constraints environment, not only decoding is problematic from the performance perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just grabbed and dropped on the floor. ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases. Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source, reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients, with reasonable defaults being set.</p> <p><code>VideoSource</code> emits events regarding its activity - which can be intercepted by custom handlers. Take into account that they are always executed in context of thread invoking them (and should be fast to complete, otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.</p> <p><code>VideoSource</code> implementation is naturally multithreading, with different thread decoding video and different one consuming it and manipulating source state. Implementation of user interface is thread-safe, although stream it is meant to be consumed by a single thread only.</p> <p>ENV variables involved: * VIDEO_SOURCE_BUFFER_SIZE - default: 64 * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1 * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0 * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10 * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int]</code> <p>Either str with file or stream reference, or int representing device ID</p> required <code>buffer_size</code> <code>int</code> <p>size of decoding buffer</p> <code>DEFAULT_BUFFER_SIZE</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers for status updates</p> <code>None</code> <code>buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Settings for buffer filling strategy - if not given - automatic choice regarding source type will be applied</p> <code>None</code> <code>buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Settings for buffer consumption strategy, if not given - automatic choice regarding source type will be applied</p> <code>None</code> <code>adaptive_mode_stream_pace_tolerance</code> <code>float</code> <p>Maximum deviation between frames grabbing pace and stream pace that will not trigger adaptive mode frame drop</p> <code>DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE</code> <code>adaptive_mode_reader_pace_tolerance</code> <code>float</code> <p>Maximum deviation between decoding pace and stream consumption pace that will not trigger adaptive mode frame drop</p> <code>DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE</code> <code>minimum_adaptive_mode_samples</code> <code>int</code> <p>Minimal number of frames to be used to establish actual pace of processing, before adaptive mode can drop any frame</p> <code>DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES</code> <code>maximum_adaptive_frames_dropped_in_row</code> <code>int</code> <p>Maximum number of frames dropped in row due to application of adaptive strategy</p> <code>DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW</code> <code>video_source_properties</code> <code>Optional[dict[str, float]]</code> <p>Optional dictionary with video source properties corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.</p> <code>None</code> <code>source_id</code> <code>Optional[int]</code> <p>Optional identifier of video source - mainly useful to recognise specific source when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised to keep it unique within all sources in use.</p> <code>None</code> <p>Returns: Instance of <code>VideoSource</code> class</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_reference: Union[str, int],\n    buffer_size: int = DEFAULT_BUFFER_SIZE,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    adaptive_mode_stream_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE,\n    adaptive_mode_reader_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE,\n    minimum_adaptive_mode_samples: int = DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES,\n    maximum_adaptive_frames_dropped_in_row: int = DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    source_id: Optional[int] = None,\n):\n\"\"\"\n    This class is meant to represent abstraction over video sources - both video files and\n    on-line streams that are possible to be consumed and used by other components of `inference`\n    library.\n\n    Before digging into details of the class behaviour, it is advised to familiarise with the following\n    concepts and implementation assumptions:\n\n    1. Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by\n        its processing capabilities. If processing is faster than the frame rate of video, operations\n        may be executed in a time shorter than the time of video playback. In the opposite case - consumer\n        may freely decode and process frames in its own pace, without risk for failures due to temporal\n        dependencies of processing - this is classical offline processing example.\n    2. Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -\n        in other words - this is on-line processing example. Consumer being faster than incoming stream\n        frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.\n        Slow consumer, however, may not be able to process everything on time and to keep up with the pace\n        of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of\n        sync with the stream causing decoding failures or unpredictable behavior.\n\n    To fit those two types of video sources, `VideoSource` introduces the concept of buffered decoding of\n    video stream (like at the YouTube - player buffers some frames that are soon to be displayed).\n    The way on how buffer is filled and consumed dictates the behavior of `VideoSource`.\n\n    Starting from `BufferFillingStrategy` - we have 3 basic options:\n    * WAIT: in case of slow video consumption, when buffer is full - `VideoSource` will wait for\n    the empty spot in buffer before next frame will be processed - this is suitable in cases when\n    we want to ensure EACH FRAME of the video to be processed\n    * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped -\n    this is suitable for cases when we want to process the most recent frames possible\n    * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when\n    it is expected to have processing performance drops, but we would like to consume portions of\n    video that are locally smooth - but this is probably the least common use-case.\n\n    On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST,\n    which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion\n    of that mode will be described later.\n\n    Naturally, decoded frames must also be consumed. `VideoSource` provides a handy interface for reading\n    a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via\n    `BufferConsumptionStrategy`:\n    * LAZY - consume all the frames from decoding buffer one-by-one\n    * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent\n\n    In consequence - there are various combinations of `BufferFillingStrategy` and `BufferConsumptionStrategy`.\n    The most popular would be:\n    * `BufferFillingStrategy.WAIT` and `BufferConsumptionStrategy.LAZY` - to always decode and process each and\n        every frame of the source (useful while processing video files - and default behaviour enforced by\n        `inference` if there is no explicit configuration)\n    * `BufferFillingStrategy.DROP_OLDEST` and `BufferConsumptionStrategy.EAGER` - to always process the most\n        recent frames of source (useful while processing video streams when low latency [real-time experience]\n        is required - ADAPTIVE version of this is default for streams)\n\n    ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume\n    video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing\n    against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as `DROP_OLDEST`\n    and `DROP_LATEST` strategies, but there are two more conditions that may influence frame drop:\n    * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that\n    MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing\n    deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner\n    * consumption rate - in resource constraints environment, not only decoding is problematic from the performance\n    perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources\n    for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently\n    than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just\n    grabbed and dropped on the floor.\n    ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases.\n    Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source,\n    reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients,\n    with reasonable defaults being set.\n\n    `VideoSource` emits events regarding its activity - which can be intercepted by custom handlers. Take\n    into account that they are always executed in context of thread invoking them (and should be fast to complete,\n    otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.\n\n    `VideoSource` implementation is naturally multithreading, with different thread decoding video and different\n    one consuming it and manipulating source state. Implementation of user interface is thread-safe, although\n    stream it is meant to be consumed by a single thread only.\n\n    ENV variables involved:\n    * VIDEO_SOURCE_BUFFER_SIZE - default: 64\n    * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1\n    * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0\n    * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10\n    * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n\n    Args:\n        video_reference (Union[str, int]): Either str with file or stream reference, or int representing device ID\n        buffer_size (int): size of decoding buffer\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers for status updates\n        buffer_filling_strategy (Optional[BufferFillingStrategy]): Settings for buffer filling strategy - if not\n            given - automatic choice regarding source type will be applied\n        buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Settings for buffer consumption strategy,\n            if not given - automatic choice regarding source type will be applied\n        adaptive_mode_stream_pace_tolerance (float): Maximum deviation between frames grabbing pace and stream pace\n            that will not trigger adaptive mode frame drop\n        adaptive_mode_reader_pace_tolerance (float): Maximum deviation between decoding pace and stream consumption\n            pace that will not trigger adaptive mode frame drop\n        minimum_adaptive_mode_samples (int): Minimal number of frames to be used to establish actual pace of\n            processing, before adaptive mode can drop any frame\n        maximum_adaptive_frames_dropped_in_row (int): Maximum number of frames dropped in row due to application of\n            adaptive strategy\n        video_source_properties (Optional[dict[str, float]]): Optional dictionary with video source properties\n            corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.\n        source_id (Optional[int]): Optional identifier of video source - mainly useful to recognise specific source\n            when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised\n            to keep it unique within all sources in use.\n\n    Returns: Instance of `VideoSource` class\n    \"\"\"\n    frames_buffer = Queue(maxsize=buffer_size)\n    if status_update_handlers is None:\n        status_update_handlers = []\n    video_consumer = VideoConsumer.init(\n        buffer_filling_strategy=buffer_filling_strategy,\n        adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n        adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n        minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n        maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n        status_update_handlers=status_update_handlers,\n    )\n    return cls(\n        stream_reference=video_reference,\n        frames_buffer=frames_buffer,\n        status_update_handlers=status_update_handlers,\n        buffer_consumption_strategy=buffer_consumption_strategy,\n        video_consumer=video_consumer,\n        video_source_properties=video_source_properties,\n        source_id=source_id,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.mute","title":"<code>mute()</code>","text":"<p>Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing intermediate frames to be dropped. May be also used against files, although arguably less useful. Eligible to be used in states: [RUNNING] End state: * MUTED</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef mute(self) -&gt; None:\n\"\"\"\n    Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where\n    frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing\n    intermediate frames to be dropped. May be also used against files, although arguably less useful.\n    Eligible to be used in states:\n    [RUNNING]\n    End state:\n    * MUTED\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in MUTE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not MUTE stream in state: {self._state}\"\n        )\n    self._mute()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.pause","title":"<code>pause()</code>","text":"<p>Method to be used to pause source consumption. During pause - no new frames are consumed. Used on on-line streams for too long may cause stream disconnection. Eligible to be used in states: [RUNNING] End state: * PAUSED</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef pause(self) -&gt; None:\n\"\"\"\n    Method to be used to pause source consumption. During pause - no new frames are consumed.\n    Used on on-line streams for too long may cause stream disconnection.\n    Eligible to be used in states:\n    [RUNNING]\n    End state:\n    * PAUSED\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in PAUSE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not PAUSE stream in state: {self._state}\"\n        )\n    self._pause()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.read_frame","title":"<code>read_frame(timeout=None)</code>","text":"<p>Method to be used by the consumer to get decoded source frame.</p> <p>Throws:     * EndOfStreamError: when trying to get the frame from closed source.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def read_frame(self, timeout: Optional[float] = None) -&gt; Optional[VideoFrame]:\n\"\"\"\n    Method to be used by the consumer to get decoded source frame.\n\n    Returns: VideoFrame object with decoded frame and its metadata.\n    Throws:\n        * EndOfStreamError: when trying to get the frame from closed source.\n    \"\"\"\n    video_frame: Optional[Union[VideoFrame, str]] = get_from_queue(\n        queue=self._frames_buffer,\n        on_successful_read=self._video_consumer.notify_frame_consumed,\n        timeout=timeout,\n        purge=self._buffer_consumption_strategy is BufferConsumptionStrategy.EAGER,\n    )\n    if video_frame == POISON_PILL:\n        raise EndOfStreamError(\n            \"Attempted to retrieve frame from stream that already ended.\"\n        )\n    if video_frame is not None:\n        send_video_source_status_update(\n            severity=UpdateSeverity.DEBUG,\n            event_type=FRAME_CONSUMED_EVENT,\n            payload={\n                \"frame_timestamp\": video_frame.frame_timestamp,\n                \"frame_id\": video_frame.frame_id,\n                \"source_id\": video_frame.source_id,\n            },\n            status_update_handlers=self._status_update_handlers,\n        )\n    return video_frame\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.restart","title":"<code>restart(wait_on_frames_consumption=True, purge_frames_buffer=False)</code>","text":"<p>Method to restart source consumption. Eligible to be used in states: [MUTED, RUNNING, PAUSED, ENDED, ERROR]. End state: * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed * ERROR - if it was not possible to connect with source</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Parameters:</p> Name Type Description Default <code>wait_on_frames_consumption</code> <code>bool</code> <p>Flag telling if all frames from buffer must be consumed before completion of this operation.</p> <code>True</code> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source     * SourceConnectionError: if source cannot be connected</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef restart(\n    self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n) -&gt; None:\n\"\"\"\n    Method to restart source consumption. Eligible to be used in states:\n    [MUTED, RUNNING, PAUSED, ENDED, ERROR].\n    End state:\n    * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n    * ERROR - if it was not possible to connect with source\n\n    Thread safe - only one transition of states possible at the time.\n\n    Args:\n        wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n            completion of this operation.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        * SourceConnectionError: if source cannot be connected\n    \"\"\"\n    if self._state not in RESTART_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not RESTART stream in state: {self._state}\"\n        )\n    self._restart(\n        wait_on_frames_consumption=wait_on_frames_consumption,\n        purge_frames_buffer=purge_frames_buffer,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.resume","title":"<code>resume()</code>","text":"<p>Method to recover from pause or mute into running state. [PAUSED, MUTED] End state: * RUNNING</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef resume(self) -&gt; None:\n\"\"\"\n    Method to recover from pause or mute into running state.\n    [PAUSED, MUTED]\n    End state:\n    * RUNNING\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in RESUME_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not RESUME stream in state: {self._state}\"\n        )\n    self._resume()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.start","title":"<code>start()</code>","text":"<p>Method to be used to start source consumption. Eligible to be used in states: [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)] End state: * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed * ERROR - if it was not possible to connect with source</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source     * SourceConnectionError: if source cannot be connected</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef start(self) -&gt; None:\n\"\"\"\n    Method to be used to start source consumption. Eligible to be used in states:\n    [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)]\n    End state:\n    * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n    * ERROR - if it was not possible to connect with source\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        * SourceConnectionError: if source cannot be connected\n    \"\"\"\n    if self._state not in START_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not START stream in state: {self._state}\"\n        )\n    self._start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.terminate","title":"<code>terminate(wait_on_frames_consumption=True, purge_frames_buffer=False)</code>","text":"<p>Method to be used to terminate source consumption. Eligible to be used in states: [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)] End state: * ENDED - indicating success of the process * ERROR - if error with processing occurred</p> <p>Must be used to properly dispose resources at the end.</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Parameters:</p> Name Type Description Default <code>wait_on_frames_consumption</code> <code>bool</code> <p>Flag telling if all frames from buffer must be consumed before completion of this operation.</p> <code>True</code> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef terminate(\n    self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n) -&gt; None:\n\"\"\"\n    Method to be used to terminate source consumption. Eligible to be used in states:\n    [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)]\n    End state:\n    * ENDED - indicating success of the process\n    * ERROR - if error with processing occurred\n\n    Must be used to properly dispose resources at the end.\n\n    Thread safe - only one transition of states possible at the time.\n\n    Args:\n        wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n            completion of this operation.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in TERMINATE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not TERMINATE stream in state: {self._state}\"\n        )\n    self._terminate(\n        wait_on_frames_consumption=wait_on_frames_consumption,\n        purge_frames_buffer=purge_frames_buffer,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.get_from_queue","title":"<code>get_from_queue(queue, timeout=None, on_successful_read=lambda: None, purge=False)</code>","text":"<p>Function is supposed to take element from the queue waiting on the first element to appear using <code>timeout</code> parameter. One may ask to go to the very last element of the queue and return it - then <code>purge</code> should be set to True. No additional wait on new elements to appear happen and the purge stops once queue is free returning last element consumed. queue.task_done() and on_successful_read(...) will be called on each received element.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def get_from_queue(\n    queue: Queue,\n    timeout: Optional[float] = None,\n    on_successful_read: Callable[[], None] = lambda: None,\n    purge: bool = False,\n) -&gt; Optional[Any]:\n\"\"\"\n    Function is supposed to take element from the queue waiting on the first element to appear using `timeout`\n    parameter. One may ask to go to the very last element of the queue and return it - then `purge` should be set\n    to True. No additional wait on new elements to appear happen and the purge stops once queue is free returning last\n    element consumed.\n    queue.task_done() and on_successful_read(...) will be called on each received element.\n    \"\"\"\n    result = None\n    if queue.empty() or not purge:\n        try:\n            result = queue.get(timeout=timeout)\n            queue.task_done()\n            on_successful_read()\n        except Empty:\n            pass\n    while not queue.empty() and purge:\n        result = queue.get()\n        queue.task_done()\n        on_successful_read()\n    return result\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/http_api/","title":"http_api","text":""},{"location":"docs/reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.HttpInterface","title":"<code>HttpInterface</code>","text":"<p>             Bases: <code>BaseInterface</code></p> <p>Roboflow defined HTTP interface for a general-purpose inference server.</p> <p>This class sets up the FastAPI application and adds necessary middleware, as well as initializes the model manager and model registry for the inference server.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>class HttpInterface(BaseInterface):\n\"\"\"Roboflow defined HTTP interface for a general-purpose inference server.\n\n    This class sets up the FastAPI application and adds necessary middleware,\n    as well as initializes the model manager and model registry for the inference server.\n\n    Attributes:\n        app (FastAPI): The FastAPI application instance.\n        model_manager (ModelManager): The manager for handling different models.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: ModelManager,\n        root_path: Optional[str] = None,\n    ):\n\"\"\"\n        Initializes the HttpInterface with given model manager and model registry.\n\n        Args:\n            model_manager (ModelManager): The manager for handling different models.\n            root_path (Optional[str]): The root path for the FastAPI application.\n\n        Description:\n            Deploy Roboflow trained models to nearly any compute environment!\n        \"\"\"\n        description = \"Roboflow inference server\"\n        app = FastAPI(\n            title=\"Roboflow Inference Server\",\n            description=description,\n            version=__version__,\n            terms_of_service=\"https://roboflow.com/terms\",\n            contact={\n                \"name\": \"Roboflow Inc.\",\n                \"url\": \"https://roboflow.com/contact\",\n                \"email\": \"help@roboflow.com\",\n            },\n            license_info={\n                \"name\": \"Apache 2.0\",\n                \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n            },\n            root_path=root_path,\n        )\n        if METLO_KEY:\n            app.add_middleware(\n                ASGIMiddleware, host=\"https://app.metlo.com\", api_key=METLO_KEY\n            )\n\n        if len(ALLOW_ORIGINS) &gt; 0:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=ALLOW_ORIGINS,\n                allow_credentials=True,\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n\n        # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n        if PROFILE:\n            app.add_middleware(\n                CProfileMiddleware,\n                enable=True,\n                server_app=app,\n                filename=\"/profile/output.pstats\",\n                strip_dirs=False,\n                sort_by=\"cumulative\",\n            )\n        app.add_middleware(asgi_correlation_id.CorrelationIdMiddleware)\n\n        if METRICS_ENABLED:\n\n            @app.middleware(\"http\")\n            async def count_errors(request: Request, call_next):\n\"\"\"Middleware to count errors.\n\n                Args:\n                    request (Request): The incoming request.\n                    call_next (Callable): The next middleware or endpoint to call.\n\n                Returns:\n                    Response: The response from the next middleware or endpoint.\n                \"\"\"\n                response = await call_next(request)\n                if response.status_code &gt;= 400:\n                    self.model_manager.num_errors += 1\n                return response\n\n        self.app = app\n        self.model_manager = model_manager\n        self.workflows_active_learning_middleware = WorkflowsActiveLearningMiddleware(\n            cache=cache,\n        )\n\n        async def process_inference_request(\n            inference_request: InferenceRequest, **kwargs\n        ) -&gt; InferenceResponse:\n\"\"\"Processes an inference request by calling the appropriate model.\n\n            Args:\n                inference_request (InferenceRequest): The request containing model ID and other inference details.\n\n            Returns:\n                InferenceResponse: The response containing the inference results.\n            \"\"\"\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=inference_request.model_id\n            )\n            self.model_manager.add_model(de_aliased_model_id, inference_request.api_key)\n            resp = await self.model_manager.infer_from_request(\n                de_aliased_model_id, inference_request, **kwargs\n            )\n            return orjson_response(resp)\n\n        async def process_workflow_inference_request(\n            workflow_request: WorkflowInferenceRequest,\n            workflow_specification: dict,\n            background_tasks: Optional[BackgroundTasks],\n        ) -&gt; WorkflowInferenceResponse:\n            step_execution_mode = StepExecutionMode(WORKFLOWS_STEP_EXECUTION_MODE)\n            result = await compile_and_execute_async(\n                workflow_specification=workflow_specification,\n                runtime_parameters=workflow_request.inputs,\n                model_manager=model_manager,\n                api_key=workflow_request.api_key,\n                max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                step_execution_mode=step_execution_mode,\n                active_learning_middleware=self.workflows_active_learning_middleware,\n                background_tasks=background_tasks,\n            )\n            outputs = serialise_workflow_result(\n                result=result,\n                excluded_fields=workflow_request.excluded_fields,\n            )\n            response = WorkflowInferenceResponse(outputs=outputs)\n            return orjson_response(response=response)\n\n        def load_core_model(\n            inference_request: InferenceRequest,\n            api_key: Optional[str] = None,\n            core_model: str = None,\n        ) -&gt; None:\n\"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n            Args:\n                inference_request (InferenceRequest): The request containing version and other details.\n                api_key (Optional[str]): The API key for the request.\n                core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n\n            Returns:\n                str: The core model ID.\n            \"\"\"\n            if api_key:\n                inference_request.api_key = api_key\n            version_id_field = f\"{core_model}_version_id\"\n            core_model_id = (\n                f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n            )\n            self.model_manager.add_model(core_model_id, inference_request.api_key)\n            return core_model_id\n\n        load_clip_model = partial(load_core_model, core_model=\"clip\")\n\"\"\"Loads the CLIP model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The CLIP model ID.\n        \"\"\"\n\n        load_sam_model = partial(load_core_model, core_model=\"sam\")\n\"\"\"Loads the SAM model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The SAM model ID.\n        \"\"\"\n\n        load_gaze_model = partial(load_core_model, core_model=\"gaze\")\n\"\"\"Loads the GAZE model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The GAZE model ID.\n        \"\"\"\n\n        load_doctr_model = partial(load_core_model, core_model=\"doctr\")\n\"\"\"Loads the DocTR model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The DocTR model ID.\n        \"\"\"\n        load_cogvlm_model = partial(load_core_model, core_model=\"cogvlm\")\n\n        load_grounding_dino_model = partial(\n            load_core_model, core_model=\"grounding_dino\"\n        )\n\"\"\"Loads the Grounding DINO model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The Grounding DINO model ID.\n        \"\"\"\n\n        load_yolo_world_model = partial(load_core_model, core_model=\"yolo_world\")\n\"\"\"Loads the YOLO World model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The YOLO World model ID.\n        \"\"\"\n\n        @app.get(\n            \"/info\",\n            response_model=ServerVersionInfo,\n            summary=\"Info\",\n            description=\"Get the server name and version number\",\n        )\n        async def root():\n\"\"\"Endpoint to get the server name and version number.\n\n            Returns:\n                ServerVersionInfo: The server version information.\n            \"\"\"\n            return ServerVersionInfo(\n                name=\"Roboflow Inference Server\",\n                version=__version__,\n                uuid=GLOBAL_INFERENCE_SERVER_ID,\n            )\n\n        # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n        if not LAMBDA:\n\n            @app.get(\n                \"/model/registry\",\n                response_model=ModelsDescriptions,\n                summary=\"Get model keys\",\n                description=\"Get the ID of each loaded model\",\n            )\n            async def registry():\n\"\"\"Get the ID of each loaded model in the registry.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/registry\")\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/add\",\n                response_model=ModelsDescriptions,\n                summary=\"Load a model\",\n                description=\"Load the model with the given model ID\",\n            )\n            @with_route_exceptions\n            async def model_add(request: AddModelRequest):\n\"\"\"Load the model with the given model ID into the model manager.\n\n                Args:\n                    request (AddModelRequest): The request containing the model ID and optional API key.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/add\")\n                de_aliased_model_id = resolve_roboflow_model_alias(\n                    model_id=request.model_id\n                )\n                self.model_manager.add_model(de_aliased_model_id, request.api_key)\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/remove\",\n                response_model=ModelsDescriptions,\n                summary=\"Remove a model\",\n                description=\"Remove the model with the given model ID\",\n            )\n            @with_route_exceptions\n            async def model_remove(request: ClearModelRequest):\n\"\"\"Remove the model with the given model ID from the model manager.\n\n                Args:\n                    request (ClearModelRequest): The request containing the model ID to be removed.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/remove\")\n                de_aliased_model_id = resolve_roboflow_model_alias(\n                    model_id=request.model_id\n                )\n                self.model_manager.remove(de_aliased_model_id)\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/clear\",\n                response_model=ModelsDescriptions,\n                summary=\"Remove all models\",\n                description=\"Remove all loaded models\",\n            )\n            @with_route_exceptions\n            async def model_clear():\n\"\"\"Remove all loaded models from the model manager.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/clear\")\n                self.model_manager.clear()\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/infer/object_detection\",\n                response_model=Union[\n                    ObjectDetectionInferenceResponse,\n                    List[ObjectDetectionInferenceResponse],\n                    StubResponse,\n                ],\n                summary=\"Object detection infer\",\n                description=\"Run inference with the specified object detection model\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def infer_object_detection(\n                inference_request: ObjectDetectionInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ):\n\"\"\"Run inference with the specified object detection model.\n\n                Args:\n                    inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/object_detection\")\n                return await process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n\n            @app.post(\n                \"/infer/instance_segmentation\",\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse, StubResponse\n                ],\n                summary=\"Instance segmentation infer\",\n                description=\"Run inference with the specified instance segmentation model\",\n            )\n            @with_route_exceptions\n            async def infer_instance_segmentation(\n                inference_request: InstanceSegmentationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ):\n\"\"\"Run inference with the specified instance segmentation model.\n\n                Args:\n                    inference_request (InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    InstanceSegmentationInferenceResponse: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/instance_segmentation\")\n                return await process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n\n            @app.post(\n                \"/infer/classification\",\n                response_model=Union[\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                ],\n                summary=\"Classification infer\",\n                description=\"Run inference with the specified classification model\",\n            )\n            @with_route_exceptions\n            async def infer_classification(\n                inference_request: ClassificationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ):\n\"\"\"Run inference with the specified classification model.\n\n                Args:\n                    inference_request (ClassificationInferenceRequest): The request containing the necessary details for classification.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/classification\")\n                return await process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n\n            @app.post(\n                \"/infer/keypoints_detection\",\n                response_model=Union[KeypointsDetectionInferenceResponse, StubResponse],\n                summary=\"Keypoints detection infer\",\n                description=\"Run inference with the specified keypoints detection model\",\n            )\n            @with_route_exceptions\n            async def infer_keypoints(\n                inference_request: KeypointsDetectionInferenceRequest,\n            ):\n\"\"\"Run inference with the specified keypoints detection model.\n\n                Args:\n                    inference_request (KeypointsDetectionInferenceRequest): The request containing the necessary details for keypoints detection.\n\n                Returns:\n                    Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/keypoints_detection\")\n                return await process_inference_request(inference_request)\n\n        if not DISABLE_WORKFLOW_ENDPOINTS:\n\n            @app.post(\n                \"/infer/workflows/{workspace_name}/{workflow_name}\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"Endpoint to trigger inference from predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body\",\n            )\n            @with_route_exceptions\n            async def infer_from_predefined_workflow(\n                workspace_name: str,\n                workflow_name: str,\n                workflow_request: WorkflowInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ) -&gt; WorkflowInferenceResponse:\n                workflow_specification = get_workflow_specification(\n                    api_key=workflow_request.api_key,\n                    workspace_id=workspace_name,\n                    workflow_name=workflow_name,\n                )\n                return await process_workflow_inference_request(\n                    workflow_request=workflow_request,\n                    workflow_specification=workflow_specification,\n                    background_tasks=background_tasks if not LAMBDA else None,\n                )\n\n            @app.post(\n                \"/infer/workflows\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"Endpoint to trigger inference from workflow specification provided in payload\",\n                description=\"Parses and executes workflow specification, injecting runtime parameters from request body\",\n            )\n            @with_route_exceptions\n            async def infer_from_workflow(\n                workflow_request: WorkflowSpecificationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ) -&gt; WorkflowInferenceResponse:\n                workflow_specification = {\n                    \"specification\": workflow_request.specification\n                }\n                return await process_workflow_inference_request(\n                    workflow_request=workflow_request,\n                    workflow_specification=workflow_specification,\n                    background_tasks=background_tasks if not LAMBDA else None,\n                )\n\n        if CORE_MODELS_ENABLED:\n            if CORE_MODEL_CLIP_ENABLED:\n\n                @app.post(\n                    \"/clip/embed_image\",\n                    response_model=ClipEmbeddingResponse,\n                    summary=\"CLIP Image Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def clip_embed_image(\n                    inference_request: ClipImageEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Embeds image data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipEmbeddingResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/embed_image\")\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/embed_text\",\n                    response_model=ClipEmbeddingResponse,\n                    summary=\"CLIP Text Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed text data.\",\n                )\n                @with_route_exceptions\n                async def clip_embed_text(\n                    inference_request: ClipTextEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Embeds text data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipEmbeddingResponse: The response containing the embedded text.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/embed_text\")\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/compare\",\n                    response_model=ClipCompareResponse,\n                    summary=\"CLIP Compare\",\n                    description=\"Run the Open AI CLIP model to compute similarity scores.\",\n                )\n                @with_route_exceptions\n                async def clip_compare(\n                    inference_request: ClipCompareRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Computes similarity scores using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipCompareRequest): The request containing the data to be compared.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipCompareResponse: The response containing the similarity scores.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/compare\")\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor, n=2)\n                    return response\n\n            if CORE_MODEL_GROUNDINGDINO_ENABLED:\n\n                @app.post(\n                    \"/grounding_dino/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"Grounding DINO inference.\",\n                    description=\"Run the Grounding DINO zero-shot object detection model.\",\n                )\n                @with_route_exceptions\n                async def grounding_dino_infer(\n                    inference_request: GroundingDINOInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Embeds image data using the Grounding DINO model.\n\n                    Args:\n                        inference_request GroundingDINOInferenceRequest): The request containing the image on which to run object detection.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ObjectDetectionInferenceResponse: The object detection response.\n                    \"\"\"\n                    logger.debug(f\"Reached /grounding_dino/infer\")\n                    grounding_dino_model_id = load_grounding_dino_model(\n                        inference_request, api_key=api_key\n                    )\n                    response = await self.model_manager.infer_from_request(\n                        grounding_dino_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(grounding_dino_model_id, actor)\n                    return response\n\n            if CORE_MODEL_YOLO_WORLD_ENABLED:\n\n                @app.post(\n                    \"/yolo_world/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"YOLO-World inference.\",\n                    description=\"Run the YOLO-World zero-shot object detection model.\",\n                    response_model_exclude_none=True,\n                )\n                @with_route_exceptions\n                async def yolo_world_infer(\n                    inference_request: YOLOWorldInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Runs the YOLO-World zero-shot object detection model.\n\n                    Args:\n                        inference_request (YOLOWorldInferenceRequest): The request containing the image on which to run object detection.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ObjectDetectionInferenceResponse: The object detection response.\n                    \"\"\"\n                    logger.debug(f\"Reached /yolo_world/infer. Loading model\")\n                    yolo_world_model_id = load_yolo_world_model(\n                        inference_request, api_key=api_key\n                    )\n                    logger.debug(\"YOLOWorld model loaded. Staring the inference.\")\n                    response = await self.model_manager.infer_from_request(\n                        yolo_world_model_id, inference_request\n                    )\n                    logger.debug(\"YOLOWorld prediction available.\")\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(yolo_world_model_id, actor)\n                        logger.debug(\"Usage of YOLOWorld denoted.\")\n                    return response\n\n            if CORE_MODEL_DOCTR_ENABLED:\n\n                @app.post(\n                    \"/doctr/ocr\",\n                    response_model=DoctrOCRInferenceResponse,\n                    summary=\"DocTR OCR response\",\n                    description=\"Run the DocTR OCR model to retrieve text in an image.\",\n                )\n                @with_route_exceptions\n                async def doctr_retrieve_text(\n                    inference_request: DoctrOCRInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Embeds image data using the DocTR model.\n\n                    Args:\n                        inference_request (M.DoctrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.DoctrOCRInferenceResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /doctr/ocr\")\n                    doctr_model_id = load_doctr_model(\n                        inference_request, api_key=api_key\n                    )\n                    response = await self.model_manager.infer_from_request(\n                        doctr_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(doctr_model_id, actor)\n                    return response\n\n            if CORE_MODEL_SAM_ENABLED:\n\n                @app.post(\n                    \"/sam/embed_image\",\n                    response_model=SamEmbeddingResponse,\n                    summary=\"SAM Image Embeddings\",\n                    description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def sam_embed_image(\n                    inference_request: SamEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam/embed_image\")\n                    sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response.embeddings,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n                @app.post(\n                    \"/sam/segment_image\",\n                    response_model=SamSegmentationResponse,\n                    summary=\"SAM Image Segmentation\",\n                    description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n                )\n                @with_route_exceptions\n                async def sam_segment_image(\n                    inference_request: SamSegmentationRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamSegmentationRequest): The request containing the image to be segmented.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamSegmentationResponse or Response: The response containing the segmented image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam/segment_image\")\n                    sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n            if CORE_MODEL_GAZE_ENABLED:\n\n                @app.post(\n                    \"/gaze/gaze_detection\",\n                    response_model=List[GazeDetectionInferenceResponse],\n                    summary=\"Gaze Detection\",\n                    description=\"Run the gaze detection model to detect gaze.\",\n                )\n                @with_route_exceptions\n                async def gaze_detection(\n                    inference_request: GazeDetectionInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Detect gaze using the gaze detection model.\n\n                    Args:\n                        inference_request (M.GazeDetectionRequest): The request containing the image to be detected.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.GazeDetectionResponse: The response containing all the detected faces and the corresponding gazes.\n                    \"\"\"\n                    logger.debug(f\"Reached /gaze/gaze_detection\")\n                    gaze_model_id = load_gaze_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        gaze_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(gaze_model_id, actor)\n                    return response\n\n            if CORE_MODEL_COGVLM_ENABLED:\n\n                @app.post(\n                    \"/llm/cogvlm\",\n                    response_model=CogVLMResponse,\n                    summary=\"CogVLM\",\n                    description=\"Run the CogVLM model to chat or describe an image.\",\n                )\n                @with_route_exceptions\n                async def cog_vlm(\n                    inference_request: CogVLMInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n\"\"\"\n                    Chat with CogVLM or ask it about an image. Multi-image requests not currently supported.\n\n                    Args:\n                        inference_request (M.CogVLMInferenceRequest): The request containing the prompt and image to be described.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.CogVLMResponse: The model's text response\n                    \"\"\"\n                    logger.debug(f\"Reached /llm/cogvlm\")\n                    cog_model_id = load_cogvlm_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        cog_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(cog_model_id, actor)\n                    return response\n\n        if LEGACY_ROUTE_ENABLED:\n            # Legacy object detection inference path for backwards compatability\n            @app.post(\n                \"/{dataset_id}/{version_id}\",\n                # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse,\n                    KeypointsDetectionInferenceResponse,\n                    ObjectDetectionInferenceResponse,\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                    Any,\n                ],\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def legacy_infer_from_request(\n                background_tasks: BackgroundTasks,\n                request: Request,\n                dataset_id: str = Path(\n                    description=\"ID of a Roboflow dataset corresponding to the model to use for inference\"\n                ),\n                version_id: str = Path(\n                    description=\"ID of a Roboflow dataset version corresponding to the model to use for inference\"\n                ),\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                confidence: float = Query(\n                    0.4,\n                    description=\"The confidence threshold used to filter out predictions\",\n                ),\n                keypoint_confidence: float = Query(\n                    0.0,\n                    description=\"The confidence threshold used to filter out keypoints that are not visible based on model confidence\",\n                ),\n                format: str = Query(\n                    \"json\",\n                    description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n                ),\n                image: Optional[str] = Query(\n                    None,\n                    description=\"The publically accessible URL of an image to use for inference.\",\n                ),\n                image_type: Optional[str] = Query(\n                    \"base64\",\n                    description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n                ),\n                labels: Optional[bool] = Query(\n                    False,\n                    description=\"If true, labels will be include in any inference visualization.\",\n                ),\n                mask_decode_mode: Optional[str] = Query(\n                    \"accurate\",\n                    description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n                ),\n                tradeoff_factor: Optional[float] = Query(\n                    0.0,\n                    description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n                ),\n                max_detections: int = Query(\n                    300,\n                    description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n                ),\n                overlap: float = Query(\n                    0.3,\n                    description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n                ),\n                stroke: int = Query(\n                    1, description=\"The stroke width used when visualizing predictions\"\n                ),\n                countinference: Optional[bool] = Query(\n                    True,\n                    description=\"If false, does not track inference against usage.\",\n                    include_in_schema=False,\n                ),\n                service_secret: Optional[str] = Query(\n                    None,\n                    description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                    include_in_schema=False,\n                ),\n                disable_preproc_auto_orient: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic image orientation\"\n                ),\n                disable_preproc_contrast: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic contrast adjustment\"\n                ),\n                disable_preproc_grayscale: Optional[bool] = Query(\n                    False,\n                    description=\"If true, disables automatic grayscale conversion\",\n                ),\n                disable_preproc_static_crop: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic static crop\"\n                ),\n                disable_active_learning: Optional[bool] = Query(\n                    default=False,\n                    description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n                ),\n                active_learning_target_dataset: Optional[str] = Query(\n                    default=None,\n                    description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n                ),\n                source: Optional[str] = Query(\n                    \"external\",\n                    description=\"The source of the inference request\",\n                ),\n                source_info: Optional[str] = Query(\n                    \"external\",\n                    description=\"The detailed source information of the inference request\",\n                ),\n            ):\n\"\"\"\n                Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n                Args:\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    # Other parameters described in the function signature...\n\n                Returns:\n                    Union[InstanceSegmentationInferenceResponse, KeypointsDetectionInferenceRequest, ObjectDetectionInferenceResponse, ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n                \"\"\"\n                logger.debug(\n                    f\"Reached legacy route /:dataset_id/:version_id with {dataset_id}/{version_id}\"\n                )\n                model_id = f\"{dataset_id}/{version_id}\"\n\n                if confidence &gt;= 1:\n                    confidence /= 100\n                elif confidence &lt; 0.01:\n                    confidence = 0.01\n\n                if overlap &gt;= 1:\n                    overlap /= 100\n\n                if image is not None:\n                    request_image = InferenceRequestImage(type=\"url\", value=image)\n                else:\n                    if \"Content-Type\" not in request.headers:\n                        raise ContentTypeMissing(\n                            f\"Request must include a Content-Type header\"\n                        )\n                    if \"multipart/form-data\" in request.headers[\"Content-Type\"]:\n                        form_data = await request.form()\n                        base64_image_str = await form_data[\"file\"].read()\n                        base64_image_str = base64.b64encode(base64_image_str)\n                        request_image = InferenceRequestImage(\n                            type=\"base64\", value=base64_image_str.decode(\"ascii\")\n                        )\n                    elif (\n                        \"application/x-www-form-urlencoded\"\n                        in request.headers[\"Content-Type\"]\n                        or \"application/json\" in request.headers[\"Content-Type\"]\n                    ):\n                        data = await request.body()\n                        request_image = InferenceRequestImage(\n                            type=image_type, value=data\n                        )\n                    else:\n                        raise ContentTypeInvalid(\n                            f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                        )\n\n                if LAMBDA:\n                    request_model_id = (\n                        request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                            \"lambda\"\n                        ][\"model\"][\"endpoint\"]\n                        .replace(\"--\", \"/\")\n                        .replace(\"rf-\", \"\")\n                        .replace(\"nu-\", \"\")\n                    )\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"actor\"]\n                    if countinference:\n                        trackUsage(request_model_id, actor)\n                    else:\n                        if service_secret != ROBOFLOW_SERVICE_SECRET:\n                            raise MissingServiceSecretError(\n                                \"Service secret is required to disable inference usage tracking\"\n                            )\n                else:\n                    request_model_id = model_id\n                logger.debug(\n                    f\"State of model registry: {self.model_manager.describe_models()}\"\n                )\n                self.model_manager.add_model(\n                    request_model_id, api_key, model_id_alias=model_id\n                )\n\n                task_type = self.model_manager.get_task_type(model_id, api_key=api_key)\n                inference_request_type = ObjectDetectionInferenceRequest\n                args = dict()\n                if task_type == \"instance-segmentation\":\n                    inference_request_type = InstanceSegmentationInferenceRequest\n                    args = {\n                        \"mask_decode_mode\": mask_decode_mode,\n                        \"tradeoff_factor\": tradeoff_factor,\n                    }\n                elif task_type == \"classification\":\n                    inference_request_type = ClassificationInferenceRequest\n                elif task_type == \"keypoint-detection\":\n                    inference_request_type = KeypointsDetectionInferenceRequest\n                    args = {\"keypoint_confidence\": keypoint_confidence}\n                inference_request = inference_request_type(\n                    api_key=api_key,\n                    model_id=model_id,\n                    image=request_image,\n                    confidence=confidence,\n                    iou_threshold=overlap,\n                    max_detections=max_detections,\n                    visualization_labels=labels,\n                    visualization_stroke_width=stroke,\n                    visualize_predictions=True if format == \"image\" else False,\n                    disable_preproc_auto_orient=disable_preproc_auto_orient,\n                    disable_preproc_contrast=disable_preproc_contrast,\n                    disable_preproc_grayscale=disable_preproc_grayscale,\n                    disable_preproc_static_crop=disable_preproc_static_crop,\n                    disable_active_learning=disable_active_learning,\n                    active_learning_target_dataset=active_learning_target_dataset,\n                    source=source,\n                    source_info=source_info,\n                    **args,\n                )\n\n                inference_response = await self.model_manager.infer_from_request(\n                    inference_request.model_id,\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n                logger.debug(\"Response ready.\")\n                if format == \"image\":\n                    return Response(\n                        content=inference_response.visualization,\n                        media_type=\"image/jpeg\",\n                    )\n                else:\n                    return orjson_response(inference_response)\n\n        if not LAMBDA:\n            # Legacy clear cache endpoint for backwards compatability\n            @app.get(\"/clear_cache\", response_model=str)\n            async def legacy_clear_cache():\n\"\"\"\n                Clears the model cache.\n\n                This endpoint provides a way to clear the cache of loaded models.\n\n                Returns:\n                    str: A string indicating that the cache has been cleared.\n                \"\"\"\n                logger.debug(f\"Reached /clear_cache\")\n                await model_clear()\n                return \"Cache Cleared\"\n\n            # Legacy add model endpoint for backwards compatability\n            @app.get(\"/start/{dataset_id}/{version_id}\")\n            async def model_add(dataset_id: str, version_id: str, api_key: str = None):\n\"\"\"\n                Starts a model inference session.\n\n                This endpoint initializes and starts an inference session for the specified model version.\n\n                Args:\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                    api_key (str, optional): Roboflow API Key for artifact retrieval.\n\n                Returns:\n                    JSONResponse: A response object containing the status and a success message.\n                \"\"\"\n                logger.debug(\n                    f\"Reached /start/{dataset_id}/{version_id} with {dataset_id}/{version_id}\"\n                )\n                model_id = f\"{dataset_id}/{version_id}\"\n                self.model_manager.add_model(model_id, api_key)\n\n                return JSONResponse(\n                    {\n                        \"status\": 200,\n                        \"message\": \"inference session started from local memory.\",\n                    }\n                )\n\n        if not LAMBDA:\n\n            @app.get(\n                \"/notebook/start\",\n                summary=\"Jupyter Lab Server Start\",\n                description=\"Starts a jupyter lab server for running development code\",\n            )\n            @with_route_exceptions\n            async def notebook_start(browserless: bool = False):\n\"\"\"Starts a jupyter lab server for running development code.\n\n                Args:\n                    inference_request (NotebookStartRequest): The request containing the necessary details for starting a jupyter lab server.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    NotebookStartResponse: The response containing the URL of the jupyter lab server.\n                \"\"\"\n                logger.debug(f\"Reached /notebook/start\")\n                if NOTEBOOK_ENABLED:\n                    start_notebook()\n                    if browserless:\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Jupyter Lab server started at http://localhost:{NOTEBOOK_PORT}?token={NOTEBOOK_PASSWORD}\",\n                        }\n                    else:\n                        sleep(2)\n                        return RedirectResponse(\n                            f\"http://localhost:{NOTEBOOK_PORT}/lab/tree/quickstart.ipynb?token={NOTEBOOK_PASSWORD}\"\n                        )\n                else:\n                    if browserless:\n                        return {\n                            \"success\": False,\n                            \"message\": \"Notebook server is not enabled. Enable notebooks via the NOTEBOOK_ENABLED environment variable.\",\n                        }\n                    else:\n                        return RedirectResponse(f\"/notebook-instructions.html\")\n\n        app.mount(\n            \"/\",\n            StaticFiles(directory=\"./inference/landing/out\", html=True),\n            name=\"static\",\n        )\n\n    def run(self):\n        uvicorn.run(self.app, host=\"127.0.0.1\", port=8080)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.HttpInterface.__init__","title":"<code>__init__(model_manager, root_path=None)</code>","text":"<p>Initializes the HttpInterface with given model manager and model registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> required <code>root_path</code> <code>Optional[str]</code> <p>The root path for the FastAPI application.</p> <code>None</code> Description <p>Deploy Roboflow trained models to nearly any compute environment!</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def __init__(\n    self,\n    model_manager: ModelManager,\n    root_path: Optional[str] = None,\n):\n\"\"\"\n    Initializes the HttpInterface with given model manager and model registry.\n\n    Args:\n        model_manager (ModelManager): The manager for handling different models.\n        root_path (Optional[str]): The root path for the FastAPI application.\n\n    Description:\n        Deploy Roboflow trained models to nearly any compute environment!\n    \"\"\"\n    description = \"Roboflow inference server\"\n    app = FastAPI(\n        title=\"Roboflow Inference Server\",\n        description=description,\n        version=__version__,\n        terms_of_service=\"https://roboflow.com/terms\",\n        contact={\n            \"name\": \"Roboflow Inc.\",\n            \"url\": \"https://roboflow.com/contact\",\n            \"email\": \"help@roboflow.com\",\n        },\n        license_info={\n            \"name\": \"Apache 2.0\",\n            \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n        },\n        root_path=root_path,\n    )\n    if METLO_KEY:\n        app.add_middleware(\n            ASGIMiddleware, host=\"https://app.metlo.com\", api_key=METLO_KEY\n        )\n\n    if len(ALLOW_ORIGINS) &gt; 0:\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=ALLOW_ORIGINS,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n    # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n    if PROFILE:\n        app.add_middleware(\n            CProfileMiddleware,\n            enable=True,\n            server_app=app,\n            filename=\"/profile/output.pstats\",\n            strip_dirs=False,\n            sort_by=\"cumulative\",\n        )\n    app.add_middleware(asgi_correlation_id.CorrelationIdMiddleware)\n\n    if METRICS_ENABLED:\n\n        @app.middleware(\"http\")\n        async def count_errors(request: Request, call_next):\n\"\"\"Middleware to count errors.\n\n            Args:\n                request (Request): The incoming request.\n                call_next (Callable): The next middleware or endpoint to call.\n\n            Returns:\n                Response: The response from the next middleware or endpoint.\n            \"\"\"\n            response = await call_next(request)\n            if response.status_code &gt;= 400:\n                self.model_manager.num_errors += 1\n            return response\n\n    self.app = app\n    self.model_manager = model_manager\n    self.workflows_active_learning_middleware = WorkflowsActiveLearningMiddleware(\n        cache=cache,\n    )\n\n    async def process_inference_request(\n        inference_request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Processes an inference request by calling the appropriate model.\n\n        Args:\n            inference_request (InferenceRequest): The request containing model ID and other inference details.\n\n        Returns:\n            InferenceResponse: The response containing the inference results.\n        \"\"\"\n        de_aliased_model_id = resolve_roboflow_model_alias(\n            model_id=inference_request.model_id\n        )\n        self.model_manager.add_model(de_aliased_model_id, inference_request.api_key)\n        resp = await self.model_manager.infer_from_request(\n            de_aliased_model_id, inference_request, **kwargs\n        )\n        return orjson_response(resp)\n\n    async def process_workflow_inference_request(\n        workflow_request: WorkflowInferenceRequest,\n        workflow_specification: dict,\n        background_tasks: Optional[BackgroundTasks],\n    ) -&gt; WorkflowInferenceResponse:\n        step_execution_mode = StepExecutionMode(WORKFLOWS_STEP_EXECUTION_MODE)\n        result = await compile_and_execute_async(\n            workflow_specification=workflow_specification,\n            runtime_parameters=workflow_request.inputs,\n            model_manager=model_manager,\n            api_key=workflow_request.api_key,\n            max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n            step_execution_mode=step_execution_mode,\n            active_learning_middleware=self.workflows_active_learning_middleware,\n            background_tasks=background_tasks,\n        )\n        outputs = serialise_workflow_result(\n            result=result,\n            excluded_fields=workflow_request.excluded_fields,\n        )\n        response = WorkflowInferenceResponse(outputs=outputs)\n        return orjson_response(response=response)\n\n    def load_core_model(\n        inference_request: InferenceRequest,\n        api_key: Optional[str] = None,\n        core_model: str = None,\n    ) -&gt; None:\n\"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n        Args:\n            inference_request (InferenceRequest): The request containing version and other details.\n            api_key (Optional[str]): The API key for the request.\n            core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n\n        Returns:\n            str: The core model ID.\n        \"\"\"\n        if api_key:\n            inference_request.api_key = api_key\n        version_id_field = f\"{core_model}_version_id\"\n        core_model_id = (\n            f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n        )\n        self.model_manager.add_model(core_model_id, inference_request.api_key)\n        return core_model_id\n\n    load_clip_model = partial(load_core_model, core_model=\"clip\")\n\"\"\"Loads the CLIP model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The CLIP model ID.\n    \"\"\"\n\n    load_sam_model = partial(load_core_model, core_model=\"sam\")\n\"\"\"Loads the SAM model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The SAM model ID.\n    \"\"\"\n\n    load_gaze_model = partial(load_core_model, core_model=\"gaze\")\n\"\"\"Loads the GAZE model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The GAZE model ID.\n    \"\"\"\n\n    load_doctr_model = partial(load_core_model, core_model=\"doctr\")\n\"\"\"Loads the DocTR model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The DocTR model ID.\n    \"\"\"\n    load_cogvlm_model = partial(load_core_model, core_model=\"cogvlm\")\n\n    load_grounding_dino_model = partial(\n        load_core_model, core_model=\"grounding_dino\"\n    )\n\"\"\"Loads the Grounding DINO model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The Grounding DINO model ID.\n    \"\"\"\n\n    load_yolo_world_model = partial(load_core_model, core_model=\"yolo_world\")\n\"\"\"Loads the YOLO World model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The YOLO World model ID.\n    \"\"\"\n\n    @app.get(\n        \"/info\",\n        response_model=ServerVersionInfo,\n        summary=\"Info\",\n        description=\"Get the server name and version number\",\n    )\n    async def root():\n\"\"\"Endpoint to get the server name and version number.\n\n        Returns:\n            ServerVersionInfo: The server version information.\n        \"\"\"\n        return ServerVersionInfo(\n            name=\"Roboflow Inference Server\",\n            version=__version__,\n            uuid=GLOBAL_INFERENCE_SERVER_ID,\n        )\n\n    # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n    if not LAMBDA:\n\n        @app.get(\n            \"/model/registry\",\n            response_model=ModelsDescriptions,\n            summary=\"Get model keys\",\n            description=\"Get the ID of each loaded model\",\n        )\n        async def registry():\n\"\"\"Get the ID of each loaded model in the registry.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/registry\")\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/add\",\n            response_model=ModelsDescriptions,\n            summary=\"Load a model\",\n            description=\"Load the model with the given model ID\",\n        )\n        @with_route_exceptions\n        async def model_add(request: AddModelRequest):\n\"\"\"Load the model with the given model ID into the model manager.\n\n            Args:\n                request (AddModelRequest): The request containing the model ID and optional API key.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/add\")\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=request.model_id\n            )\n            self.model_manager.add_model(de_aliased_model_id, request.api_key)\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/remove\",\n            response_model=ModelsDescriptions,\n            summary=\"Remove a model\",\n            description=\"Remove the model with the given model ID\",\n        )\n        @with_route_exceptions\n        async def model_remove(request: ClearModelRequest):\n\"\"\"Remove the model with the given model ID from the model manager.\n\n            Args:\n                request (ClearModelRequest): The request containing the model ID to be removed.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/remove\")\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=request.model_id\n            )\n            self.model_manager.remove(de_aliased_model_id)\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/clear\",\n            response_model=ModelsDescriptions,\n            summary=\"Remove all models\",\n            description=\"Remove all loaded models\",\n        )\n        @with_route_exceptions\n        async def model_clear():\n\"\"\"Remove all loaded models from the model manager.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/clear\")\n            self.model_manager.clear()\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/infer/object_detection\",\n            response_model=Union[\n                ObjectDetectionInferenceResponse,\n                List[ObjectDetectionInferenceResponse],\n                StubResponse,\n            ],\n            summary=\"Object detection infer\",\n            description=\"Run inference with the specified object detection model\",\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        async def infer_object_detection(\n            inference_request: ObjectDetectionInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ):\n\"\"\"Run inference with the specified object detection model.\n\n            Args:\n                inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/object_detection\")\n            return await process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n\n        @app.post(\n            \"/infer/instance_segmentation\",\n            response_model=Union[\n                InstanceSegmentationInferenceResponse, StubResponse\n            ],\n            summary=\"Instance segmentation infer\",\n            description=\"Run inference with the specified instance segmentation model\",\n        )\n        @with_route_exceptions\n        async def infer_instance_segmentation(\n            inference_request: InstanceSegmentationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ):\n\"\"\"Run inference with the specified instance segmentation model.\n\n            Args:\n                inference_request (InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                InstanceSegmentationInferenceResponse: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/instance_segmentation\")\n            return await process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n\n        @app.post(\n            \"/infer/classification\",\n            response_model=Union[\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n            ],\n            summary=\"Classification infer\",\n            description=\"Run inference with the specified classification model\",\n        )\n        @with_route_exceptions\n        async def infer_classification(\n            inference_request: ClassificationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ):\n\"\"\"Run inference with the specified classification model.\n\n            Args:\n                inference_request (ClassificationInferenceRequest): The request containing the necessary details for classification.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/classification\")\n            return await process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n\n        @app.post(\n            \"/infer/keypoints_detection\",\n            response_model=Union[KeypointsDetectionInferenceResponse, StubResponse],\n            summary=\"Keypoints detection infer\",\n            description=\"Run inference with the specified keypoints detection model\",\n        )\n        @with_route_exceptions\n        async def infer_keypoints(\n            inference_request: KeypointsDetectionInferenceRequest,\n        ):\n\"\"\"Run inference with the specified keypoints detection model.\n\n            Args:\n                inference_request (KeypointsDetectionInferenceRequest): The request containing the necessary details for keypoints detection.\n\n            Returns:\n                Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/keypoints_detection\")\n            return await process_inference_request(inference_request)\n\n    if not DISABLE_WORKFLOW_ENDPOINTS:\n\n        @app.post(\n            \"/infer/workflows/{workspace_name}/{workflow_name}\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"Endpoint to trigger inference from predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body\",\n        )\n        @with_route_exceptions\n        async def infer_from_predefined_workflow(\n            workspace_name: str,\n            workflow_name: str,\n            workflow_request: WorkflowInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ) -&gt; WorkflowInferenceResponse:\n            workflow_specification = get_workflow_specification(\n                api_key=workflow_request.api_key,\n                workspace_id=workspace_name,\n                workflow_name=workflow_name,\n            )\n            return await process_workflow_inference_request(\n                workflow_request=workflow_request,\n                workflow_specification=workflow_specification,\n                background_tasks=background_tasks if not LAMBDA else None,\n            )\n\n        @app.post(\n            \"/infer/workflows\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"Endpoint to trigger inference from workflow specification provided in payload\",\n            description=\"Parses and executes workflow specification, injecting runtime parameters from request body\",\n        )\n        @with_route_exceptions\n        async def infer_from_workflow(\n            workflow_request: WorkflowSpecificationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ) -&gt; WorkflowInferenceResponse:\n            workflow_specification = {\n                \"specification\": workflow_request.specification\n            }\n            return await process_workflow_inference_request(\n                workflow_request=workflow_request,\n                workflow_specification=workflow_specification,\n                background_tasks=background_tasks if not LAMBDA else None,\n            )\n\n    if CORE_MODELS_ENABLED:\n        if CORE_MODEL_CLIP_ENABLED:\n\n            @app.post(\n                \"/clip/embed_image\",\n                response_model=ClipEmbeddingResponse,\n                summary=\"CLIP Image Embeddings\",\n                description=\"Run the Open AI CLIP model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def clip_embed_image(\n                inference_request: ClipImageEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Embeds image data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipEmbeddingResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /clip/embed_image\")\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/embed_text\",\n                response_model=ClipEmbeddingResponse,\n                summary=\"CLIP Text Embeddings\",\n                description=\"Run the Open AI CLIP model to embed text data.\",\n            )\n            @with_route_exceptions\n            async def clip_embed_text(\n                inference_request: ClipTextEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Embeds text data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipEmbeddingResponse: The response containing the embedded text.\n                \"\"\"\n                logger.debug(f\"Reached /clip/embed_text\")\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/compare\",\n                response_model=ClipCompareResponse,\n                summary=\"CLIP Compare\",\n                description=\"Run the Open AI CLIP model to compute similarity scores.\",\n            )\n            @with_route_exceptions\n            async def clip_compare(\n                inference_request: ClipCompareRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Computes similarity scores using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipCompareRequest): The request containing the data to be compared.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipCompareResponse: The response containing the similarity scores.\n                \"\"\"\n                logger.debug(f\"Reached /clip/compare\")\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor, n=2)\n                return response\n\n        if CORE_MODEL_GROUNDINGDINO_ENABLED:\n\n            @app.post(\n                \"/grounding_dino/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"Grounding DINO inference.\",\n                description=\"Run the Grounding DINO zero-shot object detection model.\",\n            )\n            @with_route_exceptions\n            async def grounding_dino_infer(\n                inference_request: GroundingDINOInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Embeds image data using the Grounding DINO model.\n\n                Args:\n                    inference_request GroundingDINOInferenceRequest): The request containing the image on which to run object detection.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ObjectDetectionInferenceResponse: The object detection response.\n                \"\"\"\n                logger.debug(f\"Reached /grounding_dino/infer\")\n                grounding_dino_model_id = load_grounding_dino_model(\n                    inference_request, api_key=api_key\n                )\n                response = await self.model_manager.infer_from_request(\n                    grounding_dino_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(grounding_dino_model_id, actor)\n                return response\n\n        if CORE_MODEL_YOLO_WORLD_ENABLED:\n\n            @app.post(\n                \"/yolo_world/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"YOLO-World inference.\",\n                description=\"Run the YOLO-World zero-shot object detection model.\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def yolo_world_infer(\n                inference_request: YOLOWorldInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Runs the YOLO-World zero-shot object detection model.\n\n                Args:\n                    inference_request (YOLOWorldInferenceRequest): The request containing the image on which to run object detection.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ObjectDetectionInferenceResponse: The object detection response.\n                \"\"\"\n                logger.debug(f\"Reached /yolo_world/infer. Loading model\")\n                yolo_world_model_id = load_yolo_world_model(\n                    inference_request, api_key=api_key\n                )\n                logger.debug(\"YOLOWorld model loaded. Staring the inference.\")\n                response = await self.model_manager.infer_from_request(\n                    yolo_world_model_id, inference_request\n                )\n                logger.debug(\"YOLOWorld prediction available.\")\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(yolo_world_model_id, actor)\n                    logger.debug(\"Usage of YOLOWorld denoted.\")\n                return response\n\n        if CORE_MODEL_DOCTR_ENABLED:\n\n            @app.post(\n                \"/doctr/ocr\",\n                response_model=DoctrOCRInferenceResponse,\n                summary=\"DocTR OCR response\",\n                description=\"Run the DocTR OCR model to retrieve text in an image.\",\n            )\n            @with_route_exceptions\n            async def doctr_retrieve_text(\n                inference_request: DoctrOCRInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Embeds image data using the DocTR model.\n\n                Args:\n                    inference_request (M.DoctrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.DoctrOCRInferenceResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /doctr/ocr\")\n                doctr_model_id = load_doctr_model(\n                    inference_request, api_key=api_key\n                )\n                response = await self.model_manager.infer_from_request(\n                    doctr_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(doctr_model_id, actor)\n                return response\n\n        if CORE_MODEL_SAM_ENABLED:\n\n            @app.post(\n                \"/sam/embed_image\",\n                response_model=SamEmbeddingResponse,\n                summary=\"SAM Image Embeddings\",\n                description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def sam_embed_image(\n                inference_request: SamEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /sam/embed_image\")\n                sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response.embeddings,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n            @app.post(\n                \"/sam/segment_image\",\n                response_model=SamSegmentationResponse,\n                summary=\"SAM Image Segmentation\",\n                description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n            )\n            @with_route_exceptions\n            async def sam_segment_image(\n                inference_request: SamSegmentationRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamSegmentationRequest): The request containing the image to be segmented.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamSegmentationResponse or Response: The response containing the segmented image.\n                \"\"\"\n                logger.debug(f\"Reached /sam/segment_image\")\n                sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n        if CORE_MODEL_GAZE_ENABLED:\n\n            @app.post(\n                \"/gaze/gaze_detection\",\n                response_model=List[GazeDetectionInferenceResponse],\n                summary=\"Gaze Detection\",\n                description=\"Run the gaze detection model to detect gaze.\",\n            )\n            @with_route_exceptions\n            async def gaze_detection(\n                inference_request: GazeDetectionInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Detect gaze using the gaze detection model.\n\n                Args:\n                    inference_request (M.GazeDetectionRequest): The request containing the image to be detected.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.GazeDetectionResponse: The response containing all the detected faces and the corresponding gazes.\n                \"\"\"\n                logger.debug(f\"Reached /gaze/gaze_detection\")\n                gaze_model_id = load_gaze_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    gaze_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(gaze_model_id, actor)\n                return response\n\n        if CORE_MODEL_COGVLM_ENABLED:\n\n            @app.post(\n                \"/llm/cogvlm\",\n                response_model=CogVLMResponse,\n                summary=\"CogVLM\",\n                description=\"Run the CogVLM model to chat or describe an image.\",\n            )\n            @with_route_exceptions\n            async def cog_vlm(\n                inference_request: CogVLMInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n\"\"\"\n                Chat with CogVLM or ask it about an image. Multi-image requests not currently supported.\n\n                Args:\n                    inference_request (M.CogVLMInferenceRequest): The request containing the prompt and image to be described.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.CogVLMResponse: The model's text response\n                \"\"\"\n                logger.debug(f\"Reached /llm/cogvlm\")\n                cog_model_id = load_cogvlm_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    cog_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(cog_model_id, actor)\n                return response\n\n    if LEGACY_ROUTE_ENABLED:\n        # Legacy object detection inference path for backwards compatability\n        @app.post(\n            \"/{dataset_id}/{version_id}\",\n            # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n            response_model=Union[\n                InstanceSegmentationInferenceResponse,\n                KeypointsDetectionInferenceResponse,\n                ObjectDetectionInferenceResponse,\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n                Any,\n            ],\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        async def legacy_infer_from_request(\n            background_tasks: BackgroundTasks,\n            request: Request,\n            dataset_id: str = Path(\n                description=\"ID of a Roboflow dataset corresponding to the model to use for inference\"\n            ),\n            version_id: str = Path(\n                description=\"ID of a Roboflow dataset version corresponding to the model to use for inference\"\n            ),\n            api_key: Optional[str] = Query(\n                None,\n                description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n            ),\n            confidence: float = Query(\n                0.4,\n                description=\"The confidence threshold used to filter out predictions\",\n            ),\n            keypoint_confidence: float = Query(\n                0.0,\n                description=\"The confidence threshold used to filter out keypoints that are not visible based on model confidence\",\n            ),\n            format: str = Query(\n                \"json\",\n                description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n            ),\n            image: Optional[str] = Query(\n                None,\n                description=\"The publically accessible URL of an image to use for inference.\",\n            ),\n            image_type: Optional[str] = Query(\n                \"base64\",\n                description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n            ),\n            labels: Optional[bool] = Query(\n                False,\n                description=\"If true, labels will be include in any inference visualization.\",\n            ),\n            mask_decode_mode: Optional[str] = Query(\n                \"accurate\",\n                description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n            ),\n            tradeoff_factor: Optional[float] = Query(\n                0.0,\n                description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n            ),\n            max_detections: int = Query(\n                300,\n                description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n            ),\n            overlap: float = Query(\n                0.3,\n                description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n            ),\n            stroke: int = Query(\n                1, description=\"The stroke width used when visualizing predictions\"\n            ),\n            countinference: Optional[bool] = Query(\n                True,\n                description=\"If false, does not track inference against usage.\",\n                include_in_schema=False,\n            ),\n            service_secret: Optional[str] = Query(\n                None,\n                description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                include_in_schema=False,\n            ),\n            disable_preproc_auto_orient: Optional[bool] = Query(\n                False, description=\"If true, disables automatic image orientation\"\n            ),\n            disable_preproc_contrast: Optional[bool] = Query(\n                False, description=\"If true, disables automatic contrast adjustment\"\n            ),\n            disable_preproc_grayscale: Optional[bool] = Query(\n                False,\n                description=\"If true, disables automatic grayscale conversion\",\n            ),\n            disable_preproc_static_crop: Optional[bool] = Query(\n                False, description=\"If true, disables automatic static crop\"\n            ),\n            disable_active_learning: Optional[bool] = Query(\n                default=False,\n                description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n            ),\n            active_learning_target_dataset: Optional[str] = Query(\n                default=None,\n                description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n            ),\n            source: Optional[str] = Query(\n                \"external\",\n                description=\"The source of the inference request\",\n            ),\n            source_info: Optional[str] = Query(\n                \"external\",\n                description=\"The detailed source information of the inference request\",\n            ),\n        ):\n\"\"\"\n            Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n            Args:\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference.\n                api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                # Other parameters described in the function signature...\n\n            Returns:\n                Union[InstanceSegmentationInferenceResponse, KeypointsDetectionInferenceRequest, ObjectDetectionInferenceResponse, ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n            \"\"\"\n            logger.debug(\n                f\"Reached legacy route /:dataset_id/:version_id with {dataset_id}/{version_id}\"\n            )\n            model_id = f\"{dataset_id}/{version_id}\"\n\n            if confidence &gt;= 1:\n                confidence /= 100\n            elif confidence &lt; 0.01:\n                confidence = 0.01\n\n            if overlap &gt;= 1:\n                overlap /= 100\n\n            if image is not None:\n                request_image = InferenceRequestImage(type=\"url\", value=image)\n            else:\n                if \"Content-Type\" not in request.headers:\n                    raise ContentTypeMissing(\n                        f\"Request must include a Content-Type header\"\n                    )\n                if \"multipart/form-data\" in request.headers[\"Content-Type\"]:\n                    form_data = await request.form()\n                    base64_image_str = await form_data[\"file\"].read()\n                    base64_image_str = base64.b64encode(base64_image_str)\n                    request_image = InferenceRequestImage(\n                        type=\"base64\", value=base64_image_str.decode(\"ascii\")\n                    )\n                elif (\n                    \"application/x-www-form-urlencoded\"\n                    in request.headers[\"Content-Type\"]\n                    or \"application/json\" in request.headers[\"Content-Type\"]\n                ):\n                    data = await request.body()\n                    request_image = InferenceRequestImage(\n                        type=image_type, value=data\n                    )\n                else:\n                    raise ContentTypeInvalid(\n                        f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                    )\n\n            if LAMBDA:\n                request_model_id = (\n                    request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"model\"][\"endpoint\"]\n                    .replace(\"--\", \"/\")\n                    .replace(\"rf-\", \"\")\n                    .replace(\"nu-\", \"\")\n                )\n                actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                    \"lambda\"\n                ][\"actor\"]\n                if countinference:\n                    trackUsage(request_model_id, actor)\n                else:\n                    if service_secret != ROBOFLOW_SERVICE_SECRET:\n                        raise MissingServiceSecretError(\n                            \"Service secret is required to disable inference usage tracking\"\n                        )\n            else:\n                request_model_id = model_id\n            logger.debug(\n                f\"State of model registry: {self.model_manager.describe_models()}\"\n            )\n            self.model_manager.add_model(\n                request_model_id, api_key, model_id_alias=model_id\n            )\n\n            task_type = self.model_manager.get_task_type(model_id, api_key=api_key)\n            inference_request_type = ObjectDetectionInferenceRequest\n            args = dict()\n            if task_type == \"instance-segmentation\":\n                inference_request_type = InstanceSegmentationInferenceRequest\n                args = {\n                    \"mask_decode_mode\": mask_decode_mode,\n                    \"tradeoff_factor\": tradeoff_factor,\n                }\n            elif task_type == \"classification\":\n                inference_request_type = ClassificationInferenceRequest\n            elif task_type == \"keypoint-detection\":\n                inference_request_type = KeypointsDetectionInferenceRequest\n                args = {\"keypoint_confidence\": keypoint_confidence}\n            inference_request = inference_request_type(\n                api_key=api_key,\n                model_id=model_id,\n                image=request_image,\n                confidence=confidence,\n                iou_threshold=overlap,\n                max_detections=max_detections,\n                visualization_labels=labels,\n                visualization_stroke_width=stroke,\n                visualize_predictions=True if format == \"image\" else False,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n                disable_active_learning=disable_active_learning,\n                active_learning_target_dataset=active_learning_target_dataset,\n                source=source,\n                source_info=source_info,\n                **args,\n            )\n\n            inference_response = await self.model_manager.infer_from_request(\n                inference_request.model_id,\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n            logger.debug(\"Response ready.\")\n            if format == \"image\":\n                return Response(\n                    content=inference_response.visualization,\n                    media_type=\"image/jpeg\",\n                )\n            else:\n                return orjson_response(inference_response)\n\n    if not LAMBDA:\n        # Legacy clear cache endpoint for backwards compatability\n        @app.get(\"/clear_cache\", response_model=str)\n        async def legacy_clear_cache():\n\"\"\"\n            Clears the model cache.\n\n            This endpoint provides a way to clear the cache of loaded models.\n\n            Returns:\n                str: A string indicating that the cache has been cleared.\n            \"\"\"\n            logger.debug(f\"Reached /clear_cache\")\n            await model_clear()\n            return \"Cache Cleared\"\n\n        # Legacy add model endpoint for backwards compatability\n        @app.get(\"/start/{dataset_id}/{version_id}\")\n        async def model_add(dataset_id: str, version_id: str, api_key: str = None):\n\"\"\"\n            Starts a model inference session.\n\n            This endpoint initializes and starts an inference session for the specified model version.\n\n            Args:\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                api_key (str, optional): Roboflow API Key for artifact retrieval.\n\n            Returns:\n                JSONResponse: A response object containing the status and a success message.\n            \"\"\"\n            logger.debug(\n                f\"Reached /start/{dataset_id}/{version_id} with {dataset_id}/{version_id}\"\n            )\n            model_id = f\"{dataset_id}/{version_id}\"\n            self.model_manager.add_model(model_id, api_key)\n\n            return JSONResponse(\n                {\n                    \"status\": 200,\n                    \"message\": \"inference session started from local memory.\",\n                }\n            )\n\n    if not LAMBDA:\n\n        @app.get(\n            \"/notebook/start\",\n            summary=\"Jupyter Lab Server Start\",\n            description=\"Starts a jupyter lab server for running development code\",\n        )\n        @with_route_exceptions\n        async def notebook_start(browserless: bool = False):\n\"\"\"Starts a jupyter lab server for running development code.\n\n            Args:\n                inference_request (NotebookStartRequest): The request containing the necessary details for starting a jupyter lab server.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                NotebookStartResponse: The response containing the URL of the jupyter lab server.\n            \"\"\"\n            logger.debug(f\"Reached /notebook/start\")\n            if NOTEBOOK_ENABLED:\n                start_notebook()\n                if browserless:\n                    return {\n                        \"success\": True,\n                        \"message\": f\"Jupyter Lab server started at http://localhost:{NOTEBOOK_PORT}?token={NOTEBOOK_PASSWORD}\",\n                    }\n                else:\n                    sleep(2)\n                    return RedirectResponse(\n                        f\"http://localhost:{NOTEBOOK_PORT}/lab/tree/quickstart.ipynb?token={NOTEBOOK_PASSWORD}\"\n                    )\n            else:\n                if browserless:\n                    return {\n                        \"success\": False,\n                        \"message\": \"Notebook server is not enabled. Enable notebooks via the NOTEBOOK_ENABLED environment variable.\",\n                    }\n                else:\n                    return RedirectResponse(f\"/notebook-instructions.html\")\n\n    app.mount(\n        \"/\",\n        StaticFiles(directory=\"./inference/landing/out\", html=True),\n        name=\"static\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.with_route_exceptions","title":"<code>with_route_exceptions(route)</code>","text":"<p>A decorator that wraps a FastAPI route to handle specific exceptions. If an exception is caught, it returns a JSON response with the error message.</p> <p>Parameters:</p> Name Type Description Default <code>route</code> <code>Callable</code> <p>The FastAPI route to be wrapped.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The wrapped route.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def with_route_exceptions(route):\n\"\"\"\n    A decorator that wraps a FastAPI route to handle specific exceptions. If an exception\n    is caught, it returns a JSON response with the error message.\n\n    Args:\n        route (Callable): The FastAPI route to be wrapped.\n\n    Returns:\n        Callable: The wrapped route.\n    \"\"\"\n\n    @wraps(route)\n    async def wrapped_route(*args, **kwargs):\n        try:\n            return await route(*args, **kwargs)\n        except ContentTypeInvalid:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid Content-Type header provided with request.\"\n                },\n            )\n            traceback.print_exc()\n        except ContentTypeMissing:\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Content-Type header not provided with request.\"},\n            )\n            traceback.print_exc()\n        except InputImageLoadError as e:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": f\"Could not load input image. Cause: {e.get_public_error_details()}\"\n                },\n            )\n            traceback.print_exc()\n        except InvalidModelIDError:\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Invalid Model ID sent in request.\"},\n            )\n            traceback.print_exc()\n        except InvalidMaskDecodeArgument:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid mask decode argument sent. tradeoff_factor must be in [0.0, 1.0], \"\n                    \"mask_decode_mode: must be one of ['accurate', 'fast', 'tradeoff']\"\n                },\n            )\n            traceback.print_exc()\n        except MissingApiKeyError:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Required Roboflow API key is missing. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n            traceback.print_exc()\n        except RuntimePayloadError as e:\n            resp = JSONResponse(\n                status_code=400, content={\"message\": e.get_public_message()}\n            )\n            traceback.print_exc()\n        except RoboflowAPINotAuthorizedError:\n            resp = JSONResponse(\n                status_code=401,\n                content={\n                    \"message\": \"Unauthorized access to roboflow API - check API key and make sure the key is valid for \"\n                    \"workspace you use. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n            traceback.print_exc()\n        except (RoboflowAPINotNotFoundError, InferenceModelNotFound):\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": \"Requested Roboflow resource not found. Make sure that workspace, project or model \"\n                    \"you referred in request exists.\"\n                },\n            )\n            traceback.print_exc()\n        except (\n            InvalidEnvironmentVariableError,\n            MissingServiceSecretError,\n            ServiceConfigurationError,\n        ):\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Service misconfiguration.\"}\n            )\n            traceback.print_exc()\n        except (\n            PreProcessingError,\n            PostProcessingError,\n        ):\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": \"Model configuration related to pre- or post-processing is invalid.\"\n                },\n            )\n            traceback.print_exc()\n        except ModelArtefactError:\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Model package is broken.\"}\n            )\n            traceback.print_exc()\n        except (\n            WorkflowsCompilerError,\n            ExecutionEngineError,\n        ) as e:\n            resp = JSONResponse(\n                status_code=500, content={\"message\": e.get_public_message()}\n            )\n            traceback.print_exc()\n        except OnnxProviderNotAvailable:\n            resp = JSONResponse(\n                status_code=501,\n                content={\n                    \"message\": \"Could not find requested ONNX Runtime Provider. Check that you are using \"\n                    \"the correct docker image on a supported device.\"\n                },\n            )\n            traceback.print_exc()\n        except (\n            MalformedRoboflowAPIResponseError,\n            RoboflowAPIUnsuccessfulRequestError,\n            WorkspaceLoadError,\n            MalformedWorkflowResponseError,\n        ):\n            resp = JSONResponse(\n                status_code=502,\n                content={\"message\": \"Internal error. Request to Roboflow API failed.\"},\n            )\n            traceback.print_exc()\n        except RoboflowAPIConnectionError:\n            resp = JSONResponse(\n                status_code=503,\n                content={\n                    \"message\": \"Internal error. Could not connect to Roboflow API.\"\n                },\n            )\n            traceback.print_exc()\n        except Exception:\n            resp = JSONResponse(status_code=500, content={\"message\": \"Internal error.\"})\n            traceback.print_exc()\n        return resp\n\n    return wrapped_route\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/orjson_utils/","title":"orjson_utils","text":""},{"location":"docs/reference/inference/core/interfaces/stream/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/","title":"inference_pipeline","text":""},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline","title":"<code>InferencePipeline</code>","text":"Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>class InferencePipeline:\n    @classmethod\n    def init(\n        cls,\n        video_reference: Union[str, int, List[Union[str, int]]],\n        model_id: str,\n        on_prediction: SinkHandler = None,\n        api_key: Optional[str] = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        class_agnostic_nms: Optional[bool] = None,\n        confidence: Optional[float] = None,\n        iou_threshold: Optional[float] = None,\n        max_candidates: Optional[int] = None,\n        max_detections: Optional[int] = None,\n        mask_decode_mode: Optional[str] = \"accurate\",\n        tradeoff_factor: Optional[float] = 0.0,\n        active_learning_enabled: Optional[bool] = None,\n        video_source_properties: Optional[\n            Union[Dict[str, float], List[Optional[Dict[str, float]]]]\n        ] = None,\n        active_learning_target_dataset: Optional[str] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ) -&gt; \"InferencePipeline\":\n\"\"\"\n        This class creates the abstraction for making inferences from Roboflow models against video stream.\n        It allows to choose model from Roboflow platform and run predictions against\n        video streams - just by the price of specifying which model to use and what to do with predictions.\n\n        It allows to set the model post-processing parameters (via .init() or env) and intercept updates\n        related to state of pipeline via `PipelineWatchDog` abstraction (although that is something probably\n        useful only for advanced use-cases).\n\n        For maximum efficiency, all separate chunks of processing: video decoding, inference, results dispatching\n        are handled by separate threads.\n\n        Given that reference to stream is passed and connectivity is lost - it attempts to re-connect with delay.\n\n        Since version 0.9.11 it works not only for object detection models but is also compatible with stubs,\n        classification, instance-segmentation and keypoint-detection models.\n\n        Since version 0.9.18, `InferencePipeline` is capable of handling multiple video sources at once. If multiple\n        sources are provided - source multiplexing will happen. One of the change introduced in that release is switch\n        from `get_video_frames_generator(...)` as video frames provider into `multiplex_videos(...)`. For a single\n        video source, the behaviour of `InferencePipeline` is remained unchanged when default parameters are used.\n        For multiple videos - frames are multiplexed, and we can adjust the pipeline behaviour using new configuration\n        options. `batch_collection_timeout` is one of the new option - it is the parameter of `multiplex_videos(...)`\n        that dictates how long the batch frames collection process may wait for all sources to provide video frame.\n        It can be set infinite (None) or with specific value representing fraction of second. We advise that value to\n        be set in production solutions to avoid processing slow-down caused by source with unstable latency spikes.\n        For more information on multiplexing process - please visit `multiplex_videos(...)` function docs.\n        Another change is the way on how sinks work. They can work in `SinkMode.ADAPTIVE` - which means that\n        video frames and predictions will be either provided to sink as list of objects, or specific elements -\n        and the determining factor is number of sources (it will behave SEQUENTIAL for one source and BATCH if multiple\n        ones are provided). All old sinks were adjusted to work in both modes, custom ones should be migrated\n        to reflect changes in sink function signature.\n\n        Args:\n            model_id (str): Name and version of model at Roboflow platform (example: \"my-model/3\")\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n                predictions against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with standard\n                Roboflow model prediction (different for specific types of models).\n            api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n                and \"API_KEY\" variables. API key, passed in some form is required.\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n                on single machine making tradeoff between number of frames and number of streams handled. Disabled\n                by default.\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n            confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CONFIDENCE\" with default \"0.5\"\n            iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"IOU_THRESHOLD\" with default \"0.5\"\n            max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_CANDIDATES\" with default \"3000\"\n            max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_DETECTIONS\" with default \"300\"\n            mask_decode_mode: (Optional[str]): Parameter of model post-processing. If not given - model \"accurate\" is\n                used. Applicable for instance segmentation models\n            tradeoff_factor (Optional[float]): Parameter of model post-processing. If not 0.0 - model default is used.\n                Applicable for instance segmentation models\n            active_learning_enabled (Optional[bool]): Flag to enable / disable Active Learning middleware (setting it\n                true does not guarantee any data to be collected, as data collection is controlled by Roboflow backend -\n                it just enables middleware intercepting predictions). If not given, env variable\n                `ACTIVE_LEARNING_ENABLED` will be used. Please point out that Active Learning will be forcefully\n                disabled in a scenario when Roboflow API key is not given, as Roboflow account is required\n                for this feature to be operational.\n            video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n                Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n                cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n                It is optional and if provided can be provided as single dict (applicable for all sources) or\n                as list of configs. Then the list must be of length of `video_reference` and may also contain None\n                values to denote that specific source should remain not configured.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            active_learning_target_dataset (Optional[str]): Parameter to be used when Active Learning data registration\n                should happen against different dataset than the one pointed by model_id\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n                handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n                in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n                in the order of video sources - with None values in the place of vide_frames / predictions that\n                were skipped due to `batch_collection_timeout`.\n                `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n                against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n                To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n                `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n                old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n                prediction element.\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n        * ACTIVE_LEARNING_ENABLED - controls Active Learning middleware if explicit parameter not given\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        if api_key is None:\n            api_key = API_KEY\n        inference_config = ModelConfig.init(\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            mask_decode_mode=mask_decode_mode,\n            tradeoff_factor=tradeoff_factor,\n        )\n        model = get_model(model_id=model_id, api_key=api_key)\n        on_video_frame = partial(\n            default_process_frame, model=model, inference_config=inference_config\n        )\n        active_learning_middleware = NullActiveLearningMiddleware()\n        if active_learning_enabled is None:\n            logger.info(\n                f\"`active_learning_enabled` parameter not set - using env `ACTIVE_LEARNING_ENABLED` \"\n                f\"with value: {ACTIVE_LEARNING_ENABLED}\"\n            )\n            active_learning_enabled = ACTIVE_LEARNING_ENABLED\n        if api_key is None:\n            logger.info(\n                f\"Roboflow API key not given - Active Learning is forced to be disabled.\"\n            )\n            active_learning_enabled = False\n        if active_learning_enabled is True:\n            resolved_model_id = resolve_roboflow_model_alias(model_id=model_id)\n            target_dataset = (\n                active_learning_target_dataset or resolved_model_id.split(\"/\")[0]\n            )\n            active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n                api_key=api_key,\n                target_dataset=target_dataset,\n                model_id=resolved_model_id,\n                cache=cache,\n            )\n            al_sink = partial(\n                active_learning_sink,\n                active_learning_middleware=active_learning_middleware,\n                model_type=model.task_type,\n                disable_preproc_auto_orient=DISABLE_PREPROC_AUTO_ORIENT,\n            )\n            logger.info(\n                \"AL enabled - wrapping `on_prediction` with multi_sink() and active_learning_sink()\"\n            )\n            on_prediction = partial(multi_sink, sinks=[on_prediction, al_sink])\n        on_pipeline_start = active_learning_middleware.start_registration_thread\n        on_pipeline_end = active_learning_middleware.stop_registration_thread\n        return InferencePipeline.init_with_custom_logic(\n            video_reference=video_reference,\n            on_video_frame=on_video_frame,\n            on_prediction=on_prediction,\n            on_pipeline_start=on_pipeline_start,\n            on_pipeline_end=on_pipeline_end,\n            max_fps=max_fps,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            video_source_properties=video_source_properties,\n            batch_collection_timeout=batch_collection_timeout,\n            sink_mode=sink_mode,\n        )\n\n    @classmethod\n    def init_with_yolo_world(\n        cls,\n        video_reference: Union[str, int, List[Union[str, int]]],\n        classes: List[str],\n        model_size: str = \"s\",\n        on_prediction: SinkHandler = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        class_agnostic_nms: Optional[bool] = None,\n        confidence: Optional[float] = None,\n        iou_threshold: Optional[float] = None,\n        max_candidates: Optional[int] = None,\n        max_detections: Optional[int] = None,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ) -&gt; \"InferencePipeline\":\n\"\"\"\n        This class creates the abstraction for making inferences from YoloWorld against video stream.\n        The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n        method.\n\n        Args:\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n                predictions against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            classes (List[str]): List of classes to execute zero-shot detection against\n            model_size (str): version of model - to be chosen from `s`, `m`, `l`\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with standard\n                Roboflow Object Detection prediction.\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n                on single machine making tradeoff between number of frames and number of streams handled. Disabled\n                by default.\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n            confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CONFIDENCE\" with default \"0.5\"\n            iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"IOU_THRESHOLD\" with default \"0.5\"\n            max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_CANDIDATES\" with default \"3000\"\n            max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_DETECTIONS\" with default \"300\"\n            video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n                Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n                cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n                It is optional and if provided can be provided as single dict (applicable for all sources) or\n                as list of configs. Then the list must be of length of `video_reference` and may also contain None\n                values to denote that specific source should remain not configured.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n                handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n                in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n                in the order of video sources - with None values in the place of vide_frames / predictions that\n                were skipped due to `batch_collection_timeout`.\n                `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n                against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n                To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n                `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n                old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n                prediction element.\n\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        inference_config = ModelConfig.init(\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n        )\n        try:\n            from inference.core.interfaces.stream.model_handlers.yolo_world import (\n                build_yolo_world_inference_function,\n            )\n\n            on_video_frame = build_yolo_world_inference_function(\n                model_id=f\"yolo_world/{model_size}\",\n                classes=classes,\n                inference_config=inference_config,\n            )\n        except ImportError as error:\n            raise CannotInitialiseModelError(\n                f\"Could not initialise yolo_world/{model_size} due to lack of sufficient dependencies. \"\n                f\"Use pip install inference[yolo-world] to install missing dependencies and try again.\"\n            ) from error\n        return InferencePipeline.init_with_custom_logic(\n            video_reference=video_reference,\n            on_video_frame=on_video_frame,\n            on_prediction=on_prediction,\n            on_pipeline_start=None,\n            on_pipeline_end=None,\n            max_fps=max_fps,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            video_source_properties=video_source_properties,\n            batch_collection_timeout=batch_collection_timeout,\n            sink_mode=sink_mode,\n        )\n\n    @classmethod\n    def init_with_workflow(\n        cls,\n        video_reference: Union[str, int],\n        workflow_specification: dict,\n        api_key: Optional[str] = None,\n        image_input_name: str = \"image\",\n        workflows_parameters: Optional[Dict[str, Any]] = None,\n        on_prediction: SinkHandler = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        video_source_properties: Optional[Dict[str, float]] = None,\n    ) -&gt; \"InferencePipeline\":\n\"\"\"\n        This class creates the abstraction for making inferences from given workflow against video stream.\n        The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n        method.\n\n        Args:\n            video_reference (Union[str, int]): Reference of source to be used to make predictions against.\n                It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles).\n            workflow_specification (dict): Valid specification of workflow. See [workflow docs](https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows)\n            api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n                and \"API_KEY\" variables. API key, passed in some form is required.\n            image_input_name (str): Name of input image defined in `workflow_specification`. `InferencePipeline` will be\n                injecting video frames to workflow through that parameter name.\n            workflows_parameters (Optional[Dict[str, Any]]): Dictionary with additional parameters that can be\n                defined within `workflow_specification`.\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with workflow output.\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n                on single machine making tradeoff between number of frames and number of streams handled. Disabled\n                by default.\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            video_source_properties (Optional[dict[str, float]]): Optional source properties to set up the video source,\n                corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source\n                will be used.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        if issubclass(type(video_reference), list) and len(list) &gt; 1:\n            raise ValueError(\n                \"Usage of workflows and `InferencePipeline` is experimental feature for now. We do not support \"\n                \"multiple video sources yet.\"\n            )\n        try:\n            from inference.core.interfaces.stream.model_handlers.workflows import (\n                run_video_frame_through_workflow,\n            )\n            from inference.enterprise.workflows.complier.steps_executors.active_learning_middlewares import (\n                WorkflowsActiveLearningMiddleware,\n            )\n\n            workflows_active_learning_middleware = WorkflowsActiveLearningMiddleware(\n                cache=cache,\n            )\n            model_registry = RoboflowModelRegistry(ROBOFLOW_MODEL_TYPES)\n            model_manager = BackgroundTaskActiveLearningManager(\n                model_registry=model_registry, cache=cache\n            )\n            model_manager = WithFixedSizeCache(\n                model_manager,\n                max_size=MAX_ACTIVE_MODELS,\n            )\n            if api_key is None:\n                api_key = API_KEY\n            background_tasks = BackgroundTasks()\n            on_video_frame = partial(\n                run_video_frame_through_workflow,\n                workflow_specification=workflow_specification,\n                model_manager=model_manager,\n                image_input_name=image_input_name,\n                workflows_parameters=workflows_parameters,\n                api_key=api_key,\n                workflows_active_learning_middleware=workflows_active_learning_middleware,\n                background_tasks=background_tasks,\n            )\n        except ImportError as error:\n            raise CannotInitialiseModelError(\n                f\"Could not initialise workflow processing due to lack of dependencies required. \"\n                f\"Please provide an issue report under https://github.com/roboflow/inference/issues\"\n            ) from error\n        return InferencePipeline.init_with_custom_logic(\n            video_reference=video_reference,\n            on_video_frame=on_video_frame,\n            on_prediction=on_prediction,\n            on_pipeline_start=None,\n            on_pipeline_end=None,\n            max_fps=max_fps,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            video_source_properties=video_source_properties,\n        )\n\n    @classmethod\n    def init_with_custom_logic(\n        cls,\n        video_reference: Union[str, int, List[Union[str, int]]],\n        on_video_frame: InferenceHandler,\n        on_prediction: SinkHandler = None,\n        on_pipeline_start: Optional[Callable[[], None]] = None,\n        on_pipeline_end: Optional[Callable[[], None]] = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ) -&gt; \"InferencePipeline\":\n\"\"\"\n        This class creates the abstraction for making inferences from given workflow against video stream.\n        The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initialiser\n        method.\n\n        Args:\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n                predictions against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            on_video_frame (Callable[[VideoFrame], AnyPrediction]): function supposed to make prediction (or do another\n                kind of custom processing according to your will). Accept `VideoFrame` object and is supposed\n                to return dictionary with results of any kind.\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with output from your\n                custom callable `on_video_frame(...)`. Logic here must be adjusted to the output of `on_video_frame`.\n            on_pipeline_start (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n                whenever pipeline starts\n            on_pipeline_end (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n                whenever pipeline ends\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n                on single machine making tradeoff between number of frames and number of streams handled. Disabled\n                by default.\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n            video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n                Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n                cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n                It is optional and if provided can be provided as single dict (applicable for all sources) or\n                as list of configs. Then the list must be of length of `video_reference` and may also contain None\n                values to denote that specific source should remain not configured.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n                handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n                in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n                in the order of video sources - with None values in the place of vide_frames / predictions that\n                were skipped due to `batch_collection_timeout`.\n                `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n                against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n                To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n                `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n                old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n                prediction element.\n\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        if watchdog is None:\n            watchdog = NullPipelineWatchdog()\n        if status_update_handlers is None:\n            status_update_handlers = []\n        status_update_handlers.append(watchdog.on_status_update)\n        video_sources = prepare_video_sources(\n            video_reference=video_reference,\n            video_source_properties=video_source_properties,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        )\n        watchdog.register_video_sources(video_sources=video_sources)\n        predictions_queue = Queue(maxsize=PREDICTIONS_QUEUE_SIZE)\n        return cls(\n            on_video_frame=on_video_frame,\n            video_sources=video_sources,\n            predictions_queue=predictions_queue,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            on_prediction=on_prediction,\n            max_fps=max_fps,\n            on_pipeline_start=on_pipeline_start,\n            on_pipeline_end=on_pipeline_end,\n            batch_collection_timeout=batch_collection_timeout,\n            sink_mode=sink_mode,\n        )\n\n    def __init__(\n        self,\n        on_video_frame: InferenceHandler,\n        video_sources: List[VideoSource],\n        predictions_queue: Queue,\n        watchdog: PipelineWatchDog,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        on_prediction: SinkHandler = None,\n        on_pipeline_start: Optional[Callable[[], None]] = None,\n        on_pipeline_end: Optional[Callable[[], None]] = None,\n        max_fps: Optional[float] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ):\n        self._on_video_frame = on_video_frame\n        self._video_sources = video_sources\n        self._on_prediction = on_prediction\n        self._max_fps = max_fps\n        self._predictions_queue = predictions_queue\n        self._watchdog = watchdog\n        self._command_handler_thread: Optional[Thread] = None\n        self._inference_thread: Optional[Thread] = None\n        self._dispatching_thread: Optional[Thread] = None\n        self._stop = False\n        self._camera_restart_ongoing = False\n        self._status_update_handlers = status_update_handlers\n        self._on_pipeline_start = on_pipeline_start\n        self._on_pipeline_end = on_pipeline_end\n        self._batch_collection_timeout = batch_collection_timeout\n        self._sink_mode = sink_mode\n\n    def start(self, use_main_thread: bool = True) -&gt; None:\n        self._stop = False\n        self._inference_thread = Thread(target=self._execute_inference)\n        self._inference_thread.start()\n        if self._on_pipeline_start is not None:\n            self._on_pipeline_start()\n        if use_main_thread:\n            self._dispatch_inference_results()\n        else:\n            self._dispatching_thread = Thread(target=self._dispatch_inference_results)\n            self._dispatching_thread.start()\n\n    def terminate(self) -&gt; None:\n        self._stop = True\n        for video_source in self._video_sources:\n            video_source.terminate(\n                wait_on_frames_consumption=False, purge_frames_buffer=True\n            )\n\n    def pause_stream(self, source_id: Optional[int] = None) -&gt; None:\n        for video_source in self._video_sources:\n            if video_source.source_id == source_id or source_id is None:\n                video_source.pause()\n\n    def mute_stream(self, source_id: Optional[int] = None) -&gt; None:\n        for video_source in self._video_sources:\n            if video_source.source_id == source_id or source_id is None:\n                video_source.mute()\n\n    def resume_stream(self, source_id: Optional[int] = None) -&gt; None:\n        for video_source in self._video_sources:\n            if video_source.source_id == source_id or source_id is None:\n                video_source.resume()\n\n    def join(self) -&gt; None:\n        if self._inference_thread is not None:\n            self._inference_thread.join()\n            self._inference_thread = None\n        if self._dispatching_thread is not None:\n            self._dispatching_thread.join()\n            self._dispatching_thread = None\n        if self._on_pipeline_end is not None:\n            self._on_pipeline_end()\n\n    def _execute_inference(self) -&gt; None:\n        send_inference_pipeline_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=INFERENCE_THREAD_STARTED_EVENT,\n            status_update_handlers=self._status_update_handlers,\n        )\n        logger.info(f\"Inference thread started\")\n        try:\n            for video_frames in self._generate_frames():\n                self._watchdog.on_model_inference_started(\n                    frames=video_frames,\n                )\n                predictions = self._on_video_frame(video_frames)\n                self._watchdog.on_model_prediction_ready(\n                    frames=video_frames,\n                )\n                self._predictions_queue.put((predictions, video_frames))\n                send_inference_pipeline_status_update(\n                    severity=UpdateSeverity.DEBUG,\n                    event_type=INFERENCE_COMPLETED_EVENT,\n                    payload={\n                        \"frames_ids\": [f.frame_id for f in video_frames],\n                        \"frames_timestamps\": [f.frame_timestamp for f in video_frames],\n                        \"sources_id\": [f.source_id for f in video_frames],\n                    },\n                    status_update_handlers=self._status_update_handlers,\n                )\n\n        except Exception as error:\n            payload = {\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"inference_thread\",\n            }\n            send_inference_pipeline_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=INFERENCE_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.exception(f\"Encountered inference error: {error}\")\n        finally:\n            self._predictions_queue.put(None)\n            send_inference_pipeline_status_update(\n                severity=UpdateSeverity.INFO,\n                event_type=INFERENCE_THREAD_FINISHED_EVENT,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.info(f\"Inference thread finished\")\n\n    def _dispatch_inference_results(self) -&gt; None:\n        while True:\n            inference_results: Optional[\n                Tuple[List[AnyPrediction], List[VideoFrame]]\n            ] = self._predictions_queue.get()\n            if inference_results is None:\n                self._predictions_queue.task_done()\n                break\n            predictions, video_frames = inference_results\n            if self._on_prediction is not None:\n                self._handle_predictions_dispatching(\n                    predictions=predictions,\n                    video_frames=video_frames,\n                )\n            self._predictions_queue.task_done()\n\n    def _handle_predictions_dispatching(\n        self,\n        predictions: List[AnyPrediction],\n        video_frames: List[VideoFrame],\n    ) -&gt; None:\n        if self._should_use_batch_sink():\n            self._use_batch_sink(predictions, video_frames)\n            return None\n        for frame_predictions, video_frame in zip(predictions, video_frames):\n            self._use_sink(frame_predictions, video_frame)\n\n    def _should_use_batch_sink(self) -&gt; bool:\n        return self._sink_mode is SinkMode.BATCH or (\n            self._sink_mode is SinkMode.ADAPTIVE and len(self._video_sources) &gt; 1\n        )\n\n    def _use_batch_sink(\n        self,\n        predictions: List[AnyPrediction],\n        video_frames: List[VideoFrame],\n    ) -&gt; None:\n        # This function makes it possible to always call sinks with payloads aligned to order of\n        # video sources - marking empty frames as None\n        results_by_source_id = {\n            video_frame.source_id: (frame_predictions, video_frame)\n            for frame_predictions, video_frame in zip(predictions, video_frames)\n        }\n        source_id_aligned_sink_payload = [\n            results_by_source_id.get(video_source.source_id, (None, None))\n            for video_source in self._video_sources\n        ]\n        source_id_aligned_predictions = [e[0] for e in source_id_aligned_sink_payload]\n        source_id_aligned_frames = [e[1] for e in source_id_aligned_sink_payload]\n        self._use_sink(\n            predictions=source_id_aligned_predictions,\n            video_frames=source_id_aligned_frames,\n        )\n\n    def _use_sink(\n        self,\n        predictions: Union[AnyPrediction, List[Optional[AnyPrediction]]],\n        video_frames: Union[VideoFrame, List[Optional[VideoFrame]]],\n    ) -&gt; None:\n        try:\n            self._on_prediction(predictions, video_frames)\n        except Exception as error:\n            payload = {\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"inference_results_dispatching\",\n            }\n            send_inference_pipeline_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=INFERENCE_RESULTS_DISPATCHING_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.warning(f\"Error in results dispatching - {error}\")\n\n    def _generate_frames(\n        self,\n    ) -&gt; Generator[List[VideoFrame], None, None]:\n        for video_source in self._video_sources:\n            video_source.start()\n        yield from multiplex_videos(\n            videos=self._video_sources,\n            max_fps=self._max_fps,\n            batch_collection_timeout=self._batch_collection_timeout,\n            should_stop=lambda: self._stop,\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init","title":"<code>init(video_reference, model_id, on_prediction=None, api_key=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, class_agnostic_nms=None, confidence=None, iou_threshold=None, max_candidates=None, max_detections=None, mask_decode_mode='accurate', tradeoff_factor=0.0, active_learning_enabled=None, video_source_properties=None, active_learning_target_dataset=None, batch_collection_timeout=None, sink_mode=SinkMode.ADAPTIVE)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from Roboflow models against video stream. It allows to choose model from Roboflow platform and run predictions against video streams - just by the price of specifying which model to use and what to do with predictions.</p> <p>It allows to set the model post-processing parameters (via .init() or env) and intercept updates related to state of pipeline via <code>PipelineWatchDog</code> abstraction (although that is something probably useful only for advanced use-cases).</p> <p>For maximum efficiency, all separate chunks of processing: video decoding, inference, results dispatching are handled by separate threads.</p> <p>Given that reference to stream is passed and connectivity is lost - it attempts to re-connect with delay.</p> <p>Since version 0.9.11 it works not only for object detection models but is also compatible with stubs, classification, instance-segmentation and keypoint-detection models.</p> <p>Since version 0.9.18, <code>InferencePipeline</code> is capable of handling multiple video sources at once. If multiple sources are provided - source multiplexing will happen. One of the change introduced in that release is switch from <code>get_video_frames_generator(...)</code> as video frames provider into <code>multiplex_videos(...)</code>. For a single video source, the behaviour of <code>InferencePipeline</code> is remained unchanged when default parameters are used. For multiple videos - frames are multiplexed, and we can adjust the pipeline behaviour using new configuration options. <code>batch_collection_timeout</code> is one of the new option - it is the parameter of <code>multiplex_videos(...)</code> that dictates how long the batch frames collection process may wait for all sources to provide video frame. It can be set infinite (None) or with specific value representing fraction of second. We advise that value to be set in production solutions to avoid processing slow-down caused by source with unstable latency spikes. For more information on multiplexing process - please visit <code>multiplex_videos(...)</code> function docs. Another change is the way on how sinks work. They can work in <code>SinkMode.ADAPTIVE</code> - which means that video frames and predictions will be either provided to sink as list of objects, or specific elements - and the determining factor is number of sources (it will behave SEQUENTIAL for one source and BATCH if multiple ones are provided). All old sinks were adjusted to work in both modes, custom ones should be migrated to reflect changes in sink function signature.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name and version of model at Roboflow platform (example: \"my-model/3\")</p> required <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source or sources to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with standard Roboflow model prediction (different for specific types of models).</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\" and \"API_KEY\" variables. API key, passed in some form is required.</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines on single machine making tradeoff between number of frames and number of streams handled. Disabled by default.</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CONFIDENCE\" with default \"0.5\"</p> <code>None</code> <code>iou_threshold</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"IOU_THRESHOLD\" with default \"0.5\"</p> <code>None</code> <code>max_candidates</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_CANDIDATES\" with default \"3000\"</p> <code>None</code> <code>max_detections</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_DETECTIONS\" with default \"300\"</p> <code>None</code> <code>mask_decode_mode</code> <code>Optional[str]</code> <p>(Optional[str]): Parameter of model post-processing. If not given - model \"accurate\" is used. Applicable for instance segmentation models</p> <code>'accurate'</code> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not 0.0 - model default is used. Applicable for instance segmentation models</p> <code>0.0</code> <code>active_learning_enabled</code> <code>Optional[bool]</code> <p>Flag to enable / disable Active Learning middleware (setting it true does not guarantee any data to be collected, as data collection is controlled by Roboflow backend - it just enables middleware intercepting predictions). If not given, env variable <code>ACTIVE_LEARNING_ENABLED</code> will be used. Please point out that Active Learning will be forcefully disabled in a scenario when Roboflow API key is not given, as Roboflow account is required for this feature to be operational.</p> <code>None</code> <code>video_source_properties</code> <code>Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. It is optional and if provided can be provided as single dict (applicable for all sources) or as list of configs. Then the list must be of length of <code>video_reference</code> and may also contain None values to denote that specific source should remain not configured. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>active_learning_target_dataset</code> <code>Optional[str]</code> <p>Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>sink_mode</code> <code>SinkMode</code> <p>Parameter that controls how video frames and predictions will be passed to sink handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink, in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>. <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos - sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p> <code>ADAPTIVE</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop * ACTIVE_LEARNING_ENABLED - controls Active Learning middleware if explicit parameter not given</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_reference: Union[str, int, List[Union[str, int]]],\n    model_id: str,\n    on_prediction: SinkHandler = None,\n    api_key: Optional[str] = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    class_agnostic_nms: Optional[bool] = None,\n    confidence: Optional[float] = None,\n    iou_threshold: Optional[float] = None,\n    max_candidates: Optional[int] = None,\n    max_detections: Optional[int] = None,\n    mask_decode_mode: Optional[str] = \"accurate\",\n    tradeoff_factor: Optional[float] = 0.0,\n    active_learning_enabled: Optional[bool] = None,\n    video_source_properties: Optional[\n        Union[Dict[str, float], List[Optional[Dict[str, float]]]]\n    ] = None,\n    active_learning_target_dataset: Optional[str] = None,\n    batch_collection_timeout: Optional[float] = None,\n    sink_mode: SinkMode = SinkMode.ADAPTIVE,\n) -&gt; \"InferencePipeline\":\n\"\"\"\n    This class creates the abstraction for making inferences from Roboflow models against video stream.\n    It allows to choose model from Roboflow platform and run predictions against\n    video streams - just by the price of specifying which model to use and what to do with predictions.\n\n    It allows to set the model post-processing parameters (via .init() or env) and intercept updates\n    related to state of pipeline via `PipelineWatchDog` abstraction (although that is something probably\n    useful only for advanced use-cases).\n\n    For maximum efficiency, all separate chunks of processing: video decoding, inference, results dispatching\n    are handled by separate threads.\n\n    Given that reference to stream is passed and connectivity is lost - it attempts to re-connect with delay.\n\n    Since version 0.9.11 it works not only for object detection models but is also compatible with stubs,\n    classification, instance-segmentation and keypoint-detection models.\n\n    Since version 0.9.18, `InferencePipeline` is capable of handling multiple video sources at once. If multiple\n    sources are provided - source multiplexing will happen. One of the change introduced in that release is switch\n    from `get_video_frames_generator(...)` as video frames provider into `multiplex_videos(...)`. For a single\n    video source, the behaviour of `InferencePipeline` is remained unchanged when default parameters are used.\n    For multiple videos - frames are multiplexed, and we can adjust the pipeline behaviour using new configuration\n    options. `batch_collection_timeout` is one of the new option - it is the parameter of `multiplex_videos(...)`\n    that dictates how long the batch frames collection process may wait for all sources to provide video frame.\n    It can be set infinite (None) or with specific value representing fraction of second. We advise that value to\n    be set in production solutions to avoid processing slow-down caused by source with unstable latency spikes.\n    For more information on multiplexing process - please visit `multiplex_videos(...)` function docs.\n    Another change is the way on how sinks work. They can work in `SinkMode.ADAPTIVE` - which means that\n    video frames and predictions will be either provided to sink as list of objects, or specific elements -\n    and the determining factor is number of sources (it will behave SEQUENTIAL for one source and BATCH if multiple\n    ones are provided). All old sinks were adjusted to work in both modes, custom ones should be migrated\n    to reflect changes in sink function signature.\n\n    Args:\n        model_id (str): Name and version of model at Roboflow platform (example: \"my-model/3\")\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n            predictions against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with standard\n            Roboflow model prediction (different for specific types of models).\n        api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n            and \"API_KEY\" variables. API key, passed in some form is required.\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n            on single machine making tradeoff between number of frames and number of streams handled. Disabled\n            by default.\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n        confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CONFIDENCE\" with default \"0.5\"\n        iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"IOU_THRESHOLD\" with default \"0.5\"\n        max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_CANDIDATES\" with default \"3000\"\n        max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_DETECTIONS\" with default \"300\"\n        mask_decode_mode: (Optional[str]): Parameter of model post-processing. If not given - model \"accurate\" is\n            used. Applicable for instance segmentation models\n        tradeoff_factor (Optional[float]): Parameter of model post-processing. If not 0.0 - model default is used.\n            Applicable for instance segmentation models\n        active_learning_enabled (Optional[bool]): Flag to enable / disable Active Learning middleware (setting it\n            true does not guarantee any data to be collected, as data collection is controlled by Roboflow backend -\n            it just enables middleware intercepting predictions). If not given, env variable\n            `ACTIVE_LEARNING_ENABLED` will be used. Please point out that Active Learning will be forcefully\n            disabled in a scenario when Roboflow API key is not given, as Roboflow account is required\n            for this feature to be operational.\n        video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n            Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n            cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n            It is optional and if provided can be provided as single dict (applicable for all sources) or\n            as list of configs. Then the list must be of length of `video_reference` and may also contain None\n            values to denote that specific source should remain not configured.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        active_learning_target_dataset (Optional[str]): Parameter to be used when Active Learning data registration\n            should happen against different dataset than the one pointed by model_id\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n            handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n            in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n            in the order of video sources - with None values in the place of vide_frames / predictions that\n            were skipped due to `batch_collection_timeout`.\n            `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n            against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n            To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n            `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n            old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n            prediction element.\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n    * ACTIVE_LEARNING_ENABLED - controls Active Learning middleware if explicit parameter not given\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    if api_key is None:\n        api_key = API_KEY\n    inference_config = ModelConfig.init(\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        mask_decode_mode=mask_decode_mode,\n        tradeoff_factor=tradeoff_factor,\n    )\n    model = get_model(model_id=model_id, api_key=api_key)\n    on_video_frame = partial(\n        default_process_frame, model=model, inference_config=inference_config\n    )\n    active_learning_middleware = NullActiveLearningMiddleware()\n    if active_learning_enabled is None:\n        logger.info(\n            f\"`active_learning_enabled` parameter not set - using env `ACTIVE_LEARNING_ENABLED` \"\n            f\"with value: {ACTIVE_LEARNING_ENABLED}\"\n        )\n        active_learning_enabled = ACTIVE_LEARNING_ENABLED\n    if api_key is None:\n        logger.info(\n            f\"Roboflow API key not given - Active Learning is forced to be disabled.\"\n        )\n        active_learning_enabled = False\n    if active_learning_enabled is True:\n        resolved_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        target_dataset = (\n            active_learning_target_dataset or resolved_model_id.split(\"/\")[0]\n        )\n        active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n            api_key=api_key,\n            target_dataset=target_dataset,\n            model_id=resolved_model_id,\n            cache=cache,\n        )\n        al_sink = partial(\n            active_learning_sink,\n            active_learning_middleware=active_learning_middleware,\n            model_type=model.task_type,\n            disable_preproc_auto_orient=DISABLE_PREPROC_AUTO_ORIENT,\n        )\n        logger.info(\n            \"AL enabled - wrapping `on_prediction` with multi_sink() and active_learning_sink()\"\n        )\n        on_prediction = partial(multi_sink, sinks=[on_prediction, al_sink])\n    on_pipeline_start = active_learning_middleware.start_registration_thread\n    on_pipeline_end = active_learning_middleware.stop_registration_thread\n    return InferencePipeline.init_with_custom_logic(\n        video_reference=video_reference,\n        on_video_frame=on_video_frame,\n        on_prediction=on_prediction,\n        on_pipeline_start=on_pipeline_start,\n        on_pipeline_end=on_pipeline_end,\n        max_fps=max_fps,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        video_source_properties=video_source_properties,\n        batch_collection_timeout=batch_collection_timeout,\n        sink_mode=sink_mode,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init_with_custom_logic","title":"<code>init_with_custom_logic(video_reference, on_video_frame, on_prediction=None, on_pipeline_start=None, on_pipeline_end=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, video_source_properties=None, batch_collection_timeout=None, sink_mode=SinkMode.ADAPTIVE)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from given workflow against video stream. The way of how <code>InferencePipeline</code> works is displayed in <code>InferencePipeline.init(...)</code> initialiser method.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source or sources to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>on_video_frame</code> <code>Callable[[VideoFrame], AnyPrediction]</code> <p>function supposed to make prediction (or do another kind of custom processing according to your will). Accept <code>VideoFrame</code> object and is supposed to return dictionary with results of any kind.</p> required <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with output from your custom callable <code>on_video_frame(...)</code>. Logic here must be adjusted to the output of <code>on_video_frame</code>.</p> <code>None</code> <code>on_pipeline_start</code> <code>Optional[Callable[[], None]]</code> <p>Optional (parameter-free) function to be called whenever pipeline starts</p> <code>None</code> <code>on_pipeline_end</code> <code>Optional[Callable[[], None]]</code> <p>Optional (parameter-free) function to be called whenever pipeline ends</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines on single machine making tradeoff between number of frames and number of streams handled. Disabled by default.</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>video_source_properties</code> <code>Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. It is optional and if provided can be provided as single dict (applicable for all sources) or as list of configs. Then the list must be of length of <code>video_reference</code> and may also contain None values to denote that specific source should remain not configured. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>sink_mode</code> <code>SinkMode</code> <p>Parameter that controls how video frames and predictions will be passed to sink handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink, in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>. <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos - sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p> <code>ADAPTIVE</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init_with_custom_logic(\n    cls,\n    video_reference: Union[str, int, List[Union[str, int]]],\n    on_video_frame: InferenceHandler,\n    on_prediction: SinkHandler = None,\n    on_pipeline_start: Optional[Callable[[], None]] = None,\n    on_pipeline_end: Optional[Callable[[], None]] = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    batch_collection_timeout: Optional[float] = None,\n    sink_mode: SinkMode = SinkMode.ADAPTIVE,\n) -&gt; \"InferencePipeline\":\n\"\"\"\n    This class creates the abstraction for making inferences from given workflow against video stream.\n    The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initialiser\n    method.\n\n    Args:\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n            predictions against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        on_video_frame (Callable[[VideoFrame], AnyPrediction]): function supposed to make prediction (or do another\n            kind of custom processing according to your will). Accept `VideoFrame` object and is supposed\n            to return dictionary with results of any kind.\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with output from your\n            custom callable `on_video_frame(...)`. Logic here must be adjusted to the output of `on_video_frame`.\n        on_pipeline_start (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n            whenever pipeline starts\n        on_pipeline_end (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n            whenever pipeline ends\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n            on single machine making tradeoff between number of frames and number of streams handled. Disabled\n            by default.\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n            Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n            cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n            It is optional and if provided can be provided as single dict (applicable for all sources) or\n            as list of configs. Then the list must be of length of `video_reference` and may also contain None\n            values to denote that specific source should remain not configured.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n            handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n            in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n            in the order of video sources - with None values in the place of vide_frames / predictions that\n            were skipped due to `batch_collection_timeout`.\n            `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n            against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n            To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n            `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n            old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n            prediction element.\n\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    if watchdog is None:\n        watchdog = NullPipelineWatchdog()\n    if status_update_handlers is None:\n        status_update_handlers = []\n    status_update_handlers.append(watchdog.on_status_update)\n    video_sources = prepare_video_sources(\n        video_reference=video_reference,\n        video_source_properties=video_source_properties,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n    )\n    watchdog.register_video_sources(video_sources=video_sources)\n    predictions_queue = Queue(maxsize=PREDICTIONS_QUEUE_SIZE)\n    return cls(\n        on_video_frame=on_video_frame,\n        video_sources=video_sources,\n        predictions_queue=predictions_queue,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        on_prediction=on_prediction,\n        max_fps=max_fps,\n        on_pipeline_start=on_pipeline_start,\n        on_pipeline_end=on_pipeline_end,\n        batch_collection_timeout=batch_collection_timeout,\n        sink_mode=sink_mode,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init_with_workflow","title":"<code>init_with_workflow(video_reference, workflow_specification, api_key=None, image_input_name='image', workflows_parameters=None, on_prediction=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, video_source_properties=None)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from given workflow against video stream. The way of how <code>InferencePipeline</code> works is displayed in <code>InferencePipeline.init(...)</code> initializer method.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int]</code> <p>Reference of source to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles).</p> required <code>workflow_specification</code> <code>dict</code> <p>Valid specification of workflow. See workflow docs</p> required <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\" and \"API_KEY\" variables. API key, passed in some form is required.</p> <code>None</code> <code>image_input_name</code> <code>str</code> <p>Name of input image defined in <code>workflow_specification</code>. <code>InferencePipeline</code> will be injecting video frames to workflow through that parameter name.</p> <code>'image'</code> <code>workflows_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary with additional parameters that can be defined within <code>workflow_specification</code>.</p> <code>None</code> <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with workflow output.</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines on single machine making tradeoff between number of frames and number of streams handled. Disabled by default.</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>video_source_properties</code> <code>Optional[dict[str, float]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init_with_workflow(\n    cls,\n    video_reference: Union[str, int],\n    workflow_specification: dict,\n    api_key: Optional[str] = None,\n    image_input_name: str = \"image\",\n    workflows_parameters: Optional[Dict[str, Any]] = None,\n    on_prediction: SinkHandler = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    video_source_properties: Optional[Dict[str, float]] = None,\n) -&gt; \"InferencePipeline\":\n\"\"\"\n    This class creates the abstraction for making inferences from given workflow against video stream.\n    The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n    method.\n\n    Args:\n        video_reference (Union[str, int]): Reference of source to be used to make predictions against.\n            It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles).\n        workflow_specification (dict): Valid specification of workflow. See [workflow docs](https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows)\n        api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n            and \"API_KEY\" variables. API key, passed in some form is required.\n        image_input_name (str): Name of input image defined in `workflow_specification`. `InferencePipeline` will be\n            injecting video frames to workflow through that parameter name.\n        workflows_parameters (Optional[Dict[str, Any]]): Dictionary with additional parameters that can be\n            defined within `workflow_specification`.\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with workflow output.\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n            on single machine making tradeoff between number of frames and number of streams handled. Disabled\n            by default.\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        video_source_properties (Optional[dict[str, float]]): Optional source properties to set up the video source,\n            corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source\n            will be used.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    if issubclass(type(video_reference), list) and len(list) &gt; 1:\n        raise ValueError(\n            \"Usage of workflows and `InferencePipeline` is experimental feature for now. We do not support \"\n            \"multiple video sources yet.\"\n        )\n    try:\n        from inference.core.interfaces.stream.model_handlers.workflows import (\n            run_video_frame_through_workflow,\n        )\n        from inference.enterprise.workflows.complier.steps_executors.active_learning_middlewares import (\n            WorkflowsActiveLearningMiddleware,\n        )\n\n        workflows_active_learning_middleware = WorkflowsActiveLearningMiddleware(\n            cache=cache,\n        )\n        model_registry = RoboflowModelRegistry(ROBOFLOW_MODEL_TYPES)\n        model_manager = BackgroundTaskActiveLearningManager(\n            model_registry=model_registry, cache=cache\n        )\n        model_manager = WithFixedSizeCache(\n            model_manager,\n            max_size=MAX_ACTIVE_MODELS,\n        )\n        if api_key is None:\n            api_key = API_KEY\n        background_tasks = BackgroundTasks()\n        on_video_frame = partial(\n            run_video_frame_through_workflow,\n            workflow_specification=workflow_specification,\n            model_manager=model_manager,\n            image_input_name=image_input_name,\n            workflows_parameters=workflows_parameters,\n            api_key=api_key,\n            workflows_active_learning_middleware=workflows_active_learning_middleware,\n            background_tasks=background_tasks,\n        )\n    except ImportError as error:\n        raise CannotInitialiseModelError(\n            f\"Could not initialise workflow processing due to lack of dependencies required. \"\n            f\"Please provide an issue report under https://github.com/roboflow/inference/issues\"\n        ) from error\n    return InferencePipeline.init_with_custom_logic(\n        video_reference=video_reference,\n        on_video_frame=on_video_frame,\n        on_prediction=on_prediction,\n        on_pipeline_start=None,\n        on_pipeline_end=None,\n        max_fps=max_fps,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        video_source_properties=video_source_properties,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init_with_yolo_world","title":"<code>init_with_yolo_world(video_reference, classes, model_size='s', on_prediction=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, class_agnostic_nms=None, confidence=None, iou_threshold=None, max_candidates=None, max_detections=None, video_source_properties=None, batch_collection_timeout=None, sink_mode=SinkMode.ADAPTIVE)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from YoloWorld against video stream. The way of how <code>InferencePipeline</code> works is displayed in <code>InferencePipeline.init(...)</code> initializer method.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source or sources to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>classes</code> <code>List[str]</code> <p>List of classes to execute zero-shot detection against</p> required <code>model_size</code> <code>str</code> <p>version of model - to be chosen from <code>s</code>, <code>m</code>, <code>l</code></p> <code>'s'</code> <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with standard Roboflow Object Detection prediction.</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines on single machine making tradeoff between number of frames and number of streams handled. Disabled by default.</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CONFIDENCE\" with default \"0.5\"</p> <code>None</code> <code>iou_threshold</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"IOU_THRESHOLD\" with default \"0.5\"</p> <code>None</code> <code>max_candidates</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_CANDIDATES\" with default \"3000\"</p> <code>None</code> <code>max_detections</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_DETECTIONS\" with default \"300\"</p> <code>None</code> <code>video_source_properties</code> <code>Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. It is optional and if provided can be provided as single dict (applicable for all sources) or as list of configs. Then the list must be of length of <code>video_reference</code> and may also contain None values to denote that specific source should remain not configured. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>sink_mode</code> <code>SinkMode</code> <p>Parameter that controls how video frames and predictions will be passed to sink handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink, in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>. <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos - sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p> <code>ADAPTIVE</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init_with_yolo_world(\n    cls,\n    video_reference: Union[str, int, List[Union[str, int]]],\n    classes: List[str],\n    model_size: str = \"s\",\n    on_prediction: SinkHandler = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    class_agnostic_nms: Optional[bool] = None,\n    confidence: Optional[float] = None,\n    iou_threshold: Optional[float] = None,\n    max_candidates: Optional[int] = None,\n    max_detections: Optional[int] = None,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    batch_collection_timeout: Optional[float] = None,\n    sink_mode: SinkMode = SinkMode.ADAPTIVE,\n) -&gt; \"InferencePipeline\":\n\"\"\"\n    This class creates the abstraction for making inferences from YoloWorld against video stream.\n    The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n    method.\n\n    Args:\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n            predictions against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        classes (List[str]): List of classes to execute zero-shot detection against\n        model_size (str): version of model - to be chosen from `s`, `m`, `l`\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with standard\n            Roboflow Object Detection prediction.\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of processing. It can be useful if we wanted to run concurrent inference pipelines\n            on single machine making tradeoff between number of frames and number of streams handled. Disabled\n            by default.\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](../camera/video_source.py)\n        class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n        confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CONFIDENCE\" with default \"0.5\"\n        iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"IOU_THRESHOLD\" with default \"0.5\"\n        max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_CANDIDATES\" with default \"3000\"\n        max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_DETECTIONS\" with default \"300\"\n        video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n            Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n            cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n            It is optional and if provided can be provided as single dict (applicable for all sources) or\n            as list of configs. Then the list must be of length of `video_reference` and may also contain None\n            values to denote that specific source should remain not configured.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n            handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n            in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n            in the order of video sources - with None values in the place of vide_frames / predictions that\n            were skipped due to `batch_collection_timeout`.\n            `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n            against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n            To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n            `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n            old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n            prediction element.\n\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    inference_config = ModelConfig.init(\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n    )\n    try:\n        from inference.core.interfaces.stream.model_handlers.yolo_world import (\n            build_yolo_world_inference_function,\n        )\n\n        on_video_frame = build_yolo_world_inference_function(\n            model_id=f\"yolo_world/{model_size}\",\n            classes=classes,\n            inference_config=inference_config,\n        )\n    except ImportError as error:\n        raise CannotInitialiseModelError(\n            f\"Could not initialise yolo_world/{model_size} due to lack of sufficient dependencies. \"\n            f\"Use pip install inference[yolo-world] to install missing dependencies and try again.\"\n        ) from error\n    return InferencePipeline.init_with_custom_logic(\n        video_reference=video_reference,\n        on_video_frame=on_video_frame,\n        on_prediction=on_prediction,\n        on_pipeline_start=None,\n        on_pipeline_end=None,\n        max_fps=max_fps,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        video_source_properties=video_source_properties,\n        batch_collection_timeout=batch_collection_timeout,\n        sink_mode=sink_mode,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/","title":"sinks","text":""},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink","title":"<code>UDPSink</code>","text":"Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>class UDPSink:\n    @classmethod\n    def init(cls, ip_address: str, port: int) -&gt; \"UDPSink\":\n\"\"\"\n        Creates `InferencePipeline` predictions sink capable of sending model predictions over network\n        using UDP socket.\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n        Args:\n            ip_address (str): IP address to send predictions\n            port (int): Port to send predictions\n\n        Returns: Initialised object of `UDPSink` class.\n        \"\"\"\n        udp_socket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n        return cls(\n            ip_address=ip_address,\n            port=port,\n            udp_socket=udp_socket,\n        )\n\n    def __init__(self, ip_address: str, port: int, udp_socket: socket.socket):\n        self._ip_address = ip_address\n        self._port = port\n        self._socket = udp_socket\n\n    def send_predictions(\n        self,\n        predictions: Union[dict, List[Optional[dict]]],\n        video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    ) -&gt; None:\n\"\"\"\n        Method to send predictions via UDP socket. Useful in combination with `InferencePipeline` as\n        a sink for predictions.\n\n        Args:\n            predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n                processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n                should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n            video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n                by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n                to `predictions` list). Order is expected to match with `predictions`\n\n        Returns: None\n        Side effects: Sends serialised `predictions` and `video_frame` metadata via the UDP socket as\n            JSON string. It adds key named \"inference_metadata\" into `predictions` dict (mutating its\n            state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message\n            emission time in datetime iso format.\n\n        Example:\n            ```python\n            import cv2\n            from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\n            from inference.core.interfaces.stream.sinks import UDPSink\n\n            udp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\n            pipeline = InferencePipeline.init(\n                 model_id=\"your-model/3\",\n                 video_reference=\"./some_file.mp4\",\n                 on_prediction=udp_sink.send_predictions,\n            )\n            pipeline.start()\n            pipeline.join()\n            ```\n            `UDPSink` used in this way will emit predictions to receiver automatically.\n        \"\"\"\n        video_frame = wrap_in_list(element=video_frame)\n        predictions = wrap_in_list(element=predictions)\n        for single_frame, frame_predictions in zip(video_frame, predictions):\n            if single_frame is None:\n                continue\n            inference_metadata = {\n                \"source_id\": single_frame.source_id,\n                \"frame_id\": single_frame.frame_id,\n                \"frame_decoding_time\": single_frame.frame_timestamp.isoformat(),\n                \"emission_time\": datetime.now().isoformat(),\n            }\n            frame_predictions[\"inference_metadata\"] = inference_metadata\n            serialised_predictions = json.dumps(frame_predictions).encode(\"utf-8\")\n            self._socket.sendto(\n                serialised_predictions,\n                (\n                    self._ip_address,\n                    self._port,\n                ),\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink.init","title":"<code>init(ip_address, port)</code>  <code>classmethod</code>","text":"<p>Creates <code>InferencePipeline</code> predictions sink capable of sending model predictions over network using UDP socket.</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects. Args:     ip_address (str): IP address to send predictions     port (int): Port to send predictions</p> <p>Returns: Initialised object of <code>UDPSink</code> class.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>@classmethod\ndef init(cls, ip_address: str, port: int) -&gt; \"UDPSink\":\n\"\"\"\n    Creates `InferencePipeline` predictions sink capable of sending model predictions over network\n    using UDP socket.\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n    Args:\n        ip_address (str): IP address to send predictions\n        port (int): Port to send predictions\n\n    Returns: Initialised object of `UDPSink` class.\n    \"\"\"\n    udp_socket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n    return cls(\n        ip_address=ip_address,\n        port=port,\n        udp_socket=udp_socket,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink.send_predictions","title":"<code>send_predictions(predictions, video_frame)</code>","text":"<p>Method to send predictions via UDP socket. Useful in combination with <code>InferencePipeline</code> as a sink for predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <p>Side effects: Sends serialised <code>predictions</code> and <code>video_frame</code> metadata via the UDP socket as     JSON string. It adds key named \"inference_metadata\" into <code>predictions</code> dict (mutating its     state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message     emission time in datetime iso format.</p> Example <p><pre><code>import cv2\nfrom inference.core.interfaces.stream.inference_pipeline import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import UDPSink\n\nudp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\npipeline = InferencePipeline.init(\n     model_id=\"your-model/3\",\n     video_reference=\"./some_file.mp4\",\n     on_prediction=udp_sink.send_predictions,\n)\npipeline.start()\npipeline.join()\n</code></pre> <code>UDPSink</code> used in this way will emit predictions to receiver automatically.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def send_predictions(\n    self,\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n\"\"\"\n    Method to send predictions via UDP socket. Useful in combination with `InferencePipeline` as\n    a sink for predictions.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n\n    Returns: None\n    Side effects: Sends serialised `predictions` and `video_frame` metadata via the UDP socket as\n        JSON string. It adds key named \"inference_metadata\" into `predictions` dict (mutating its\n        state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message\n        emission time in datetime iso format.\n\n    Example:\n        ```python\n        import cv2\n        from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\n        from inference.core.interfaces.stream.sinks import UDPSink\n\n        udp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\n        pipeline = InferencePipeline.init(\n             model_id=\"your-model/3\",\n             video_reference=\"./some_file.mp4\",\n             on_prediction=udp_sink.send_predictions,\n        )\n        pipeline.start()\n        pipeline.join()\n        ```\n        `UDPSink` used in this way will emit predictions to receiver automatically.\n    \"\"\"\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    for single_frame, frame_predictions in zip(video_frame, predictions):\n        if single_frame is None:\n            continue\n        inference_metadata = {\n            \"source_id\": single_frame.source_id,\n            \"frame_id\": single_frame.frame_id,\n            \"frame_decoding_time\": single_frame.frame_timestamp.isoformat(),\n            \"emission_time\": datetime.now().isoformat(),\n        }\n        frame_predictions[\"inference_metadata\"] = inference_metadata\n        serialised_predictions = json.dumps(frame_predictions).encode(\"utf-8\")\n        self._socket.sendto(\n            serialised_predictions,\n            (\n                self._ip_address,\n                self._port,\n            ),\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink","title":"<code>VideoFileSink</code>","text":"Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>class VideoFileSink:\n    @classmethod\n    def init(\n        cls,\n        video_file_name: str,\n        annotator: sv.BoxAnnotator = DEFAULT_ANNOTATOR,\n        display_size: Optional[Tuple[int, int]] = (1280, 720),\n        fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n        display_statistics: bool = False,\n        output_fps: int = 25,\n        quiet: bool = False,\n        video_frame_size: Tuple[int, int] = (1280, 720),\n    ) -&gt; \"VideoFileSink\":\n\"\"\"\n        Creates `InferencePipeline` predictions sink capable of saving model predictions into video file.\n        It works both for pipelines with single input video and multiple ones.\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n        Args:\n            video_file_name (str): name of the video file to save predictions\n            annotator (sv.BoxAnnotator): Annotator used to draw Bounding Boxes - if custom object is not passed,\n                default is used.\n            display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should\n                be set to the same value as `display_size` for InferencePipeline with single video source, otherwise\n                it represents the size of single visualisation tile (whole tiles mosaic will be scaled to\n                `video_frame_size`)\n            fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n            display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n                if enabled, throughput will only be presented if `fps_monitor` is not None\n            output_fps (int): desired FPS of output file\n            quiet (bool): Flag to decide whether to log progress\n            video_frame_size (Tuple[int, int]): The size of frame in target video file.\n\n        Attributes:\n            on_prediction (Callable[[dict, VideoFrame], None]): callable to be used as a sink for predictions\n\n        Returns: Initialized object of `VideoFileSink` class.\n\n        Example:\n            ```python\n            import cv2\n            from inference import InferencePipeline\n            from inference.core.interfaces.stream.sinks import VideoFileSink\n\n            video_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\n            pipeline = InferencePipeline.init(\n                model_id=\"your-model/3\",\n                video_reference=\"./some_file.mp4\",\n                on_prediction=video_sink.on_prediction,\n            )\n            pipeline.start()\n            pipeline.join()\n            video_sink.release()\n            ```\n\n            `VideoFileSink` used in this way will save predictions to video file automatically.\n        \"\"\"\n        return cls(\n            video_file_name=video_file_name,\n            annotator=annotator,\n            display_size=display_size,\n            fps_monitor=fps_monitor,\n            display_statistics=display_statistics,\n            output_fps=output_fps,\n            quiet=quiet,\n            video_frame_size=video_frame_size,\n        )\n\n    def __init__(\n        self,\n        video_file_name: str,\n        annotator: sv.BoxAnnotator,\n        display_size: Optional[Tuple[int, int]],\n        fps_monitor: Optional[sv.FPSMonitor],\n        display_statistics: bool,\n        output_fps: int,\n        quiet: bool,\n        video_frame_size: Tuple[int, int],\n    ):\n        self._video_file_name = video_file_name\n        self._annotator = annotator\n        self._display_size = display_size\n        self._fps_monitor = fps_monitor\n        self._display_statistics = display_statistics\n        self._output_fps = output_fps\n        self._quiet = quiet\n        self._frame_idx = 0\n        self._video_frame_size = video_frame_size\n        self._video_writer: Optional[cv2.VideoWriter] = None\n        self.on_prediction = partial(\n            render_boxes,\n            annotator=self._annotator,\n            display_size=self._display_size,\n            fps_monitor=self._fps_monitor,\n            display_statistics=self._display_statistics,\n            on_frame_rendered=self._save_predictions,\n        )\n\n    def release(self) -&gt; None:\n\"\"\"\n        Releases VideoWriter object.\n        \"\"\"\n        if self._video_writer is not None and self._video_writer.isOpened():\n            self._video_writer.release()\n\n    def _save_predictions(\n        self,\n        frame: Union[ImageWithSourceID, List[ImageWithSourceID]],\n    ) -&gt; None:\n        if self._video_writer is None:\n            self._initialise_sink()\n        if issubclass(type(frame), list):\n            frame = create_tiles(images=[i[1] for i in frame])\n        else:\n            frame = frame[1]\n        if (frame.shape[1], frame.shape[0]) != self._video_frame_size:\n            frame = letterbox_image(image=frame, desired_size=self._video_frame_size)\n        self._video_writer.write(frame)\n        if not self._quiet:\n            print(f\"Writing frame {self._frame_idx}\", end=\"\\r\")\n        self._frame_idx += 1\n\n    def _initialise_sink(self) -&gt; None:\n        self._video_writer = cv2.VideoWriter(\n            self._video_file_name,\n            cv2.VideoWriter_fourcc(*\"MJPG\"),\n            self._output_fps,\n            self._video_frame_size,\n        )\n\n    def __enter__(self) -&gt; \"VideoFileSink\":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        self.release()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink.init","title":"<code>init(video_file_name, annotator=DEFAULT_ANNOTATOR, display_size=(1280, 720), fps_monitor=DEFAULT_FPS_MONITOR, display_statistics=False, output_fps=25, quiet=False, video_frame_size=(1280, 720))</code>  <code>classmethod</code>","text":"<p>Creates <code>InferencePipeline</code> predictions sink capable of saving model predictions into video file. It works both for pipelines with single input video and multiple ones.</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects. Args:     video_file_name (str): name of the video file to save predictions     annotator (sv.BoxAnnotator): Annotator used to draw Bounding Boxes - if custom object is not passed,         default is used.     display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should         be set to the same value as <code>display_size</code> for InferencePipeline with single video source, otherwise         it represents the size of single visualisation tile (whole tiles mosaic will be scaled to         <code>video_frame_size</code>)     fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput     display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,         if enabled, throughput will only be presented if <code>fps_monitor</code> is not None     output_fps (int): desired FPS of output file     quiet (bool): Flag to decide whether to log progress     video_frame_size (Tuple[int, int]): The size of frame in target video file.</p> <p>Attributes:</p> Name Type Description <code>on_prediction</code> <code>Callable[[dict, VideoFrame], None]</code> <p>callable to be used as a sink for predictions</p> <p>Returns: Initialized object of <code>VideoFileSink</code> class.</p> Example <pre><code>import cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import VideoFileSink\n\nvideo_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\npipeline = InferencePipeline.init(\n    model_id=\"your-model/3\",\n    video_reference=\"./some_file.mp4\",\n    on_prediction=video_sink.on_prediction,\n)\npipeline.start()\npipeline.join()\nvideo_sink.release()\n</code></pre> <p><code>VideoFileSink</code> used in this way will save predictions to video file automatically.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_file_name: str,\n    annotator: sv.BoxAnnotator = DEFAULT_ANNOTATOR,\n    display_size: Optional[Tuple[int, int]] = (1280, 720),\n    fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n    display_statistics: bool = False,\n    output_fps: int = 25,\n    quiet: bool = False,\n    video_frame_size: Tuple[int, int] = (1280, 720),\n) -&gt; \"VideoFileSink\":\n\"\"\"\n    Creates `InferencePipeline` predictions sink capable of saving model predictions into video file.\n    It works both for pipelines with single input video and multiple ones.\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n    Args:\n        video_file_name (str): name of the video file to save predictions\n        annotator (sv.BoxAnnotator): Annotator used to draw Bounding Boxes - if custom object is not passed,\n            default is used.\n        display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should\n            be set to the same value as `display_size` for InferencePipeline with single video source, otherwise\n            it represents the size of single visualisation tile (whole tiles mosaic will be scaled to\n            `video_frame_size`)\n        fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n        display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n            if enabled, throughput will only be presented if `fps_monitor` is not None\n        output_fps (int): desired FPS of output file\n        quiet (bool): Flag to decide whether to log progress\n        video_frame_size (Tuple[int, int]): The size of frame in target video file.\n\n    Attributes:\n        on_prediction (Callable[[dict, VideoFrame], None]): callable to be used as a sink for predictions\n\n    Returns: Initialized object of `VideoFileSink` class.\n\n    Example:\n        ```python\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import VideoFileSink\n\n        video_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\n        pipeline = InferencePipeline.init(\n            model_id=\"your-model/3\",\n            video_reference=\"./some_file.mp4\",\n            on_prediction=video_sink.on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        video_sink.release()\n        ```\n\n        `VideoFileSink` used in this way will save predictions to video file automatically.\n    \"\"\"\n    return cls(\n        video_file_name=video_file_name,\n        annotator=annotator,\n        display_size=display_size,\n        fps_monitor=fps_monitor,\n        display_statistics=display_statistics,\n        output_fps=output_fps,\n        quiet=quiet,\n        video_frame_size=video_frame_size,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink.release","title":"<code>release()</code>","text":"<p>Releases VideoWriter object.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def release(self) -&gt; None:\n\"\"\"\n    Releases VideoWriter object.\n    \"\"\"\n    if self._video_writer is not None and self._video_writer.isOpened():\n        self._video_writer.release()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.active_learning_sink","title":"<code>active_learning_sink(predictions, video_frame, active_learning_middleware, model_type, disable_preproc_auto_orient=False)</code>","text":"<p>Function to serve as Active Learning sink for InferencePipeline.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <code>active_learning_middleware</code> <code>ActiveLearningMiddleware</code> <p>instance of middleware to register data.</p> required <code>model_type</code> <code>str</code> <p>Type of Roboflow model in use</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>Flag to denote how image is preprocessed which is important in Active Learning.</p> <code>False</code> <p>Returns: None Side effects: Can register data and predictions in Roboflow backend if that's the evaluation of sampling engine.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def active_learning_sink(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    active_learning_middleware: ActiveLearningMiddleware,\n    model_type: str,\n    disable_preproc_auto_orient: bool = False,\n) -&gt; None:\n\"\"\"\n    Function to serve as Active Learning sink for InferencePipeline.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n        active_learning_middleware (ActiveLearningMiddleware): instance of middleware to register data.\n        model_type (str): Type of Roboflow model in use\n        disable_preproc_auto_orient (bool): Flag to denote how image is preprocessed which is important in\n            Active Learning.\n\n    Returns: None\n    Side effects: Can register data and predictions in Roboflow backend if that's the evaluation of sampling engine.\n    \"\"\"\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    images = [f.image for f in video_frame if f is not None]\n    predictions = [p for p in predictions if p is not None]\n    active_learning_middleware.register_batch(\n        inference_inputs=images,\n        predictions=predictions,\n        prediction_type=model_type,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.multi_sink","title":"<code>multi_sink(predictions, video_frame, sinks)</code>","text":"<p>Helper util useful to combine multiple sinks together, while using <code>InferencePipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>video_frame</code> <code>VideoFrame</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code></p> required <code>predictions</code> <code>dict</code> <p>Roboflow object detection predictions with Bounding Boxes</p> required <code>sinks</code> <code>List[Callable[[VideoFrame, dict], None]]</code> <p>list of sinks to be used. Each will be executed one-by-one in the order pointed in input list, all errors will be caught and reported via logger, without re-raising.</p> required <p>Returns: None Side effects: Uses all sinks in context if (video_frame, predictions) input.</p> Example <pre><code>from functools import partial\nimport cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import UDPSink, render_boxes\n\nudp_sink = UDPSink(ip_address=\"127.0.0.1\", port=9090)\non_prediction = partial(multi_sink, sinks=[udp_sink.send_predictions, render_boxes])\n\npipeline = InferencePipeline.init(\n    model_id=\"your-model/3\",\n    video_reference=\"./some_file.mp4\",\n    on_prediction=on_prediction,\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>As a result, predictions will both be sent via UDP socket and displayed in the screen.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def multi_sink(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    sinks: List[SinkHandler],\n) -&gt; None:\n\"\"\"\n    Helper util useful to combine multiple sinks together, while using `InferencePipeline`.\n\n    Args:\n        video_frame (VideoFrame): frame of video with its basic metadata emitted by `VideoSource`\n        predictions (dict): Roboflow object detection predictions with Bounding Boxes\n        sinks (List[Callable[[VideoFrame, dict], None]]): list of sinks to be used. Each will be executed\n            one-by-one in the order pointed in input list, all errors will be caught and reported via logger,\n            without re-raising.\n\n    Returns: None\n    Side effects: Uses all sinks in context if (video_frame, predictions) input.\n\n    Example:\n        ```python\n        from functools import partial\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import UDPSink, render_boxes\n\n        udp_sink = UDPSink(ip_address=\"127.0.0.1\", port=9090)\n        on_prediction = partial(multi_sink, sinks=[udp_sink.send_predictions, render_boxes])\n\n        pipeline = InferencePipeline.init(\n            model_id=\"your-model/3\",\n            video_reference=\"./some_file.mp4\",\n            on_prediction=on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        ```\n\n        As a result, predictions will both be sent via UDP socket and displayed in the screen.\n    \"\"\"\n    for sink in sinks:\n        try:\n            sink(predictions, video_frame)\n        except Exception as error:\n            logger.error(\n                f\"Could not sent prediction with frame_id={video_frame.frame_id} to sink \"\n                f\"due to error: {error}.\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.render_boxes","title":"<code>render_boxes(predictions, video_frame, annotator=DEFAULT_ANNOTATOR, display_size=(1280, 720), fps_monitor=DEFAULT_FPS_MONITOR, display_statistics=False, on_frame_rendered=display_image)</code>","text":"<p>Helper tool to render object detection predictions on top of video frame. It is designed to be used with <code>InferencePipeline</code>, as sink for predictions. By default, it uses standard <code>sv.BoxAnnotator()</code> to draw bounding boxes and resizes prediction to 1280x720 (keeping aspect ratio and adding black padding). One may configure default behaviour, for instance to display latency and throughput statistics. In batch mode it will display tiles of frames and overlay predictions.</p> <p>This sink is only partially compatible with stubs and classification models (it will not fail, although predictions will not be displayed).</p> <p>Since version <code>0.9.18</code>, when multi-source InferencePipeline was introduced - it support batch input, without changes to old functionality when single (predictions, video_frame) is used.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <code>annotator</code> <code>BoxAnnotator</code> <p>Annotator used to draw Bounding Boxes - if custom object is not passed, default is used.</p> <code>DEFAULT_ANNOTATOR</code> <code>display_size</code> <code>Tuple[int, int]</code> <p>tuple in format (width, height) to resize visualisation output</p> <code>(1280, 720)</code> <code>fps_monitor</code> <code>Optional[FPSMonitor]</code> <p>FPS monitor used to monitor throughput</p> <code>DEFAULT_FPS_MONITOR</code> <code>display_statistics</code> <code>bool</code> <p>Flag to decide if throughput and latency can be displayed in the result image, if enabled, throughput will only be presented if <code>fps_monitor</code> is not None</p> <code>False</code> <code>on_frame_rendered</code> <code>Callable[[Union[ImageWithSourceID, List[ImageWithSourceID]]], None]</code> <p>callback to be called once frame is rendered - by default, function will display OpenCV window. It expects optional integer identifier with np.ndarray or list of those elements. Identifier is supposed to refer to either source_id (for sequential input) or position in the batch (from 0 to batch_size-1).</p> <code>display_image</code> <p>Side effects: on_frame_rendered() is called against the np.ndarray produced from video frame     and predictions.</p> Example <pre><code>from functools import partial\nimport cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\noutput_size = (640, 480)\nvideo_sink = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 25.0, output_size)\non_prediction = partial(render_boxes, display_size=output_size, on_frame_rendered=video_sink.write)\n\npipeline = InferencePipeline.init(\n     model_id=\"your-model/3\",\n     video_reference=\"./some_file.mp4\",\n     on_prediction=on_prediction,\n)\npipeline.start()\npipeline.join()\nvideo_sink.release()\n</code></pre> <p>In this example, <code>render_boxes()</code> is used as a sink for <code>InferencePipeline</code> predictions - making frames with predictions displayed to be saved into video file.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def render_boxes(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    annotator: sv.BoxAnnotator = DEFAULT_ANNOTATOR,\n    display_size: Optional[Tuple[int, int]] = (1280, 720),\n    fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n    display_statistics: bool = False,\n    on_frame_rendered: Callable[\n        [Union[ImageWithSourceID, List[ImageWithSourceID]]], None\n    ] = display_image,\n) -&gt; None:\n\"\"\"\n    Helper tool to render object detection predictions on top of video frame. It is designed\n    to be used with `InferencePipeline`, as sink for predictions. By default, it uses standard `sv.BoxAnnotator()`\n    to draw bounding boxes and resizes prediction to 1280x720 (keeping aspect ratio and adding black padding).\n    One may configure default behaviour, for instance to display latency and throughput statistics.\n    In batch mode it will display tiles of frames and overlay predictions.\n\n    This sink is only partially compatible with stubs and classification models (it will not fail,\n    although predictions will not be displayed).\n\n    Since version `0.9.18`, when multi-source InferencePipeline was introduced - it support batch input, without\n    changes to old functionality when single (predictions, video_frame) is used.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n        annotator (sv.BoxAnnotator): Annotator used to draw Bounding Boxes - if custom object is not passed,\n            default is used.\n        display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output\n        fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n        display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n            if enabled, throughput will only be presented if `fps_monitor` is not None\n        on_frame_rendered (Callable[[Union[ImageWithSourceID, List[ImageWithSourceID]]], None]): callback to be\n            called once frame is rendered - by default, function will display OpenCV window. It expects optional integer\n            identifier with np.ndarray or list of those elements. Identifier is supposed to refer to either source_id\n            (for sequential input) or position in the batch (from 0 to batch_size-1).\n\n    Returns: None\n    Side effects: on_frame_rendered() is called against the np.ndarray produced from video frame\n        and predictions.\n\n    Example:\n        ```python\n        from functools import partial\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import render_boxes\n\n        output_size = (640, 480)\n        video_sink = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 25.0, output_size)\n        on_prediction = partial(render_boxes, display_size=output_size, on_frame_rendered=video_sink.write)\n\n        pipeline = InferencePipeline.init(\n             model_id=\"your-model/3\",\n             video_reference=\"./some_file.mp4\",\n             on_prediction=on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        video_sink.release()\n        ```\n\n        In this example, `render_boxes()` is used as a sink for `InferencePipeline` predictions - making frames with\n        predictions displayed to be saved into video file.\n    \"\"\"\n    sequential_input_provided = False\n    if not issubclass(type(video_frame), list):\n        sequential_input_provided = True\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    fps_value = None\n    if fps_monitor is not None:\n        ticks = sum(f is not None for f in video_frame)\n        for _ in range(ticks):\n            fps_monitor.tick()\n        fps_value = fps_monitor()\n    images: List[ImageWithSourceID] = []\n    for idx, (single_frame, frame_prediction) in enumerate(\n        zip(video_frame, predictions)\n    ):\n        image = _handle_frame_rendering(\n            frame=single_frame,\n            prediction=frame_prediction,\n            annotator=annotator,\n            display_size=display_size,\n            display_statistics=display_statistics,\n            fps_value=fps_value,\n        )\n        images.append((idx, image))\n    if sequential_input_provided:\n        on_frame_rendered((video_frame[0].source_id, images[0][1]))\n    else:\n        on_frame_rendered(images)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/","title":"stream","text":""},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream","title":"<code>Stream</code>","text":"<p>             Bases: <code>BaseInterface</code></p> <p>Roboflow defined stream interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>json_response</code> <code>bool</code> <p>Flag to toggle JSON response format.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model</code> <code>str | Callable</code> <p>The model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> <code>use_bytetrack</code> <code>bool</code> <p>Flag to use bytetrack,</p> <p>Methods:</p> Name Description <code>init_infer</code> <p>Initialize the inference with a test frame.</p> <code>preprocess_thread</code> <p>Preprocess incoming frames for inference.</p> <code>inference_request_thread</code> <p>Manage the inference requests.</p> <code>run_thread</code> <p>Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>class Stream(BaseInterface):\n\"\"\"Roboflow defined stream interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        json_response (bool): Flag to toggle JSON response format.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model (str|Callable): The model to be used.\n        stream_id (str): The ID of the stream to be used.\n        use_bytetrack (bool): Flag to use bytetrack,\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        enforce_fps: bool = ENFORCE_FPS,\n        iou_threshold: float = IOU_THRESHOLD,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model: Union[str, Callable] = MODEL_ID,\n        source: Union[int, str] = STREAM_ID,\n        use_bytetrack: bool = ENABLE_BYTE_TRACK,\n        use_main_thread: bool = False,\n        output_channel_order: str = \"RGB\",\n        on_prediction: Callable = None,\n        on_start: Callable = None,\n        on_stop: Callable = None,\n    ):\n\"\"\"Initialize the stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        logger.info(\"Initializing server\")\n\n        self.frame_count = 0\n        self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n        self.use_bytetrack = use_bytetrack\n\n        if source == \"webcam\":\n            stream_id = 0\n        else:\n            stream_id = source\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n\n        self.active_learning_middleware = NullActiveLearningMiddleware()\n        if isinstance(model, str):\n            self.model = get_model(model, self.api_key)\n            if ACTIVE_LEARNING_ENABLED:\n                self.active_learning_middleware = (\n                    ThreadingActiveLearningMiddleware.init(\n                        api_key=self.api_key,\n                        model_id=self.model_id,\n                        cache=cache,\n                    )\n                )\n            self.task_type = get_model_type(\n                model_id=self.model_id, api_key=self.api_key\n            )[0]\n        else:\n            self.model = model\n            self.task_type = \"unknown\"\n\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.use_main_thread = use_main_thread\n        self.output_channel_order = output_channel_order\n\n        self.inference_request_type = (\n            inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n        )\n\n        self.webcam_stream = WebcamStream(\n            stream_id=self.stream_id, enforce_fps=enforce_fps\n        )\n        logger.info(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.on_start_callbacks = []\n        self.on_stop_callbacks = [\n            lambda: self.active_learning_middleware.stop_registration_thread()\n        ]\n        self.on_prediction_callbacks = []\n\n        if on_prediction:\n            self.on_prediction_callbacks.append(on_prediction)\n\n        if on_start:\n            self.on_start_callbacks.append(on_start)\n\n        if on_stop:\n            self.on_stop_callbacks.append(on_stop)\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame = None\n        self.frame_cv = None\n        self.frame_id = None\n        logger.info(\"Server initialized with settings:\")\n        logger.info(f\"Stream ID: {self.stream_id}\")\n        logger.info(f\"Model ID: {self.model_id}\")\n        logger.info(f\"Enforce FPS: {enforce_fps}\")\n        logger.info(f\"Confidence: {self.confidence}\")\n        logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n        logger.info(f\"Max Candidates: {self.max_candidates}\")\n        logger.info(f\"Max Detections: {self.max_detections}\")\n\n        self.run_thread()\n\n    def on_start(self, callback):\n        self.on_start_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_start_callbacks.remove(callback)\n        return unsubscribe\n\n    def on_stop(self, callback):\n        self.on_stop_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_stop_callbacks.remove(callback)\n        return unsubscribe\n\n    def on_prediction(self, callback):\n        self.on_prediction_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_prediction_callbacks.remove(callback)\n        return unsubscribe\n\n    def init_infer(self):\n\"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        self.model.infer(\n            frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n        )\n        self.active_learning_middleware.start_registration_thread()\n\n    def preprocess_thread(self):\n\"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame_cv, frame_id = webcam_stream.read_opencv()\n                    if frame_id &gt; 0 and frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        self.frame = cv2.cvtColor(self.frame_cv, cv2.COLOR_BGR2RGB)\n                        self.preproc_result = self.model.preprocess(self.frame_cv)\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            traceback.print_exc()\n            logger.error(e)\n\n    def inference_request_thread(self):\n\"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        to registered callbacks.\n        \"\"\"\n        last_print = time.perf_counter()\n        print_ind = 0\n        while True:\n            if self.webcam_stream.stopped is True or self.stop:\n                while len(self.on_stop_callbacks) &gt; 0:\n                    # run each onStop callback only once from this thread\n                    cb = self.on_stop_callbacks.pop()\n                    cb()\n                break\n            if self.queue_control:\n                while len(self.on_start_callbacks) &gt; 0:\n                    # run each onStart callback only once from this thread\n                    cb = self.on_start_callbacks.pop()\n                    cb()\n\n                self.queue_control = False\n                frame_id = self.frame_id\n                inference_input = np.copy(self.frame_cv)\n                start = time.perf_counter()\n                predictions = self.model.predict(\n                    self.img_in,\n                )\n                predictions = self.model.postprocess(\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )[0]\n\n                self.active_learning_middleware.register(\n                    inference_input=inference_input,\n                    prediction=predictions.dict(by_alias=True, exclude_none=True),\n                    prediction_type=self.task_type,\n                )\n                if self.use_bytetrack:\n                    detections = sv.Detections.from_roboflow(\n                        predictions.dict(by_alias=True, exclude_none=True)\n                    )\n                    detections = self.byte_tracker.update_with_detections(detections)\n\n                    if detections.tracker_id is None:\n                        detections.tracker_id = np.array([], dtype=int)\n\n                    for pred, detect in zip(predictions.predictions, detections):\n                        pred.tracker_id = int(detect[4])\n                predictions.frame_id = frame_id\n                predictions = predictions.dict(by_alias=True, exclude_none=True)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                for cb in self.on_prediction_callbacks:\n                    if self.output_channel_order == \"BGR\":\n                        cb(predictions, self.frame_cv)\n                    else:\n                        cb(predictions, np.asarray(self.frame))\n\n                current = time.perf_counter()\n                self.webcam_stream.max_fps = 1 / (current - start)\n                logger.debug(f\"FPS: {self.webcam_stream.max_fps:.2f}\")\n\n                if time.perf_counter() - last_print &gt; 1:\n                    print_ind = (print_ind + 1) % 4\n                    last_print = time.perf_counter()\n\n    def run_thread(self):\n\"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        preprocess_thread.start()\n\n        if self.use_main_thread:\n            self.inference_request_thread()\n        else:\n            # start a thread that looks for the predictions\n            # and call the callbacks\n            inference_request_thread = threading.Thread(\n                target=self.inference_request_thread\n            )\n            inference_request_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.__init__","title":"<code>__init__(api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, enforce_fps=ENFORCE_FPS, iou_threshold=IOU_THRESHOLD, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model=MODEL_ID, source=STREAM_ID, use_bytetrack=ENABLE_BYTE_TRACK, use_main_thread=False, output_channel_order='RGB', on_prediction=None, on_start=None, on_stop=None)</code>","text":"<p>Initialize the stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    enforce_fps: bool = ENFORCE_FPS,\n    iou_threshold: float = IOU_THRESHOLD,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model: Union[str, Callable] = MODEL_ID,\n    source: Union[int, str] = STREAM_ID,\n    use_bytetrack: bool = ENABLE_BYTE_TRACK,\n    use_main_thread: bool = False,\n    output_channel_order: str = \"RGB\",\n    on_prediction: Callable = None,\n    on_start: Callable = None,\n    on_stop: Callable = None,\n):\n\"\"\"Initialize the stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    logger.info(\"Initializing server\")\n\n    self.frame_count = 0\n    self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n    self.use_bytetrack = use_bytetrack\n\n    if source == \"webcam\":\n        stream_id = 0\n    else:\n        stream_id = source\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n\n    self.active_learning_middleware = NullActiveLearningMiddleware()\n    if isinstance(model, str):\n        self.model = get_model(model, self.api_key)\n        if ACTIVE_LEARNING_ENABLED:\n            self.active_learning_middleware = (\n                ThreadingActiveLearningMiddleware.init(\n                    api_key=self.api_key,\n                    model_id=self.model_id,\n                    cache=cache,\n                )\n            )\n        self.task_type = get_model_type(\n            model_id=self.model_id, api_key=self.api_key\n        )[0]\n    else:\n        self.model = model\n        self.task_type = \"unknown\"\n\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.use_main_thread = use_main_thread\n    self.output_channel_order = output_channel_order\n\n    self.inference_request_type = (\n        inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n    )\n\n    self.webcam_stream = WebcamStream(\n        stream_id=self.stream_id, enforce_fps=enforce_fps\n    )\n    logger.info(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.on_start_callbacks = []\n    self.on_stop_callbacks = [\n        lambda: self.active_learning_middleware.stop_registration_thread()\n    ]\n    self.on_prediction_callbacks = []\n\n    if on_prediction:\n        self.on_prediction_callbacks.append(on_prediction)\n\n    if on_start:\n        self.on_start_callbacks.append(on_start)\n\n    if on_stop:\n        self.on_stop_callbacks.append(on_stop)\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame = None\n    self.frame_cv = None\n    self.frame_id = None\n    logger.info(\"Server initialized with settings:\")\n    logger.info(f\"Stream ID: {self.stream_id}\")\n    logger.info(f\"Model ID: {self.model_id}\")\n    logger.info(f\"Enforce FPS: {enforce_fps}\")\n    logger.info(f\"Confidence: {self.confidence}\")\n    logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n    logger.info(f\"Max Candidates: {self.max_candidates}\")\n    logger.info(f\"Max Detections: {self.max_detections}\")\n\n    self.run_thread()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results to registered callbacks.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def inference_request_thread(self):\n\"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    to registered callbacks.\n    \"\"\"\n    last_print = time.perf_counter()\n    print_ind = 0\n    while True:\n        if self.webcam_stream.stopped is True or self.stop:\n            while len(self.on_stop_callbacks) &gt; 0:\n                # run each onStop callback only once from this thread\n                cb = self.on_stop_callbacks.pop()\n                cb()\n            break\n        if self.queue_control:\n            while len(self.on_start_callbacks) &gt; 0:\n                # run each onStart callback only once from this thread\n                cb = self.on_start_callbacks.pop()\n                cb()\n\n            self.queue_control = False\n            frame_id = self.frame_id\n            inference_input = np.copy(self.frame_cv)\n            start = time.perf_counter()\n            predictions = self.model.predict(\n                self.img_in,\n            )\n            predictions = self.model.postprocess(\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )[0]\n\n            self.active_learning_middleware.register(\n                inference_input=inference_input,\n                prediction=predictions.dict(by_alias=True, exclude_none=True),\n                prediction_type=self.task_type,\n            )\n            if self.use_bytetrack:\n                detections = sv.Detections.from_roboflow(\n                    predictions.dict(by_alias=True, exclude_none=True)\n                )\n                detections = self.byte_tracker.update_with_detections(detections)\n\n                if detections.tracker_id is None:\n                    detections.tracker_id = np.array([], dtype=int)\n\n                for pred, detect in zip(predictions.predictions, detections):\n                    pred.tracker_id = int(detect[4])\n            predictions.frame_id = frame_id\n            predictions = predictions.dict(by_alias=True, exclude_none=True)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            for cb in self.on_prediction_callbacks:\n                if self.output_channel_order == \"BGR\":\n                    cb(predictions, self.frame_cv)\n                else:\n                    cb(predictions, np.asarray(self.frame))\n\n            current = time.perf_counter()\n            self.webcam_stream.max_fps = 1 / (current - start)\n            logger.debug(f\"FPS: {self.webcam_stream.max_fps:.2f}\")\n\n            if time.perf_counter() - last_print &gt; 1:\n                print_ind = (print_ind + 1) % 4\n                last_print = time.perf_counter()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def init_infer(self):\n\"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    self.model.infer(\n        frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n    )\n    self.active_learning_middleware.start_registration_thread()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def preprocess_thread(self):\n\"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame_cv, frame_id = webcam_stream.read_opencv()\n                if frame_id &gt; 0 and frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    self.frame = cv2.cvtColor(self.frame_cv, cv2.COLOR_BGR2RGB)\n                    self.preproc_result = self.model.preprocess(self.frame_cv)\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        traceback.print_exc()\n        logger.error(e)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def run_thread(self):\n\"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    preprocess_thread.start()\n\n    if self.use_main_thread:\n        self.inference_request_thread()\n    else:\n        # start a thread that looks for the predictions\n        # and call the callbacks\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n        inference_request_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/interfaces/stream/watchdog/","title":"watchdog","text":"<p>This module contains component intended to use in combination with <code>InferencePipeline</code> to ensure observability. Please consider them internal details of implementation.</p>"},{"location":"docs/reference/inference/core/interfaces/stream/watchdog/#inference.core.interfaces.stream.watchdog.BasePipelineWatchDog","title":"<code>BasePipelineWatchDog</code>","text":"<p>             Bases: <code>PipelineWatchDog</code></p> <p>Implementation to be used from single inference thread, as it keeps state assumed to represent status of consecutive stage of prediction process in latency monitor.</p> Source code in <code>inference/core/interfaces/stream/watchdog.py</code> <pre><code>class BasePipelineWatchDog(PipelineWatchDog):\n\"\"\"\n    Implementation to be used from single inference thread, as it keeps\n    state assumed to represent status of consecutive stage of prediction process\n    in latency monitor.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._video_sources: Optional[List[VideoSource]] = None\n        self._inference_throughput_monitor = sv.FPSMonitor()\n        self._latency_monitors: Dict[Optional[int], LatencyMonitor] = {}\n        self._stream_updates = deque(maxlen=MAX_UPDATES_CONTEXT)\n\n    def register_video_sources(self, video_sources: List[VideoSource]) -&gt; None:\n        self._video_sources = video_sources\n        for source in video_sources:\n            self._latency_monitors[source.source_id] = LatencyMonitor(\n                source_id=source.source_id\n            )\n\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        if status_update.severity.value &lt;= UpdateSeverity.DEBUG.value:\n            return None\n        self._stream_updates.append(status_update)\n\n    def on_model_inference_started(self, frames: List[VideoFrame]) -&gt; None:\n        for frame in frames:\n            self._latency_monitors[frame.source_id].register_inference_start(\n                frame_timestamp=frame.frame_timestamp,\n                frame_id=frame.frame_id,\n            )\n\n    def on_model_prediction_ready(self, frames: List[VideoFrame]) -&gt; None:\n        for frame in frames:\n            self._latency_monitors[frame.source_id].register_prediction_ready(\n                frame_timestamp=frame.frame_timestamp,\n                frame_id=frame.frame_id,\n            )\n            self._inference_throughput_monitor.tick()\n\n    def get_report(self) -&gt; PipelineStateReport:\n        sources_metadata = []\n        if self._video_sources is not None:\n            sources_metadata = [s.describe_source() for s in self._video_sources]\n        latency_reports = [\n            monitor.summarise_reports() for monitor in self._latency_monitors.values()\n        ]\n        return PipelineStateReport(\n            video_source_status_updates=list(self._stream_updates),\n            latency_reports=latency_reports,\n            inference_throughput=self._inference_throughput_monitor(),\n            sources_metadata=sources_metadata,\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/model_handlers/roboflow_models/","title":"roboflow_models","text":""},{"location":"docs/reference/inference/core/interfaces/stream/model_handlers/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/interfaces/stream/model_handlers/yolo_world/","title":"yolo_world","text":""},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/","title":"udp_stream","text":""},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream","title":"<code>UdpStream</code>","text":"<p>             Bases: <code>BaseInterface</code></p> <p>Roboflow defined UDP interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>ip_broadcast_addr</code> <code>str</code> <p>The IP address to broadcast to.</p> <code>ip_broadcast_port</code> <code>int</code> <p>The port to broadcast on.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model_id</code> <code>str</code> <p>The ID of the model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> <code>use_bytetrack</code> <code>bool</code> <p>Flag to use bytetrack,</p> <p>Methods:</p> Name Description <code>init_infer</code> <p>Initialize the inference with a test frame.</p> <code>preprocess_thread</code> <p>Preprocess incoming frames for inference.</p> <code>inference_request_thread</code> <p>Manage the inference requests.</p> <code>run_thread</code> <p>Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>class UdpStream(BaseInterface):\n\"\"\"Roboflow defined UDP interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        ip_broadcast_addr (str): The IP address to broadcast to.\n        ip_broadcast_port (int): The port to broadcast on.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model_id (str): The ID of the model to be used.\n        stream_id (str): The ID of the stream to be used.\n        use_bytetrack (bool): Flag to use bytetrack,\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        enforce_fps: bool = ENFORCE_FPS,\n        ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n        ip_broadcast_port: int = IP_BROADCAST_PORT,\n        iou_threshold: float = IOU_THRESHOLD,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model_id: str = MODEL_ID,\n        stream_id: Union[int, str] = STREAM_ID,\n        use_bytetrack: bool = ENABLE_BYTE_TRACK,\n    ):\n\"\"\"Initialize the UDP stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        logger.info(\"Initializing server\")\n\n        self.frame_count = 0\n        self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n        self.use_bytetrack = use_bytetrack\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model_id\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n        if not self.api_key:\n            raise ValueError(\n                f\"API key is missing. Either pass it explicitly to constructor, or use one of env variables: \"\n                f\"{API_KEY_ENV_NAMES}. Visit \"\n                f\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to generate \"\n                f\"the key.\"\n            )\n\n        self.model = get_model(self.model_id, self.api_key)\n        self.task_type = get_model_type(model_id=self.model_id, api_key=self.api_key)[0]\n        self.active_learning_middleware = NullActiveLearningMiddleware()\n        if ACTIVE_LEARNING_ENABLED:\n            self.active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n                api_key=self.api_key,\n                model_id=self.model_id,\n                cache=cache,\n            )\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.ip_broadcast_addr = ip_broadcast_addr\n        self.ip_broadcast_port = ip_broadcast_port\n\n        self.inference_request_type = (\n            inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n        )\n\n        self.UDPServerSocket = socket.socket(\n            family=socket.AF_INET, type=socket.SOCK_DGRAM\n        )\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n\n        self.webcam_stream = WebcamStream(\n            stream_id=self.stream_id, enforce_fps=enforce_fps\n        )\n        logger.info(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame_cv = None\n        self.frame_id = None\n        logger.info(\"Server initialized with settings:\")\n        logger.info(f\"Stream ID: {self.stream_id}\")\n        logger.info(f\"Model ID: {self.model_id}\")\n        logger.info(f\"Confidence: {self.confidence}\")\n        logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n        logger.info(f\"Max Candidates: {self.max_candidates}\")\n        logger.info(f\"Max Detections: {self.max_detections}\")\n\n    def init_infer(self):\n\"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        self.model.infer(\n            frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n        )\n        self.active_learning_middleware.start_registration_thread()\n\n    def preprocess_thread(self):\n\"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame_cv, frame_id = webcam_stream.read_opencv()\n                    if frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        self.preproc_result = self.model.preprocess(self.frame_cv)\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            logger.error(e)\n\n    def inference_request_thread(self):\n\"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        as a UDP broadcast.\n        \"\"\"\n        last_print = time.perf_counter()\n        print_ind = 0\n        print_chars = [\"|\", \"/\", \"-\", \"\\\\\"]\n        while True:\n            if self.stop:\n                break\n            if self.queue_control:\n                self.queue_control = False\n                frame_id = self.frame_id\n                inference_input = np.copy(self.frame_cv)\n                predictions = self.model.predict(\n                    self.img_in,\n                )\n                predictions = self.model.postprocess(\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )[0]\n                self.active_learning_middleware.register(\n                    inference_input=inference_input,\n                    prediction=predictions.dict(by_alias=True, exclude_none=True),\n                    prediction_type=self.task_type,\n                )\n                if self.use_bytetrack:\n                    detections = sv.Detections.from_roboflow(\n                        predictions.dict(by_alias=True), self.model.class_names\n                    )\n                    detections = self.byte_tracker.update_with_detections(detections)\n                    for pred, detect in zip(predictions.predictions, detections):\n                        pred.tracker_id = int(detect[4])\n                predictions.frame_id = frame_id\n                predictions = predictions.json(exclude_none=True, by_alias=True)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                bytesToSend = predictions.encode(\"utf-8\")\n                self.UDPServerSocket.sendto(\n                    bytesToSend,\n                    (\n                        self.ip_broadcast_addr,\n                        self.ip_broadcast_port,\n                    ),\n                )\n                if time.perf_counter() - last_print &gt; 1:\n                    print(f\"Streaming {print_chars[print_ind]}\", end=\"\\r\")\n                    print_ind = (print_ind + 1) % 4\n                    last_print = time.perf_counter()\n\n    def run_thread(self):\n\"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n\n        preprocess_thread.start()\n        inference_request_thread.start()\n\n        while True:\n            try:\n                time.sleep(10)\n            except KeyboardInterrupt:\n                logger.info(\"Stopping server...\")\n                self.stop = True\n                self.active_learning_middleware.stop_registration_thread()\n                time.sleep(3)\n                sys.exit(0)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.__init__","title":"<code>__init__(api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, enforce_fps=ENFORCE_FPS, ip_broadcast_addr=IP_BROADCAST_ADDR, ip_broadcast_port=IP_BROADCAST_PORT, iou_threshold=IOU_THRESHOLD, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model_id=MODEL_ID, stream_id=STREAM_ID, use_bytetrack=ENABLE_BYTE_TRACK)</code>","text":"<p>Initialize the UDP stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    enforce_fps: bool = ENFORCE_FPS,\n    ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n    ip_broadcast_port: int = IP_BROADCAST_PORT,\n    iou_threshold: float = IOU_THRESHOLD,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model_id: str = MODEL_ID,\n    stream_id: Union[int, str] = STREAM_ID,\n    use_bytetrack: bool = ENABLE_BYTE_TRACK,\n):\n\"\"\"Initialize the UDP stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    logger.info(\"Initializing server\")\n\n    self.frame_count = 0\n    self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n    self.use_bytetrack = use_bytetrack\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model_id\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n    if not self.api_key:\n        raise ValueError(\n            f\"API key is missing. Either pass it explicitly to constructor, or use one of env variables: \"\n            f\"{API_KEY_ENV_NAMES}. Visit \"\n            f\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to generate \"\n            f\"the key.\"\n        )\n\n    self.model = get_model(self.model_id, self.api_key)\n    self.task_type = get_model_type(model_id=self.model_id, api_key=self.api_key)[0]\n    self.active_learning_middleware = NullActiveLearningMiddleware()\n    if ACTIVE_LEARNING_ENABLED:\n        self.active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n            api_key=self.api_key,\n            model_id=self.model_id,\n            cache=cache,\n        )\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.ip_broadcast_addr = ip_broadcast_addr\n    self.ip_broadcast_port = ip_broadcast_port\n\n    self.inference_request_type = (\n        inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n    )\n\n    self.UDPServerSocket = socket.socket(\n        family=socket.AF_INET, type=socket.SOCK_DGRAM\n    )\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n\n    self.webcam_stream = WebcamStream(\n        stream_id=self.stream_id, enforce_fps=enforce_fps\n    )\n    logger.info(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame_cv = None\n    self.frame_id = None\n    logger.info(\"Server initialized with settings:\")\n    logger.info(f\"Stream ID: {self.stream_id}\")\n    logger.info(f\"Model ID: {self.model_id}\")\n    logger.info(f\"Confidence: {self.confidence}\")\n    logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n    logger.info(f\"Max Candidates: {self.max_candidates}\")\n    logger.info(f\"Max Detections: {self.max_detections}\")\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results as a UDP broadcast.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def inference_request_thread(self):\n\"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    as a UDP broadcast.\n    \"\"\"\n    last_print = time.perf_counter()\n    print_ind = 0\n    print_chars = [\"|\", \"/\", \"-\", \"\\\\\"]\n    while True:\n        if self.stop:\n            break\n        if self.queue_control:\n            self.queue_control = False\n            frame_id = self.frame_id\n            inference_input = np.copy(self.frame_cv)\n            predictions = self.model.predict(\n                self.img_in,\n            )\n            predictions = self.model.postprocess(\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )[0]\n            self.active_learning_middleware.register(\n                inference_input=inference_input,\n                prediction=predictions.dict(by_alias=True, exclude_none=True),\n                prediction_type=self.task_type,\n            )\n            if self.use_bytetrack:\n                detections = sv.Detections.from_roboflow(\n                    predictions.dict(by_alias=True), self.model.class_names\n                )\n                detections = self.byte_tracker.update_with_detections(detections)\n                for pred, detect in zip(predictions.predictions, detections):\n                    pred.tracker_id = int(detect[4])\n            predictions.frame_id = frame_id\n            predictions = predictions.json(exclude_none=True, by_alias=True)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            bytesToSend = predictions.encode(\"utf-8\")\n            self.UDPServerSocket.sendto(\n                bytesToSend,\n                (\n                    self.ip_broadcast_addr,\n                    self.ip_broadcast_port,\n                ),\n            )\n            if time.perf_counter() - last_print &gt; 1:\n                print(f\"Streaming {print_chars[print_ind]}\", end=\"\\r\")\n                print_ind = (print_ind + 1) % 4\n                last_print = time.perf_counter()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def init_infer(self):\n\"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    self.model.infer(\n        frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n    )\n    self.active_learning_middleware.start_registration_thread()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def preprocess_thread(self):\n\"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame_cv, frame_id = webcam_stream.read_opencv()\n                if frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    self.preproc_result = self.model.preprocess(self.frame_cv)\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        logger.error(e)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def run_thread(self):\n\"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    inference_request_thread = threading.Thread(\n        target=self.inference_request_thread\n    )\n\n    preprocess_thread.start()\n    inference_request_thread.start()\n\n    while True:\n        try:\n            time.sleep(10)\n        except KeyboardInterrupt:\n            logger.info(\"Stopping server...\")\n            self.stop = True\n            self.active_learning_middleware.stop_registration_thread()\n            time.sleep(3)\n            sys.exit(0)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/active_learning/","title":"active_learning","text":""},{"location":"docs/reference/inference/core/managers/base/","title":"base","text":""},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager","title":"<code>ModelManager</code>","text":"<p>Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>class ModelManager:\n\"\"\"Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.\"\"\"\n\n    def __init__(self, model_registry: ModelRegistry, models: Optional[dict] = None):\n        self.model_registry = model_registry\n        self._models: Dict[str, Model] = models if models is not None else {}\n\n    def init_pingback(self):\n\"\"\"Initializes pingback mechanism.\"\"\"\n        self.num_errors = 0  # in the device\n        self.uuid = ROBOFLOW_SERVER_UUID\n        if METRICS_ENABLED:\n            self.pingback = PingbackInfo(self)\n            self.pingback.start()\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ) -&gt; None:\n\"\"\"Adds a new model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - Adding model with model_id={model_id}, model_id_alias={model_id_alias}\"\n        )\n        resolved_identifier = model_id if model_id_alias is None else model_id_alias\n        if resolved_identifier in self._models:\n            logger.debug(\n                f\"ModelManager - model with model_id={resolved_identifier} is already loaded.\"\n            )\n            return\n        logger.debug(\"ModelManager - model initialisation...\")\n        model = self.model_registry.get_model(resolved_identifier, api_key)(\n            model_id=model_id,\n            api_key=api_key,\n        )\n        logger.debug(\"ModelManager - model successfully loaded.\")\n        self._models[resolved_identifier] = model\n\n    def check_for_model(self, model_id: str) -&gt; None:\n\"\"\"Checks whether the model with the given ID is in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Raises:\n            InferenceModelNotFound: If the model is not found in the manager.\n        \"\"\"\n        if model_id not in self:\n            raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Runs inference on the specified model with the given request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - inference from request started for model_id={model_id}.\"\n        )\n        try:\n            rtn_val = await self.model_infer(\n                model_id=model_id, request=request, **kwargs\n            )\n            logger.debug(\n                f\"ModelManager - inference from request finished for model_id={model_id}.\"\n            )\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE:\n                logger.debug(\n                    f\"ModelManager - caching inference request started for model_id={model_id}\"\n                )\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                if (\n                    hasattr(request, \"image\")\n                    and hasattr(request.image, \"type\")\n                    and request.image.type == \"numpy\"\n                ):\n                    request.image.value = str(request.image.value)\n                cache.zadd(\n                    f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value=to_cachable_inference_item(request, rtn_val),\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                logger.debug(\n                    f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                )\n            return rtn_val\n        except Exception as e:\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE:\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                cache.zadd(\n                    f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value={\n                        \"request\": jsonable_encoder(\n                            request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                        ),\n                        \"error\": str(e),\n                    },\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n            raise\n\n    async def model_infer(self, model_id: str, request: InferenceRequest, **kwargs):\n        self.check_for_model(model_id)\n        return self._models[model_id].infer_from_request(request)\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Creates a response object from the model's predictions.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (List[List[float]]): The model's predictions.\n\n        Returns:\n            InferenceResponse: The created response object.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].make_response(predictions, *args, **kwargs)\n\n    def postprocess(\n        self,\n        model_id: str,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        *args,\n        **kwargs,\n    ) -&gt; List[List[float]]:\n\"\"\"Processes the model's predictions after inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (np.ndarray): The model's predictions.\n\n        Returns:\n            List[List[float]]: The post-processed predictions.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].postprocess(\n            predictions, preprocess_return_metadata, *args, **kwargs\n        )\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n\"\"\"Runs prediction on the specified model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            np.ndarray: The predictions from the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        self._models[model_id].metrics[\"num_inferences\"] += 1\n        tic = time.perf_counter()\n        res = self._models[model_id].predict(*args, **kwargs)\n        toc = time.perf_counter()\n        self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n        return res\n\n    def preprocess(\n        self, model_id: str, request: InferenceRequest\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n\"\"\"Preprocesses the request before inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].preprocess(**request.dict())\n\n    def get_class_names(self, model_id):\n\"\"\"Retrieves the class names for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            List[str]: The class names of the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].class_names\n\n    def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n\"\"\"Retrieves the task type for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type of the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].task_type\n\n    def remove(self, model_id: str) -&gt; None:\n\"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n        \"\"\"\n        try:\n            logger.debug(f\"Removing model {model_id} from base model manager\")\n            self.check_for_model(model_id)\n            self._models[model_id].clear_cache()\n            del self._models[model_id]\n        except InferenceModelNotFound:\n            logger.warning(\n                f\"Attempted to remove model with id {model_id}, but it is not loaded. Skipping...\"\n            )\n\n    def clear(self) -&gt; None:\n\"\"\"Removes all models from the manager.\"\"\"\n        for model_id in list(self.keys()):\n            self.remove(model_id)\n\n    def __contains__(self, model_id: str) -&gt; bool:\n\"\"\"Checks if the model is contained in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: Whether the model is in the manager.\n        \"\"\"\n        return model_id in self._models\n\n    def __getitem__(self, key: str) -&gt; Model:\n\"\"\"Retrieve a model from the manager by key.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model corresponding to the key.\n        \"\"\"\n        self.check_for_model(model_id=key)\n        return self._models[key]\n\n    def __len__(self) -&gt; int:\n\"\"\"Retrieve the number of models in the manager.\n\n        Returns:\n            int: The number of models in the manager.\n        \"\"\"\n        return len(self._models)\n\n    def keys(self):\n\"\"\"Retrieve the keys (model identifiers) from the manager.\n\n        Returns:\n            List[str]: The keys of the models in the manager.\n        \"\"\"\n        return self._models.keys()\n\n    def models(self) -&gt; Dict[str, Model]:\n\"\"\"Retrieve the models dictionary from the manager.\n\n        Returns:\n            Dict[str, Model]: The keys of the models in the manager.\n        \"\"\"\n        return self._models\n\n    def describe_models(self) -&gt; List[ModelDescription]:\n        return [\n            ModelDescription(\n                model_id=model_id,\n                task_type=model.task_type,\n                batch_size=getattr(model, \"batch_size\", None),\n                input_width=getattr(model, \"img_size_w\", None),\n                input_height=getattr(model, \"img_size_h\", None),\n            )\n            for model_id, model in self._models.items()\n        ]\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if the model is contained in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the model is in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __contains__(self, model_id: str) -&gt; bool:\n\"\"\"Checks if the model is contained in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: Whether the model is in the manager.\n    \"\"\"\n    return model_id in self._models\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve a model from the manager by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model corresponding to the key.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n\"\"\"Retrieve a model from the manager by key.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model corresponding to the key.\n    \"\"\"\n    self.check_for_model(model_id=key)\n    return self._models[key]\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__len__","title":"<code>__len__()</code>","text":"<p>Retrieve the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Retrieve the number of models in the manager.\n\n    Returns:\n        int: The number of models in the manager.\n    \"\"\"\n    return len(self._models)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a new model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n) -&gt; None:\n\"\"\"Adds a new model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - Adding model with model_id={model_id}, model_id_alias={model_id_alias}\"\n    )\n    resolved_identifier = model_id if model_id_alias is None else model_id_alias\n    if resolved_identifier in self._models:\n        logger.debug(\n            f\"ModelManager - model with model_id={resolved_identifier} is already loaded.\"\n        )\n        return\n    logger.debug(\"ModelManager - model initialisation...\")\n    model = self.model_registry.get_model(resolved_identifier, api_key)(\n        model_id=model_id,\n        api_key=api_key,\n    )\n    logger.debug(\"ModelManager - model successfully loaded.\")\n    self._models[resolved_identifier] = model\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.check_for_model","title":"<code>check_for_model(model_id)</code>","text":"<p>Checks whether the model with the given ID is in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Raises:</p> Type Description <code>InferenceModelNotFound</code> <p>If the model is not found in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def check_for_model(self, model_id: str) -&gt; None:\n\"\"\"Checks whether the model with the given ID is in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Raises:\n        InferenceModelNotFound: If the model is not found in the manager.\n    \"\"\"\n    if model_id not in self:\n        raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.clear","title":"<code>clear()</code>","text":"<p>Removes all models from the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Removes all models from the manager.\"\"\"\n    for model_id in list(self.keys()):\n        self.remove(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Retrieves the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List[str]: The class names of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_class_names(self, model_id):\n\"\"\"Retrieves the class names for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        List[str]: The class names of the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].class_names\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.get_task_type","title":"<code>get_task_type(model_id, api_key=None)</code>","text":"<p>Retrieves the task type for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n\"\"\"Retrieves the task type for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type of the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].task_type\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Runs inference on the specified model with the given request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n\"\"\"Runs inference on the specified model with the given request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - inference from request started for model_id={model_id}.\"\n    )\n    try:\n        rtn_val = await self.model_infer(\n            model_id=model_id, request=request, **kwargs\n        )\n        logger.debug(\n            f\"ModelManager - inference from request finished for model_id={model_id}.\"\n        )\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE:\n            logger.debug(\n                f\"ModelManager - caching inference request started for model_id={model_id}\"\n            )\n            cache.zadd(\n                f\"models\",\n                value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            if (\n                hasattr(request, \"image\")\n                and hasattr(request.image, \"type\")\n                and request.image.type == \"numpy\"\n            ):\n                request.image.value = str(request.image.value)\n            cache.zadd(\n                f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                value=to_cachable_inference_item(request, rtn_val),\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            logger.debug(\n                f\"ModelManager - caching inference request finished for model_id={model_id}\"\n            )\n        return rtn_val\n    except Exception as e:\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE:\n            cache.zadd(\n                f\"models\",\n                value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            cache.zadd(\n                f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                value={\n                    \"request\": jsonable_encoder(\n                        request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                    ),\n                    \"error\": str(e),\n                },\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n        raise\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.init_pingback","title":"<code>init_pingback()</code>","text":"<p>Initializes pingback mechanism.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def init_pingback(self):\n\"\"\"Initializes pingback mechanism.\"\"\"\n    self.num_errors = 0  # in the device\n    self.uuid = ROBOFLOW_SERVER_UUID\n    if METRICS_ENABLED:\n        self.pingback = PingbackInfo(self)\n        self.pingback.start()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.keys","title":"<code>keys()</code>","text":"<p>Retrieve the keys (model identifiers) from the manager.</p> <p>Returns:</p> Type Description <p>List[str]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def keys(self):\n\"\"\"Retrieve the keys (model identifiers) from the manager.\n\n    Returns:\n        List[str]: The keys of the models in the manager.\n    \"\"\"\n    return self._models.keys()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.make_response","title":"<code>make_response(model_id, predictions, *args, **kwargs)</code>","text":"<p>Creates a response object from the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>List[List[float]]</code> <p>The model's predictions.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The created response object.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def make_response(\n    self, model_id: str, predictions: List[List[float]], *args, **kwargs\n) -&gt; InferenceResponse:\n\"\"\"Creates a response object from the model's predictions.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (List[List[float]]): The model's predictions.\n\n    Returns:\n        InferenceResponse: The created response object.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].make_response(predictions, *args, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.models","title":"<code>models()</code>","text":"<p>Retrieve the models dictionary from the manager.</p> <p>Returns:</p> Type Description <code>Dict[str, Model]</code> <p>Dict[str, Model]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def models(self) -&gt; Dict[str, Model]:\n\"\"\"Retrieve the models dictionary from the manager.\n\n    Returns:\n        Dict[str, Model]: The keys of the models in the manager.\n    \"\"\"\n    return self._models\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.postprocess","title":"<code>postprocess(model_id, predictions, preprocess_return_metadata, *args, **kwargs)</code>","text":"<p>Processes the model's predictions after inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>ndarray</code> <p>The model's predictions.</p> required <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: The post-processed predictions.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def postprocess(\n    self,\n    model_id: str,\n    predictions: Tuple[np.ndarray, ...],\n    preprocess_return_metadata: PreprocessReturnMetadata,\n    *args,\n    **kwargs,\n) -&gt; List[List[float]]:\n\"\"\"Processes the model's predictions after inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (np.ndarray): The model's predictions.\n\n    Returns:\n        List[List[float]]: The post-processed predictions.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].postprocess(\n        predictions, preprocess_return_metadata, *args, **kwargs\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.predict","title":"<code>predict(model_id, *args, **kwargs)</code>","text":"<p>Runs prediction on the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>np.ndarray: The predictions from the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n\"\"\"Runs prediction on the specified model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        np.ndarray: The predictions from the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    self._models[model_id].metrics[\"num_inferences\"] += 1\n    tic = time.perf_counter()\n    res = self._models[model_id].predict(*args, **kwargs)\n    toc = time.perf_counter()\n    self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Preprocesses the request before inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, PreprocessReturnMetadata]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def preprocess(\n    self, model_id: str, request: InferenceRequest\n) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n\"\"\"Preprocesses the request before inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].preprocess(**request.dict())\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def remove(self, model_id: str) -&gt; None:\n\"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n    \"\"\"\n    try:\n        logger.debug(f\"Removing model {model_id} from base model manager\")\n        self.check_for_model(model_id)\n        self._models[model_id].clear_cache()\n        del self._models[model_id]\n    except InferenceModelNotFound:\n        logger.warning(\n            f\"Attempted to remove model with id {model_id}, but it is not loaded. Skipping...\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/managers/metrics/","title":"metrics","text":""},{"location":"docs/reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_model_metrics","title":"<code>get_model_metrics(inference_server_id, model_id, min=-1, max=float('inf'))</code>","text":"<p>Gets the metrics for a given model between a specified time range.</p> <p>Parameters:</p> Name Type Description Default <code>device_id</code> <code>str</code> <p>The identifier of the device.</p> required <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>start</code> <code>float</code> <p>The starting timestamp of the time range. Defaults to -1.</p> required <code>stop</code> <code>float</code> <p>The ending timestamp of the time range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the metrics of the model:   - num_inferences (int): The number of inferences made.   - avg_inference_time (float): The average inference time.   - num_errors (int): The number of errors occurred.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_model_metrics(\n    inference_server_id: str, model_id: str, min: float = -1, max: float = float(\"inf\")\n) -&gt; dict:\n\"\"\"\n    Gets the metrics for a given model between a specified time range.\n\n    Args:\n        device_id (str): The identifier of the device.\n        model_id (str): The identifier of the model.\n        start (float, optional): The starting timestamp of the time range. Defaults to -1.\n        stop (float, optional): The ending timestamp of the time range. Defaults to float(\"inf\").\n\n    Returns:\n        dict: A dictionary containing the metrics of the model:\n              - num_inferences (int): The number of inferences made.\n              - avg_inference_time (float): The average inference time.\n              - num_errors (int): The number of errors occurred.\n    \"\"\"\n    now = time.time()\n    inferences_with_times = cache.zrangebyscore(\n        f\"inference:{inference_server_id}:{model_id}\", min=min, max=max, withscores=True\n    )\n    num_inferences = len(inferences_with_times)\n    inference_times = []\n    for inference, t in inferences_with_times:\n        response = inference[\"response\"]\n        if isinstance(response, list):\n            times = [r[\"time\"] for r in response if \"time\" in r]\n            inference_times.extend(times)\n        else:\n            if \"time\" in response:\n                inference_times.append(response[\"time\"])\n    avg_inference_time = (\n        sum(inference_times) / len(inference_times) if len(inference_times) &gt; 0 else 0\n    )\n    errors_with_times = cache.zrangebyscore(\n        f\"error:{inference_server_id}:{model_id}\", min=min, max=max, withscores=True\n    )\n    num_errors = len(errors_with_times)\n    return {\n        \"num_inferences\": num_inferences,\n        \"avg_inference_time\": avg_inference_time,\n        \"num_errors\": num_errors,\n    }\n</code></pre>"},{"location":"docs/reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Collects system information such as platform, architecture, hostname, IP address, MAC address, and processor details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing detailed system information.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_system_info() -&gt; dict:\n\"\"\"Collects system information such as platform, architecture, hostname, IP address, MAC address, and processor details.\n\n    Returns:\n        dict: A dictionary containing detailed system information.\n    \"\"\"\n    info = {}\n    try:\n        info[\"platform\"] = platform.system()\n        info[\"platform_release\"] = platform.release()\n        info[\"platform_version\"] = platform.version()\n        info[\"architecture\"] = platform.machine()\n        info[\"hostname\"] = socket.gethostname()\n        info[\"ip_address\"] = socket.gethostbyname(socket.gethostname())\n        info[\"mac_address\"] = \":\".join(re.findall(\"..\", \"%012x\" % uuid.getnode()))\n        info[\"processor\"] = platform.processor()\n        return info\n    except Exception as e:\n        logger.exception(e)\n    finally:\n        return info\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/","title":"pingback","text":""},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo","title":"<code>PingbackInfo</code>","text":"<p>Class responsible for managing pingback information for Roboflow.</p> <p>This class initializes a scheduler to periodically post data to Roboflow, containing information about the models, container, and device.</p> <p>Attributes:</p> Name Type Description <code>scheduler</code> <code>BackgroundScheduler</code> <p>A scheduler for running jobs in the background.</p> <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> <code>process_startup_time</code> <code>str</code> <p>Unix timestamp indicating when the process started.</p> <code>METRICS_URL</code> <code>str</code> <p>URL to send the pingback data to.</p> <code>system_info</code> <code>dict</code> <p>Information about the system.</p> <code>window_start_timestamp</code> <code>str</code> <p>Unix timestamp indicating the start of the current window.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>class PingbackInfo:\n\"\"\"Class responsible for managing pingback information for Roboflow.\n\n    This class initializes a scheduler to periodically post data to Roboflow, containing information about the models,\n    container, and device.\n\n    Attributes:\n        scheduler (BackgroundScheduler): A scheduler for running jobs in the background.\n        model_manager (ModelManager): Reference to the model manager object.\n        process_startup_time (str): Unix timestamp indicating when the process started.\n        METRICS_URL (str): URL to send the pingback data to.\n        system_info (dict): Information about the system.\n        window_start_timestamp (str): Unix timestamp indicating the start of the current window.\n    \"\"\"\n\n    def __init__(self, manager):\n\"\"\"Initializes PingbackInfo with the given manager.\n\n        Args:\n            manager (ModelManager): Reference to the model manager object.\n        \"\"\"\n        try:\n            self.scheduler = BackgroundScheduler()\n            self.model_manager = manager\n            self.process_startup_time = str(int(time.time()))\n            logger.debug(\n                \"UUID: \" + self.model_manager.uuid\n            )  # To correlate with UI container view\n            self.window_start_timestamp = str(int(time.time()))\n            context = {\n                \"api_key\": API_KEY,\n                \"timestamp\": str(int(time.time())),\n                \"device_id\": GLOBAL_DEVICE_ID,\n                \"inference_server_id\": GLOBAL_INFERENCE_SERVER_ID,\n                \"inference_server_version\": __version__,\n                \"tags\": TAGS,\n            }\n            self.environment_info = context | get_system_info()\n        except Exception as e:\n            logger.debug(\n                \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n                + str(e)\n            )\n\n    def start(self):\n\"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n        If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n        \"\"\"\n        if METRICS_ENABLED == False:\n            logger.warning(\n                \"Metrics reporting to Roboflow is disabled; not sending back stats to Roboflow.\"\n            )\n            return\n        try:\n            self.scheduler.add_job(\n                self.post_data,\n                \"interval\",\n                seconds=METRICS_INTERVAL,\n                args=[self.model_manager],\n            )\n            self.scheduler.start()\n        except Exception as e:\n            logger.debug(e)\n\n    def stop(self):\n\"\"\"Stops the scheduler.\"\"\"\n        self.scheduler.shutdown()\n\n    def post_data(self, model_manager):\n\"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n        Args:\n            model_manager (ModelManager): Reference to the model manager object.\n\n        The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n        \"\"\"\n        all_data = self.environment_info.copy()\n        all_data[\"inference_results\"] = []\n        try:\n            now = time.time()\n            start = now - METRICS_INTERVAL\n            for model_id in model_manager.models():\n                results = get_inference_results_for_model(\n                    GLOBAL_INFERENCE_SERVER_ID, model_id, min=start, max=now\n                )\n                all_data[\"inference_results\"] = all_data[\"inference_results\"] + results\n            res = requests.post(wrap_url(METRICS_URL), json=all_data)\n            try:\n                api_key_safe_raise_for_status(response=res)\n                logger.debug(\n                    \"Sent metrics to Roboflow {} at {}.\".format(\n                        METRICS_URL, str(all_data)\n                    )\n                )\n            except Exception as e:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable.\"\n                )\n\n        except Exception as e:\n            try:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}. Data was: {all_data}\"\n                )\n                traceback.print_exc()\n\n            except Exception as e2:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}.\"\n                )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.__init__","title":"<code>__init__(manager)</code>","text":"<p>Initializes PingbackInfo with the given manager.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def __init__(self, manager):\n\"\"\"Initializes PingbackInfo with the given manager.\n\n    Args:\n        manager (ModelManager): Reference to the model manager object.\n    \"\"\"\n    try:\n        self.scheduler = BackgroundScheduler()\n        self.model_manager = manager\n        self.process_startup_time = str(int(time.time()))\n        logger.debug(\n            \"UUID: \" + self.model_manager.uuid\n        )  # To correlate with UI container view\n        self.window_start_timestamp = str(int(time.time()))\n        context = {\n            \"api_key\": API_KEY,\n            \"timestamp\": str(int(time.time())),\n            \"device_id\": GLOBAL_DEVICE_ID,\n            \"inference_server_id\": GLOBAL_INFERENCE_SERVER_ID,\n            \"inference_server_version\": __version__,\n            \"tags\": TAGS,\n        }\n        self.environment_info = context | get_system_info()\n    except Exception as e:\n        logger.debug(\n            \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n            + str(e)\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.post_data","title":"<code>post_data(model_manager)</code>","text":"<p>Posts data to Roboflow about the models, container, device, and other relevant metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required <p>The data is collected and reset for the next window, and a POST request is made to the pingback URL.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def post_data(self, model_manager):\n\"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n    Args:\n        model_manager (ModelManager): Reference to the model manager object.\n\n    The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n    \"\"\"\n    all_data = self.environment_info.copy()\n    all_data[\"inference_results\"] = []\n    try:\n        now = time.time()\n        start = now - METRICS_INTERVAL\n        for model_id in model_manager.models():\n            results = get_inference_results_for_model(\n                GLOBAL_INFERENCE_SERVER_ID, model_id, min=start, max=now\n            )\n            all_data[\"inference_results\"] = all_data[\"inference_results\"] + results\n        res = requests.post(wrap_url(METRICS_URL), json=all_data)\n        try:\n            api_key_safe_raise_for_status(response=res)\n            logger.debug(\n                \"Sent metrics to Roboflow {} at {}.\".format(\n                    METRICS_URL, str(all_data)\n                )\n            )\n        except Exception as e:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable.\"\n            )\n\n    except Exception as e:\n        try:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}. Data was: {all_data}\"\n            )\n            traceback.print_exc()\n\n        except Exception as e2:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}.\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.start","title":"<code>start()</code>","text":"<p>Starts the scheduler to periodically post data to Roboflow.</p> <p>If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def start(self):\n\"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n    If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n    \"\"\"\n    if METRICS_ENABLED == False:\n        logger.warning(\n            \"Metrics reporting to Roboflow is disabled; not sending back stats to Roboflow.\"\n        )\n        return\n    try:\n        self.scheduler.add_job(\n            self.post_data,\n            \"interval\",\n            seconds=METRICS_INTERVAL,\n            args=[self.model_manager],\n        )\n        self.scheduler.start()\n    except Exception as e:\n        logger.debug(e)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.stop","title":"<code>stop()</code>","text":"<p>Stops the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def stop(self):\n\"\"\"Stops the scheduler.\"\"\"\n    self.scheduler.shutdown()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/stub_loader/","title":"stub_loader","text":""},{"location":"docs/reference/inference/core/managers/stub_loader/#inference.core.managers.stub_loader.StubLoaderManager","title":"<code>StubLoaderManager</code>","text":"<p>             Bases: <code>ModelManager</code></p> Source code in <code>inference/core/managers/stub_loader.py</code> <pre><code>class StubLoaderManager(ModelManager):\n    def add_model(self, model_id: str, api_key: str, model_id_alias=None) -&gt; None:\n\"\"\"Adds a new model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        if model_id in self._models:\n            return\n        model_class = self.model_registry.get_model(\n            model_id_alias if model_id_alias is not None else model_id, api_key\n        )\n        model = model_class(model_id=model_id, api_key=api_key, load_weights=False)\n        self._models[model_id] = model\n</code></pre>"},{"location":"docs/reference/inference/core/managers/stub_loader/#inference.core.managers.stub_loader.StubLoaderManager.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a new model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/stub_loader.py</code> <pre><code>def add_model(self, model_id: str, api_key: str, model_id_alias=None) -&gt; None:\n\"\"\"Adds a new model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    if model_id in self._models:\n        return\n    model_class = self.model_registry.get_model(\n        model_id_alias if model_id_alias is not None else model_id, api_key\n    )\n    model = model_class(model_id=model_id, api_key=api_key, load_weights=False)\n    self._models[model_id] = model\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/","title":"base","text":""},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator","title":"<code>ModelManagerDecorator</code>","text":"<p>             Bases: <code>ModelManager</code></p> <p>Basic decorator, it acts like a <code>ModelManager</code> and contains a <code>ModelManager</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Instance of a ModelManager.</p> required <p>Methods:</p> Name Description <code>add_model</code> <p>Adds a model to the manager.</p> <code>infer</code> <p>Processes a complete inference request.</p> <code>infer_only</code> <p>Performs only the inference part of a request.</p> <code>preprocess</code> <p>Processes the preprocessing part of a request.</p> <code>get_task_type</code> <p>Gets the task type associated with a model.</p> <code>get_class_names</code> <p>Gets the class names for a given model.</p> <code>remove</code> <p>Removes a model from the manager.</p> <code>__len__</code> <p>Returns the number of models in the manager.</p> <code>__getitem__</code> <p>Retrieves a model by its ID.</p> <code>__contains__</code> <p>Checks if a model exists in the manager.</p> <code>keys</code> <p>Returns the keys (model IDs) from the manager.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>class ModelManagerDecorator(ModelManager):\n\"\"\"Basic decorator, it acts like a `ModelManager` and contains a `ModelManager`.\n\n    Args:\n        model_manager (ModelManager): Instance of a ModelManager.\n\n    Methods:\n        add_model: Adds a model to the manager.\n        infer: Processes a complete inference request.\n        infer_only: Performs only the inference part of a request.\n        preprocess: Processes the preprocessing part of a request.\n        get_task_type: Gets the task type associated with a model.\n        get_class_names: Gets the class names for a given model.\n        remove: Removes a model from the manager.\n        __len__: Returns the number of models in the manager.\n        __getitem__: Retrieves a model by its ID.\n        __contains__: Checks if a model exists in the manager.\n        keys: Returns the keys (model IDs) from the manager.\n    \"\"\"\n\n    @property\n    def _models(self):\n        raise ValueError(\"Should only be accessing self.model_manager._models\")\n\n    @property\n    def model_registry(self):\n        raise ValueError(\"Should only be accessing self.model_manager.model_registry\")\n\n    def __init__(self, model_manager: ModelManager):\n\"\"\"Initializes the decorator with an instance of a ModelManager.\"\"\"\n        self.model_manager = model_manager\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ):\n\"\"\"Adds a model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        if model_id in self:\n            return\n        self.model_manager.add_model(model_id, api_key, model_id_alias=model_id_alias)\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Processes a complete inference request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        return await self.model_manager.infer_from_request(model_id, request, **kwargs)\n\n    def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n\"\"\"Performs only the inference part of a request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request: The request to process.\n            img_in: Input image.\n            img_dims: Image dimensions.\n            batch_size (int, optional): Batch size.\n\n        Returns:\n            Response from the inference-only operation.\n        \"\"\"\n        return self.model_manager.infer_only(\n            model_id, request, img_in, img_dims, batch_size\n        )\n\n    def preprocess(self, model_id: str, request: InferenceRequest):\n\"\"\"Processes the preprocessing part of a request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n        \"\"\"\n        return self.model_manager.preprocess(model_id, request)\n\n    def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n\"\"\"Gets the task type associated with a model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type.\n        \"\"\"\n        if api_key is None:\n            api_key = API_KEY\n        return self.model_manager.get_task_type(model_id, api_key=api_key)\n\n    def get_class_names(self, model_id):\n\"\"\"Gets the class names for a given model.\n\n        Args:\n            model_id: The identifier of the model.\n\n        Returns:\n            List of class names.\n        \"\"\"\n        return self.model_manager.get_class_names(model_id)\n\n    def remove(self, model_id: str) -&gt; Model:\n\"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            Model: The removed model.\n        \"\"\"\n        return self.model_manager.remove(model_id)\n\n    def __len__(self) -&gt; int:\n\"\"\"Returns the number of models in the manager.\n\n        Returns:\n            int: Number of models.\n        \"\"\"\n        return len(self.model_manager)\n\n    def __getitem__(self, key: str) -&gt; Model:\n\"\"\"Retrieves a model by its ID.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model instance.\n        \"\"\"\n        return self.model_manager[key]\n\n    def __contains__(self, model_id: str):\n\"\"\"Checks if a model exists in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: True if the model exists, False otherwise.\n        \"\"\"\n        return model_id in self.model_manager\n\n    def keys(self):\n\"\"\"Returns the keys (model IDs) from the manager.\n\n        Returns:\n            List of keys (model IDs).\n        \"\"\"\n        return self.model_manager.keys()\n\n    def models(self):\n        return self.model_manager.models()\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        return self.model_manager.predict(model_id, *args, **kwargs)\n\n    def postprocess(\n        self,\n        model_id: str,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        *args,\n        **kwargs\n    ) -&gt; List[List[float]]:\n        return self.model_manager.postprocess(\n            model_id, predictions, preprocess_return_metadata, *args, **kwargs\n        )\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n        return self.model_manager.make_response(model_id, predictions, *args, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if a model exists in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model exists, False otherwise.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __contains__(self, model_id: str):\n\"\"\"Checks if a model exists in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: True if the model exists, False otherwise.\n    \"\"\"\n    return model_id in self.model_manager\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieves a model by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model instance.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n\"\"\"Retrieves a model by its ID.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model instance.\n    \"\"\"\n    return self.model_manager[key]\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__init__","title":"<code>__init__(model_manager)</code>","text":"<p>Initializes the decorator with an instance of a ModelManager.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __init__(self, model_manager: ModelManager):\n\"\"\"Initializes the decorator with an instance of a ModelManager.\"\"\"\n    self.model_manager = model_manager\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of models.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Returns the number of models in the manager.\n\n    Returns:\n        int: Number of models.\n    \"\"\"\n    return len(self.model_manager)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n):\n\"\"\"Adds a model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    if model_id in self:\n        return\n    self.model_manager.add_model(model_id, api_key, model_id_alias=model_id_alias)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Gets the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List of class names.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def get_class_names(self, model_id):\n\"\"\"Gets the class names for a given model.\n\n    Args:\n        model_id: The identifier of the model.\n\n    Returns:\n        List of class names.\n    \"\"\"\n    return self.model_manager.get_class_names(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.get_task_type","title":"<code>get_task_type(model_id, api_key=None)</code>","text":"<p>Gets the task type associated with a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n\"\"\"Gets the task type associated with a model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type.\n    \"\"\"\n    if api_key is None:\n        api_key = API_KEY\n    return self.model_manager.get_task_type(model_id, api_key=api_key)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n\"\"\"Processes a complete inference request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    return await self.model_manager.infer_from_request(model_id, request, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_only","title":"<code>infer_only(model_id, request, img_in, img_dims, batch_size=None)</code>","text":"<p>Performs only the inference part of a request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <p>The request to process.</p> required <code>img_in</code> <p>Input image.</p> required <code>img_dims</code> <p>Image dimensions.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the inference-only operation.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n\"\"\"Performs only the inference part of a request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request: The request to process.\n        img_in: Input image.\n        img_dims: Image dimensions.\n        batch_size (int, optional): Batch size.\n\n    Returns:\n        Response from the inference-only operation.\n    \"\"\"\n    return self.model_manager.infer_only(\n        model_id, request, img_in, img_dims, batch_size\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.keys","title":"<code>keys()</code>","text":"<p>Returns the keys (model IDs) from the manager.</p> <p>Returns:</p> Type Description <p>List of keys (model IDs).</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def keys(self):\n\"\"\"Returns the keys (model IDs) from the manager.\n\n    Returns:\n        List of keys (model IDs).\n    \"\"\"\n    return self.model_manager.keys()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Processes the preprocessing part of a request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def preprocess(self, model_id: str, request: InferenceRequest):\n\"\"\"Processes the preprocessing part of a request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n    \"\"\"\n    return self.model_manager.preprocess(model_id, request)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The removed model.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def remove(self, model_id: str) -&gt; Model:\n\"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        Model: The removed model.\n    \"\"\"\n    return self.model_manager.remove(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/","title":"fixed_size_cache","text":""},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache","title":"<code>WithFixedSizeCache</code>","text":"<p>             Bases: <code>ModelManagerDecorator</code></p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>class WithFixedSizeCache(ModelManagerDecorator):\n    def __init__(self, model_manager: ModelManager, max_size: int = 8):\n\"\"\"Cache decorator, models will be evicted based on the last utilization (`.infer` call). Internally, a [double-ended queue](https://docs.python.org/3/library/collections.html#collections.deque) is used to keep track of model utilization.\n\n        Args:\n            model_manager (ModelManager): Instance of a ModelManager.\n            max_size (int, optional): Max number of models at the same time. Defaults to 8.\n        \"\"\"\n        super().__init__(model_manager)\n        self.max_size = max_size\n        self._key_queue = deque(self.model_manager.keys())\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ) -&gt; None:\n\"\"\"Adds a model to the manager and evicts the least recently used if the cache is full.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        queue_id = self._resolve_queue_id(\n            model_id=model_id, model_id_alias=model_id_alias\n        )\n        if queue_id in self:\n            logger.debug(\n                f\"Detected {queue_id} in WithFixedSizeCache models queue -&gt; marking as most recently used.\"\n            )\n            self._key_queue.remove(queue_id)\n            self._key_queue.append(queue_id)\n            return None\n\n        logger.debug(f\"Current capacity of ModelManager: {len(self)}/{self.max_size}\")\n        while len(self) &gt;= self.max_size:\n            to_remove_model_id = self._key_queue.popleft()\n            logger.debug(\n                f\"Reached maximum capacity of ModelManager. Unloading model {to_remove_model_id}\"\n            )\n            super().remove(to_remove_model_id)\n            logger.debug(f\"Model {to_remove_model_id} successfully unloaded.\")\n        logger.debug(f\"Marking new model {queue_id} as most recently used.\")\n        self._key_queue.append(queue_id)\n        try:\n            return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n        except Exception as error:\n            logger.debug(\n                f\"Could not initialise model {queue_id}. Removing from WithFixedSizeCache models queue.\"\n            )\n            self._key_queue.remove(queue_id)\n            raise error\n\n    def clear(self) -&gt; None:\n\"\"\"Removes all models from the manager.\"\"\"\n        for model_id in list(self.keys()):\n            self.remove(model_id)\n\n    def remove(self, model_id: str) -&gt; Model:\n        try:\n            self._key_queue.remove(model_id)\n        except ValueError:\n            logger.warning(\n                f\"Could not successfully purge model {model_id} from  WithFixedSizeCache models queue\"\n            )\n        return super().remove(model_id)\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Processes a complete inference request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return await super().infer_from_request(model_id, request, **kwargs)\n\n    def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n\"\"\"Performs only the inference part of a request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request: The request to process.\n            img_in: Input image.\n            img_dims: Image dimensions.\n            batch_size (int, optional): Batch size.\n\n        Returns:\n            Response from the inference-only operation.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return super().infer_only(model_id, request, img_in, img_dims, batch_size)\n\n    def preprocess(self, model_id: str, request):\n\"\"\"Processes the preprocessing part of a request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return super().preprocess(model_id, request)\n\n    def describe_models(self) -&gt; List[ModelDescription]:\n        return self.model_manager.describe_models()\n\n    def _resolve_queue_id(\n        self, model_id: str, model_id_alias: Optional[str] = None\n    ) -&gt; str:\n        return model_id if model_id_alias is None else model_id_alias\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.__init__","title":"<code>__init__(model_manager, max_size=8)</code>","text":"<p>Cache decorator, models will be evicted based on the last utilization (<code>.infer</code> call). Internally, a double-ended queue is used to keep track of model utilization.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Instance of a ModelManager.</p> required <code>max_size</code> <code>int</code> <p>Max number of models at the same time. Defaults to 8.</p> <code>8</code> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def __init__(self, model_manager: ModelManager, max_size: int = 8):\n\"\"\"Cache decorator, models will be evicted based on the last utilization (`.infer` call). Internally, a [double-ended queue](https://docs.python.org/3/library/collections.html#collections.deque) is used to keep track of model utilization.\n\n    Args:\n        model_manager (ModelManager): Instance of a ModelManager.\n        max_size (int, optional): Max number of models at the same time. Defaults to 8.\n    \"\"\"\n    super().__init__(model_manager)\n    self.max_size = max_size\n    self._key_queue = deque(self.model_manager.keys())\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a model to the manager and evicts the least recently used if the cache is full.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n) -&gt; None:\n\"\"\"Adds a model to the manager and evicts the least recently used if the cache is full.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    queue_id = self._resolve_queue_id(\n        model_id=model_id, model_id_alias=model_id_alias\n    )\n    if queue_id in self:\n        logger.debug(\n            f\"Detected {queue_id} in WithFixedSizeCache models queue -&gt; marking as most recently used.\"\n        )\n        self._key_queue.remove(queue_id)\n        self._key_queue.append(queue_id)\n        return None\n\n    logger.debug(f\"Current capacity of ModelManager: {len(self)}/{self.max_size}\")\n    while len(self) &gt;= self.max_size:\n        to_remove_model_id = self._key_queue.popleft()\n        logger.debug(\n            f\"Reached maximum capacity of ModelManager. Unloading model {to_remove_model_id}\"\n        )\n        super().remove(to_remove_model_id)\n        logger.debug(f\"Model {to_remove_model_id} successfully unloaded.\")\n    logger.debug(f\"Marking new model {queue_id} as most recently used.\")\n    self._key_queue.append(queue_id)\n    try:\n        return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n    except Exception as error:\n        logger.debug(\n            f\"Could not initialise model {queue_id}. Removing from WithFixedSizeCache models queue.\"\n        )\n        self._key_queue.remove(queue_id)\n        raise error\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.clear","title":"<code>clear()</code>","text":"<p>Removes all models from the manager.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Removes all models from the manager.\"\"\"\n    for model_id in list(self.keys()):\n        self.remove(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n\"\"\"Processes a complete inference request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return await super().infer_from_request(model_id, request, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.infer_only","title":"<code>infer_only(model_id, request, img_in, img_dims, batch_size=None)</code>","text":"<p>Performs only the inference part of a request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <p>The request to process.</p> required <code>img_in</code> <p>Input image.</p> required <code>img_dims</code> <p>Image dimensions.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the inference-only operation.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n\"\"\"Performs only the inference part of a request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request: The request to process.\n        img_in: Input image.\n        img_dims: Image dimensions.\n        batch_size (int, optional): Batch size.\n\n    Returns:\n        Response from the inference-only operation.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return super().infer_only(model_id, request, img_in, img_dims, batch_size)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Processes the preprocessing part of a request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def preprocess(self, model_id: str, request):\n\"\"\"Processes the preprocessing part of a request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return super().preprocess(model_id, request)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/locked_load/","title":"locked_load","text":""},{"location":"docs/reference/inference/core/managers/decorators/locked_load/#inference.core.managers.decorators.locked_load.LockedLoadModelManagerDecorator","title":"<code>LockedLoadModelManagerDecorator</code>","text":"<p>             Bases: <code>ModelManagerDecorator</code></p> <p>Must acquire lock to load model</p> Source code in <code>inference/core/managers/decorators/locked_load.py</code> <pre><code>class LockedLoadModelManagerDecorator(ModelManagerDecorator):\n\"\"\"Must acquire lock to load model\"\"\"\n\n    def add_model(self, model_id: str, api_key: str, model_id_alias=None):\n        with cache.lock(lock_str(model_id), expire=180.0):\n            return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/","title":"logger","text":""},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger","title":"<code>WithLogger</code>","text":"<p>             Bases: <code>ModelManagerDecorator</code></p> <p>Logger Decorator, it logs what's going on inside the manager.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>class WithLogger(ModelManagerDecorator):\n\"\"\"Logger Decorator, it logs what's going on inside the manager.\"\"\"\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ):\n\"\"\"Adds a model to the manager and logs the action.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n\n        Returns:\n            The result of the add_model method from the superclass.\n        \"\"\"\n        logger.info(f\"\ud83e\udd16 {model_id} added.\")\n        return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n\"\"\"Processes a complete inference request and logs both the request and response.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n        res = await super().infer_from_request(model_id, request, **kwargs)\n        logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n        return res\n\n    def remove(self, model_id: str) -&gt; Model:\n\"\"\"Removes a model from the manager and logs the action.\n\n        Args:\n            model_id (str): The identifier of the model to remove.\n\n        Returns:\n            Model: The removed model.\n        \"\"\"\n        res = super().remove(model_id)\n        logger.info(f\"\u274c removed {model_id}\")\n        return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a model to the manager and logs the action.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required <p>Returns:</p> Type Description <p>The result of the add_model method from the superclass.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n):\n\"\"\"Adds a model to the manager and logs the action.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n\n    Returns:\n        The result of the add_model method from the superclass.\n    \"\"\"\n    logger.info(f\"\ud83e\udd16 {model_id} added.\")\n    return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request and logs both the request and response.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n\"\"\"Processes a complete inference request and logs both the request and response.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n    res = await super().infer_from_request(model_id, request, **kwargs)\n    logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager and logs the action.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to remove.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The removed model.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def remove(self, model_id: str) -&gt; Model:\n\"\"\"Removes a model from the manager and logs the action.\n\n    Args:\n        model_id (str): The identifier of the model to remove.\n\n    Returns:\n        Model: The removed model.\n    \"\"\"\n    res = super().remove(model_id)\n    logger.info(f\"\u274c removed {model_id}\")\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/","title":"base","text":""},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference","title":"<code>BaseInference</code>","text":"<p>General inference class.</p> <p>This class provides a basic interface for inference tasks.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>class BaseInference:\n\"\"\"General inference class.\n\n    This class provides a basic interface for inference tasks.\n    \"\"\"\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n\"\"\"Runs inference on given data.\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        preproc_image, returned_metadata = self.preprocess(image, **kwargs)\n        logger.debug(\n            f\"Preprocessed input shape: {getattr(preproc_image, 'shape', None)}\"\n        )\n        predicted_arrays = self.predict(preproc_image, **kwargs)\n        postprocessed = self.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\n        return postprocessed\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        raise NotImplementedError\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        raise NotImplementedError\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        raise NotImplementedError\n\n    def infer_from_request(\n        self, request: InferenceRequest\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n\"\"\"Runs inference on a request\n\n        Args:\n            request (InferenceRequest): The request object.\n\n        Returns:\n            Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def make_response(\n        self, *args, **kwargs\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n\"\"\"Constructs an object detection response.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Runs inference on given data. - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n\"\"\"Runs inference on given data.\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    preproc_image, returned_metadata = self.preprocess(image, **kwargs)\n    logger.debug(\n        f\"Preprocessed input shape: {getattr(preproc_image, 'shape', None)}\"\n    )\n    predicted_arrays = self.predict(preproc_image, **kwargs)\n    postprocessed = self.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\n    return postprocessed\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Runs inference on a request</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InferenceRequest</code> <p>The request object.</p> required <p>Returns:</p> Type Description <code>Union[InferenceResponse, List[InferenceResponse]]</code> <p>Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer_from_request(\n    self, request: InferenceRequest\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n\"\"\"Runs inference on a request\n\n    Args:\n        request (InferenceRequest): The request object.\n\n    Returns:\n        Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference.make_response","title":"<code>make_response(*args, **kwargs)</code>","text":"<p>Constructs an object detection response.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def make_response(\n    self, *args, **kwargs\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n\"\"\"Constructs an object detection response.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model","title":"<code>Model</code>","text":"<p>             Bases: <code>BaseInference</code></p> <p>Base Inference Model (Inherits from BaseInference to define the needed methods)</p> <p>This class provides the foundational methods for inference and logging, and can be extended by specific models.</p> <p>Methods:</p> Name Description <code>log</code> <p>Print the given message.</p> <code>clear_cache</code> <p>Clears any cache if necessary.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>class Model(BaseInference):\n\"\"\"Base Inference Model (Inherits from BaseInference to define the needed methods)\n\n    This class provides the foundational methods for inference and logging, and can be extended by specific models.\n\n    Methods:\n        log(m): Print the given message.\n        clear_cache(): Clears any cache if necessary.\n    \"\"\"\n\n    def log(self, m):\n\"\"\"Prints the given message.\n\n        Args:\n            m (str): The message to print.\n        \"\"\"\n        print(m)\n\n    def clear_cache(self):\n\"\"\"Clears any cache if necessary. This method should be implemented in derived classes as needed.\"\"\"\n        pass\n\n    def infer_from_request(\n        self,\n        request: InferenceRequest,\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n\"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        The function can handle both single and multiple image inference requests. Optionally, it also provides\n        a visualization of the predictions if requested.\n\n        Args:\n            request (InferenceRequest): The request object containing details for inference, such as the image or\n                images to process, any classes to filter by, and whether or not to visualize the predictions.\n\n        Returns:\n            Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains\n            multiple images, or a single response object if the request contains one image. Each response object\n            contains details about the segmented instances, the time taken for inference, and optionally, a visualization.\n\n        Examples:\n            &gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n            &gt;&gt;&gt; response = infer_from_request(request)\n            &gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n            0.125\n            &gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n\n        Notes:\n            - The processing time for each response is included within the response itself.\n            - If `visualize_predictions` is set to True in the request, a visualization of the prediction\n              is also included in the response.\n        \"\"\"\n        t1 = perf_counter()\n        responses = self.infer(**request.dict(), return_image_dims=False)\n        for response in responses:\n            response.time = perf_counter() - t1\n\n        if request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if not isinstance(request.image, list) and len(responses) &gt; 0:\n            responses = responses[0]\n\n        return responses\n\n    def make_response(\n        self, *args, **kwargs\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n\"\"\"Makes an inference response from the given arguments.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            InferenceResponse: The inference response.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".make_response\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clears any cache if necessary. This method should be implemented in derived classes as needed.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def clear_cache(self):\n\"\"\"Clears any cache if necessary. This method should be implemented in derived classes as needed.\"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses. The function can handle both single and multiple image inference requests. Optionally, it also provides a visualization of the predictions if requested.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InferenceRequest</code> <p>The request object containing details for inference, such as the image or images to process, any classes to filter by, and whether or not to visualize the predictions.</p> required <p>Returns:</p> Type Description <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains</p> <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>multiple images, or a single response object if the request contains one image. Each response object</p> <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>contains details about the segmented instances, the time taken for inference, and optionally, a visualization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n&gt;&gt;&gt; response = infer_from_request(request)\n&gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n0.125\n&gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n</code></pre> Notes <ul> <li>The processing time for each response is included within the response itself.</li> <li>If <code>visualize_predictions</code> is set to True in the request, a visualization of the prediction   is also included in the response.</li> </ul> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer_from_request(\n    self,\n    request: InferenceRequest,\n) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n\"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    The function can handle both single and multiple image inference requests. Optionally, it also provides\n    a visualization of the predictions if requested.\n\n    Args:\n        request (InferenceRequest): The request object containing details for inference, such as the image or\n            images to process, any classes to filter by, and whether or not to visualize the predictions.\n\n    Returns:\n        Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains\n        multiple images, or a single response object if the request contains one image. Each response object\n        contains details about the segmented instances, the time taken for inference, and optionally, a visualization.\n\n    Examples:\n        &gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n        &gt;&gt;&gt; response = infer_from_request(request)\n        &gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n        0.125\n        &gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n\n    Notes:\n        - The processing time for each response is included within the response itself.\n        - If `visualize_predictions` is set to True in the request, a visualization of the prediction\n          is also included in the response.\n    \"\"\"\n    t1 = perf_counter()\n    responses = self.infer(**request.dict(), return_image_dims=False)\n    for response in responses:\n        response.time = perf_counter() - t1\n\n    if request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if not isinstance(request.image, list) and len(responses) &gt; 0:\n        responses = responses[0]\n\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.log","title":"<code>log(m)</code>","text":"<p>Prints the given message.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>str</code> <p>The message to print.</p> required Source code in <code>inference/core/models/base.py</code> <pre><code>def log(self, m):\n\"\"\"Prints the given message.\n\n    Args:\n        m (str): The message to print.\n    \"\"\"\n    print(m)\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.make_response","title":"<code>make_response(*args, **kwargs)</code>","text":"<p>Makes an inference response from the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>Union[InferenceResponse, List[InferenceResponse]]</code> <p>The inference response.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def make_response(\n    self, *args, **kwargs\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n\"\"\"Makes an inference response from the given arguments.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        InferenceResponse: The inference response.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".make_response\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/","title":"classification_base","text":""},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel","title":"<code>ClassificationBaseOnnxRoboflowInferenceModel</code>","text":"<p>             Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Base class for ONNX models for Roboflow classification inference.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>Whether the classification is multi-class or not.</p> <p>Methods:</p> Name Description <code>get_infer_bucket_file_list</code> <p>Get the list of required files for inference.</p> <code>softmax</code> <p>Compute softmax values for a given set of scores.</p> <code>infer</code> <p>ClassificationInferenceRequest) -&gt; Union[List[Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]], Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]]: Perform inference on a given request and return the response.</p> <code>draw_predictions</code> <p>Draw prediction visuals on an image.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>class ClassificationBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n\"\"\"Base class for ONNX models for Roboflow classification inference.\n\n    Attributes:\n        multiclass (bool): Whether the classification is multi-class or not.\n\n    Methods:\n        get_infer_bucket_file_list() -&gt; list: Get the list of required files for inference.\n        softmax(x): Compute softmax values for a given set of scores.\n        infer(request: ClassificationInferenceRequest) -&gt; Union[List[Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]], Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]]: Perform inference on a given request and return the response.\n        draw_predictions(inference_request, inference_response): Draw prediction visuals on an image.\n    \"\"\"\n\n    task_type = \"classification\"\n\n    def __init__(self, *args, **kwargs):\n\"\"\"Initialize the model, setting whether it is multiclass or not.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    def draw_predictions(self, inference_request, inference_response):\n\"\"\"Draw prediction visuals on an image.\n\n        This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.\n\n        Args:\n            inference_request: The request object containing the image and parameters.\n            inference_response: The response object containing the predictions and other details.\n\n        Returns:\n            bytes: The bytes of the visualized image in JPEG format.\n        \"\"\"\n        image = load_image_rgb(inference_request.image)\n        image = Image.fromarray(image)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        if isinstance(inference_response.predictions, list):\n            prediction = inference_response.predictions[0]\n            color = self.colors.get(prediction.class_name, \"#4892EA\")\n            draw.rectangle(\n                [0, 0, image.size[1], image.size[0]],\n                outline=color,\n                width=inference_request.visualization_stroke_width,\n            )\n            text = f\"{prediction.class_id} - {prediction.class_name} {prediction.confidence:.2f}\"\n            text_size = font.getbbox(text)\n\n            # set button size + 10px margins\n            button_size = (text_size[2] + 20, text_size[3] + 20)\n            button_img = Image.new(\"RGBA\", button_size, color)\n            # put text on button with 10px margins\n            button_draw = ImageDraw.Draw(button_img)\n            button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n            # put button on source image in position (0, 0)\n            image.paste(button_img, (0, 0))\n        else:\n            if len(inference_response.predictions) &gt; 0:\n                box_color = \"#4892EA\"\n                draw.rectangle(\n                    [0, 0, image.size[1], image.size[0]],\n                    outline=box_color,\n                    width=inference_request.visualization_stroke_width,\n                )\n            row = 0\n            predictions = [\n                (cls_name, pred)\n                for cls_name, pred in inference_response.predictions.items()\n            ]\n            predictions = sorted(\n                predictions, key=lambda x: x[1].confidence, reverse=True\n            )\n            for i, (cls_name, pred) in enumerate(predictions):\n                color = self.colors.get(cls_name, \"#4892EA\")\n                text = f\"{cls_name} {pred.confidence:.2f}\"\n                text_size = font.getbbox(text)\n\n                # set button size + 10px margins\n                button_size = (text_size[2] + 20, text_size[3] + 20)\n                button_img = Image.new(\"RGBA\", button_size, color)\n                # put text on button with 10px margins\n                button_draw = ImageDraw.Draw(button_img)\n                button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n                # put button on source image in position (0, 0)\n                image.paste(button_img, (0, row))\n                row += button_size[1]\n\n        buffered = BytesIO()\n        image = image.convert(\"RGB\")\n        image.save(buffered, format=\"JPEG\")\n        return buffered.getvalue()\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"environment.json\"].\n        \"\"\"\n        return [\"environment.json\"]\n\n    def infer(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        return_image_dims: bool = False,\n        **kwargs,\n    ):\n\"\"\"\n        Perform inference on the provided image(s) and return the predictions.\n\n        Args:\n            image (Any): The image or list of images to be processed.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            return_image_dims (bool, optional): If set to True, the function will also return the dimensions of the image. Defaults to False.\n            **kwargs: Additional parameters to customize the inference process.\n\n        Returns:\n            Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:\n            If `return_image_dims` is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.\n            If `return_image_dims` is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.\n            If `return_image_dims` is False and a list of images is provided, only the list of prediction arrays is returned.\n            If `return_image_dims` is False and a single image is provided, only the prediction array is returned.\n\n        Notes:\n            - The input image(s) will be preprocessed (normalized and reshaped) before inference.\n            - This function uses an ONNX session to perform inference on the input image(s).\n        \"\"\"\n        return super().infer(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            return_image_dims=return_image_dims,\n        )\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        return_image_dims=False,\n        **kwargs,\n    ) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n        predictions = predictions[0]\n        return self.make_response(\n            predictions, preprocess_return_metadata[\"img_dims\"], **kwargs\n        )\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        return (predictions,)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        if isinstance(image, list):\n            imgs_with_dims = [\n                self.preproc_image(\n                    i,\n                    disable_preproc_auto_orient=kwargs.get(\n                        \"disable_preproc_auto_orient\", False\n                    ),\n                    disable_preproc_contrast=kwargs.get(\n                        \"disable_preproc_contrast\", False\n                    ),\n                    disable_preproc_grayscale=kwargs.get(\n                        \"disable_preproc_grayscale\", False\n                    ),\n                    disable_preproc_static_crop=kwargs.get(\n                        \"disable_preproc_static_crop\", False\n                    ),\n                )\n                for i in image\n            ]\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in, img_dims = self.preproc_image(\n                image,\n                disable_preproc_auto_orient=kwargs.get(\n                    \"disable_preproc_auto_orient\", False\n                ),\n                disable_preproc_contrast=kwargs.get(\"disable_preproc_contrast\", False),\n                disable_preproc_grayscale=kwargs.get(\n                    \"disable_preproc_grayscale\", False\n                ),\n                disable_preproc_static_crop=kwargs.get(\n                    \"disable_preproc_static_crop\", False\n                ),\n            )\n            img_dims = [img_dims]\n\n        img_in /= 255.0\n\n        mean = (0.5, 0.5, 0.5)\n        std = (0.5, 0.5, 0.5)\n\n        img_in = img_in.astype(np.float32)\n\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[0]) / std[0]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[2]) / std[2]\n        return img_in, PreprocessReturnMetadata({\"img_dims\": img_dims})\n\n    def infer_from_request(\n        self,\n        request: ClassificationInferenceRequest,\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n\"\"\"\n        Handle an inference request to produce an appropriate response.\n\n        Args:\n            request (ClassificationInferenceRequest): The request object encapsulating the image(s) and relevant parameters.\n\n        Returns:\n            Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.\n\n        Notes:\n            - Starts a timer at the beginning to calculate inference time.\n            - Processes the image(s) through the `infer` method.\n            - Generates the appropriate response object(s) using `make_response`.\n            - Calculates and sets the time taken for inference.\n            - If visualization is requested, the predictions are drawn on the image.\n        \"\"\"\n        t1 = perf_counter()\n        responses = self.infer(**request.dict(), return_image_dims=True)\n        for response in responses:\n            response.time = perf_counter() - t1\n\n        if request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if not isinstance(request.image, list):\n            responses = responses[0]\n\n        return responses\n\n    def make_response(\n        self,\n        predictions,\n        img_dims,\n        confidence: float = 0.5,\n        **kwargs,\n    ) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n\"\"\"\n        Create response objects for the given predictions and image dimensions.\n\n        Args:\n            predictions (list): List of prediction arrays from the inference process.\n            img_dims (list): List of tuples indicating the dimensions (width, height) of each image.\n            confidence (float, optional): Confidence threshold for filtering predictions. Defaults to 0.5.\n            **kwargs: Additional parameters to influence the response creation process.\n\n        Returns:\n            Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.\n\n        Notes:\n            - If the model is multiclass, a `MultiLabelClassificationInferenceResponse` is generated for each image.\n            - If the model is not multiclass, a `ClassificationInferenceResponse` is generated for each image.\n            - Predictions below the confidence threshold are filtered out.\n        \"\"\"\n        responses = []\n        confidence_threshold = float(confidence)\n        for ind, prediction in enumerate(predictions):\n            if self.multiclass:\n                preds = prediction[0]\n                results = dict()\n                predicted_classes = []\n                for i, o in enumerate(preds):\n                    cls_name = self.class_names[i]\n                    score = float(o)\n                    results[cls_name] = {\"confidence\": score, \"class_id\": i}\n                    if score &gt; confidence_threshold:\n                        predicted_classes.append(cls_name)\n                response = MultiLabelClassificationInferenceResponse(\n                    image=InferenceResponseImage(\n                        width=img_dims[ind][0], height=img_dims[ind][1]\n                    ),\n                    predicted_classes=predicted_classes,\n                    predictions=results,\n                )\n            else:\n                preds = prediction[0]\n                preds = self.softmax(preds)\n                results = []\n                for i, cls_name in enumerate(self.class_names):\n                    score = float(preds[i])\n                    pred = {\n                        \"class_id\": i,\n                        \"class\": cls_name,\n                        \"confidence\": round(score, 4),\n                    }\n                    results.append(pred)\n                results = sorted(results, key=lambda x: x[\"confidence\"], reverse=True)\n\n                response = ClassificationInferenceResponse(\n                    image=InferenceResponseImage(\n                        width=img_dims[ind][1], height=img_dims[ind][0]\n                    ),\n                    predictions=results,\n                    top=results[0][\"class\"],\n                    confidence=results[0][\"confidence\"],\n                )\n            responses.append(response)\n\n        return responses\n\n    @staticmethod\n    def softmax(x):\n\"\"\"Compute softmax values for each set of scores in x.\n\n        Args:\n            x (np.array): The input array containing the scores.\n\n        Returns:\n            np.array: The softmax values for each set of scores.\n        \"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    def get_model_output_shape(self) -&gt; Tuple[int, int, int]:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        test_image, _ = self.preprocess(test_image)\n        output = np.array(self.predict(test_image))\n        return output.shape\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = output_shape[3]\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the model, setting whether it is multiclass or not.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Initialize the model, setting whether it is multiclass or not.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw prediction visuals on an image.</p> <p>This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <p>The request object containing the image and parameters.</p> required <code>inference_response</code> <p>The response object containing the predictions and other details.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <p>The bytes of the visualized image in JPEG format.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def draw_predictions(self, inference_request, inference_response):\n\"\"\"Draw prediction visuals on an image.\n\n    This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.\n\n    Args:\n        inference_request: The request object containing the image and parameters.\n        inference_response: The response object containing the predictions and other details.\n\n    Returns:\n        bytes: The bytes of the visualized image in JPEG format.\n    \"\"\"\n    image = load_image_rgb(inference_request.image)\n    image = Image.fromarray(image)\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    if isinstance(inference_response.predictions, list):\n        prediction = inference_response.predictions[0]\n        color = self.colors.get(prediction.class_name, \"#4892EA\")\n        draw.rectangle(\n            [0, 0, image.size[1], image.size[0]],\n            outline=color,\n            width=inference_request.visualization_stroke_width,\n        )\n        text = f\"{prediction.class_id} - {prediction.class_name} {prediction.confidence:.2f}\"\n        text_size = font.getbbox(text)\n\n        # set button size + 10px margins\n        button_size = (text_size[2] + 20, text_size[3] + 20)\n        button_img = Image.new(\"RGBA\", button_size, color)\n        # put text on button with 10px margins\n        button_draw = ImageDraw.Draw(button_img)\n        button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n        # put button on source image in position (0, 0)\n        image.paste(button_img, (0, 0))\n    else:\n        if len(inference_response.predictions) &gt; 0:\n            box_color = \"#4892EA\"\n            draw.rectangle(\n                [0, 0, image.size[1], image.size[0]],\n                outline=box_color,\n                width=inference_request.visualization_stroke_width,\n            )\n        row = 0\n        predictions = [\n            (cls_name, pred)\n            for cls_name, pred in inference_response.predictions.items()\n        ]\n        predictions = sorted(\n            predictions, key=lambda x: x[1].confidence, reverse=True\n        )\n        for i, (cls_name, pred) in enumerate(predictions):\n            color = self.colors.get(cls_name, \"#4892EA\")\n            text = f\"{cls_name} {pred.confidence:.2f}\"\n            text_size = font.getbbox(text)\n\n            # set button size + 10px margins\n            button_size = (text_size[2] + 20, text_size[3] + 20)\n            button_img = Image.new(\"RGBA\", button_size, color)\n            # put text on button with 10px margins\n            button_draw = ImageDraw.Draw(button_img)\n            button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n            # put button on source image in position (0, 0)\n            image.paste(button_img, (0, row))\n            row += button_size[1]\n\n    buffered = BytesIO()\n    image = image.convert(\"RGB\")\n    image.save(buffered, format=\"JPEG\")\n    return buffered.getvalue()\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"environment.json\"].</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"environment.json\"].\n    \"\"\"\n    return [\"environment.json\"]\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, return_image_dims=False, **kwargs)</code>","text":"<p>Perform inference on the provided image(s) and return the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be processed. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>return_image_dims</code> <code>bool</code> <p>If set to True, the function will also return the dimensions of the image. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to customize the inference process.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:</p> <p>If <code>return_image_dims</code> is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.</p> <p>If <code>return_image_dims</code> is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.</p> <p>If <code>return_image_dims</code> is False and a list of images is provided, only the list of prediction arrays is returned.</p> <p>If <code>return_image_dims</code> is False and a single image is provided, only the prediction array is returned.</p> Notes <ul> <li>The input image(s) will be preprocessed (normalized and reshaped) before inference.</li> <li>This function uses an ONNX session to perform inference on the input image(s).</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    return_image_dims: bool = False,\n    **kwargs,\n):\n\"\"\"\n    Perform inference on the provided image(s) and return the predictions.\n\n    Args:\n        image (Any): The image or list of images to be processed.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        return_image_dims (bool, optional): If set to True, the function will also return the dimensions of the image. Defaults to False.\n        **kwargs: Additional parameters to customize the inference process.\n\n    Returns:\n        Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:\n        If `return_image_dims` is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.\n        If `return_image_dims` is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.\n        If `return_image_dims` is False and a list of images is provided, only the list of prediction arrays is returned.\n        If `return_image_dims` is False and a single image is provided, only the prediction array is returned.\n\n    Notes:\n        - The input image(s) will be preprocessed (normalized and reshaped) before inference.\n        - This function uses an ONNX session to perform inference on the input image(s).\n    \"\"\"\n    return super().infer(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        return_image_dims=return_image_dims,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Handle an inference request to produce an appropriate response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClassificationInferenceRequest</code> <p>The request object encapsulating the image(s) and relevant parameters.</p> required <p>Returns:</p> Type Description <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.</p> Notes <ul> <li>Starts a timer at the beginning to calculate inference time.</li> <li>Processes the image(s) through the <code>infer</code> method.</li> <li>Generates the appropriate response object(s) using <code>make_response</code>.</li> <li>Calculates and sets the time taken for inference.</li> <li>If visualization is requested, the predictions are drawn on the image.</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def infer_from_request(\n    self,\n    request: ClassificationInferenceRequest,\n) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n\"\"\"\n    Handle an inference request to produce an appropriate response.\n\n    Args:\n        request (ClassificationInferenceRequest): The request object encapsulating the image(s) and relevant parameters.\n\n    Returns:\n        Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.\n\n    Notes:\n        - Starts a timer at the beginning to calculate inference time.\n        - Processes the image(s) through the `infer` method.\n        - Generates the appropriate response object(s) using `make_response`.\n        - Calculates and sets the time taken for inference.\n        - If visualization is requested, the predictions are drawn on the image.\n    \"\"\"\n    t1 = perf_counter()\n    responses = self.infer(**request.dict(), return_image_dims=True)\n    for response in responses:\n        response.time = perf_counter() - t1\n\n    if request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if not isinstance(request.image, list):\n        responses = responses[0]\n\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, confidence=0.5, **kwargs)</code>","text":"<p>Create response objects for the given predictions and image dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>List of prediction arrays from the inference process.</p> required <code>img_dims</code> <code>list</code> <p>List of tuples indicating the dimensions (width, height) of each image.</p> required <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering predictions. Defaults to 0.5.</p> <code>0.5</code> <code>**kwargs</code> <p>Additional parameters to influence the response creation process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]</code> <p>Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.</p> Notes <ul> <li>If the model is multiclass, a <code>MultiLabelClassificationInferenceResponse</code> is generated for each image.</li> <li>If the model is not multiclass, a <code>ClassificationInferenceResponse</code> is generated for each image.</li> <li>Predictions below the confidence threshold are filtered out.</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def make_response(\n    self,\n    predictions,\n    img_dims,\n    confidence: float = 0.5,\n    **kwargs,\n) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n\"\"\"\n    Create response objects for the given predictions and image dimensions.\n\n    Args:\n        predictions (list): List of prediction arrays from the inference process.\n        img_dims (list): List of tuples indicating the dimensions (width, height) of each image.\n        confidence (float, optional): Confidence threshold for filtering predictions. Defaults to 0.5.\n        **kwargs: Additional parameters to influence the response creation process.\n\n    Returns:\n        Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.\n\n    Notes:\n        - If the model is multiclass, a `MultiLabelClassificationInferenceResponse` is generated for each image.\n        - If the model is not multiclass, a `ClassificationInferenceResponse` is generated for each image.\n        - Predictions below the confidence threshold are filtered out.\n    \"\"\"\n    responses = []\n    confidence_threshold = float(confidence)\n    for ind, prediction in enumerate(predictions):\n        if self.multiclass:\n            preds = prediction[0]\n            results = dict()\n            predicted_classes = []\n            for i, o in enumerate(preds):\n                cls_name = self.class_names[i]\n                score = float(o)\n                results[cls_name] = {\"confidence\": score, \"class_id\": i}\n                if score &gt; confidence_threshold:\n                    predicted_classes.append(cls_name)\n            response = MultiLabelClassificationInferenceResponse(\n                image=InferenceResponseImage(\n                    width=img_dims[ind][0], height=img_dims[ind][1]\n                ),\n                predicted_classes=predicted_classes,\n                predictions=results,\n            )\n        else:\n            preds = prediction[0]\n            preds = self.softmax(preds)\n            results = []\n            for i, cls_name in enumerate(self.class_names):\n                score = float(preds[i])\n                pred = {\n                    \"class_id\": i,\n                    \"class\": cls_name,\n                    \"confidence\": round(score, 4),\n                }\n                results.append(pred)\n            results = sorted(results, key=lambda x: x[\"confidence\"], reverse=True)\n\n            response = ClassificationInferenceResponse(\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n                predictions=results,\n                top=results[0][\"class\"],\n                confidence=results[0][\"confidence\"],\n            )\n        responses.append(response)\n\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.softmax","title":"<code>softmax(x)</code>  <code>staticmethod</code>","text":"<p>Compute softmax values for each set of scores in x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>The input array containing the scores.</p> required <p>Returns:</p> Type Description <p>np.array: The softmax values for each set of scores.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>@staticmethod\ndef softmax(x):\n\"\"\"Compute softmax values for each set of scores in x.\n\n    Args:\n        x (np.array): The input array containing the scores.\n\n    Returns:\n        np.array: The softmax values for each set of scores.\n    \"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n</code></pre>"},{"location":"docs/reference/inference/core/models/defaults/","title":"defaults","text":""},{"location":"docs/reference/inference/core/models/instance_segmentation_base/","title":"instance_segmentation_base","text":""},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel","title":"<code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code>","text":"<p>             Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Instance Segmentation model.</p> <p>This class implements an instance segmentation specific inference method for ONNX models provided by Roboflow.</p> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>class InstanceSegmentationBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX Instance Segmentation model.\n\n    This class implements an instance segmentation specific inference method\n    for ONNX models provided by Roboflow.\n    \"\"\"\n\n    task_type = \"instance-segmentation\"\n    num_masks = 32\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = False,\n        confidence: float = DEFAULT_CONFIDENCE,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        mask_decode_mode: str = DEFAULT_MASK_DECODE_MODE,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        tradeoff_factor: float = DEFAULT_TRADEOFF_FACTOR,\n        **kwargs,\n    ) -&gt; Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]:\n\"\"\"\n        Process an image or list of images for instance segmentation.\n\n        Args:\n            image (Any): An image or a list of images for processing.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n            iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n            mask_decode_mode (str, optional): Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".\n            max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the processed images. Defaults to False.\n            tradeoff_factor (float, optional): Tradeoff factor used when `mask_decode_mode` is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            **kwargs: Additional parameters to customize the inference process.\n\n        Returns:\n            Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.\n\n        Raises:\n            InvalidMaskDecodeArgument: If an invalid `mask_decode_mode` is provided or if the `tradeoff_factor` is outside the allowed range.\n\n        Notes:\n            - Processes input images and normalizes them.\n            - Makes predictions using the ONNX runtime.\n            - Applies non-maximum suppression to the predictions.\n            - Decodes the masks according to the specified mode.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            iou_threshold=iou_threshold,\n            mask_decode_mode=mask_decode_mode,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            tradeoff_factor=tradeoff_factor,\n        )\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Union[\n        InstanceSegmentationInferenceResponse,\n        List[InstanceSegmentationInferenceResponse],\n    ]:\n        predictions, protos = predictions\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=kwargs[\"confidence\"],\n            iou_thresh=kwargs[\"iou_threshold\"],\n            class_agnostic=kwargs[\"class_agnostic_nms\"],\n            max_detections=kwargs[\"max_detections\"],\n            max_candidate_detections=kwargs[\"max_candidates\"],\n            num_masks=self.num_masks,\n        )\n        infer_shape = (self.img_size_h, self.img_size_w)\n        predictions = np.array(predictions)\n        masks = []\n        mask_decode_mode = kwargs[\"mask_decode_mode\"]\n        tradeoff_factor = kwargs[\"tradeoff_factor\"]\n        img_in_shape = preprocess_return_metadata[\"im_shape\"]\n        if predictions.shape[1] &gt; 0:\n            for i, (pred, proto, img_dim) in enumerate(\n                zip(predictions, protos, preprocess_return_metadata[\"img_dims\"])\n            ):\n                if mask_decode_mode == \"accurate\":\n                    batch_masks = process_mask_accurate(\n                        proto, pred[:, 7:], pred[:, :4], img_in_shape[2:]\n                    )\n                    output_mask_shape = img_in_shape[2:]\n                elif mask_decode_mode == \"tradeoff\":\n                    if not 0 &lt;= tradeoff_factor &lt;= 1:\n                        raise InvalidMaskDecodeArgument(\n                            f\"Invalid tradeoff_factor: {tradeoff_factor}. Must be in [0.0, 1.0]\"\n                        )\n                    batch_masks = process_mask_tradeoff(\n                        proto,\n                        pred[:, 7:],\n                        pred[:, :4],\n                        img_in_shape[2:],\n                        tradeoff_factor,\n                    )\n                    output_mask_shape = batch_masks.shape[1:]\n                elif mask_decode_mode == \"fast\":\n                    batch_masks = process_mask_fast(\n                        proto, pred[:, 7:], pred[:, :4], img_in_shape[2:]\n                    )\n                    output_mask_shape = batch_masks.shape[1:]\n                else:\n                    raise InvalidMaskDecodeArgument(\n                        f\"Invalid mask_decode_mode: {mask_decode_mode}. Must be one of ['accurate', 'fast', 'tradeoff']\"\n                    )\n                polys = masks2poly(batch_masks)\n                pred[:, :4] = post_process_bboxes(\n                    [pred[:, :4]],\n                    infer_shape,\n                    [img_dim],\n                    self.preproc,\n                    resize_method=self.resize_method,\n                    disable_preproc_static_crop=preprocess_return_metadata[\n                        \"disable_preproc_static_crop\"\n                    ],\n                )[0]\n                polys = post_process_polygons(\n                    img_dim,\n                    polys,\n                    output_mask_shape,\n                    self.preproc,\n                    resize_method=self.resize_method,\n                )\n                masks.append(polys)\n        else:\n            masks.extend([[]] * len(predictions))\n        return self.make_response(\n            predictions, masks, preprocess_return_metadata[\"img_dims\"], **kwargs\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=kwargs.get(\"disable_preproc_auto_orient\"),\n            disable_preproc_contrast=kwargs.get(\"disable_preproc_contrast\"),\n            disable_preproc_grayscale=kwargs.get(\"disable_preproc_grayscale\"),\n            disable_preproc_static_crop=kwargs.get(\"disable_preproc_static_crop\"),\n        )\n\n        img_in /= 255.0\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"im_shape\": img_in.shape,\n                \"disable_preproc_static_crop\": kwargs.get(\n                    \"disable_preproc_static_crop\"\n                ),\n            }\n        )\n\n    def make_response(\n        self,\n        predictions: List[List[List[float]]],\n        masks: List[List[List[float]]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: List[str] = [],\n        **kwargs,\n    ) -&gt; Union[\n        InstanceSegmentationInferenceResponse,\n        List[InstanceSegmentationInferenceResponse],\n    ]:\n\"\"\"\n        Create instance segmentation inference response objects for the provided predictions and masks.\n\n        Args:\n            predictions (List[List[List[float]]]): List of prediction data, one for each image.\n            masks (List[List[List[float]]]): List of masks corresponding to the predictions.\n            img_dims (List[Tuple[int, int]]): List of image dimensions corresponding to the processed images.\n            class_filter (List[str], optional): List of class names to filter predictions by. Defaults to an empty list (no filtering).\n\n        Returns:\n            Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.\n\n        Notes:\n            - For each image, constructs an `InstanceSegmentationInferenceResponse` object.\n            - Each response contains a list of `InstanceSegmentationPrediction` objects.\n        \"\"\"\n        responses = [\n            InstanceSegmentationInferenceResponse(\n                predictions=[\n                    InstanceSegmentationPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                        }\n                    )\n                    for pred, mask in zip(batch_predictions, batch_masks)\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, (batch_predictions, batch_masks) in enumerate(\n                zip(predictions, masks)\n            )\n        ]\n        return responses\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Runs inference on the ONNX model.\n\n        Args:\n            img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError(\"predict must be implemented by a subclass\")\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            output_shape[2], masks=self.num_masks\n        )\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, class_agnostic_nms=False, confidence=DEFAULT_CONFIDENCE, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, iou_threshold=DEFAULT_IOU_THRESH, mask_decode_mode=DEFAULT_MASK_DECODE_MODE, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, tradeoff_factor=DEFAULT_TRADEOFF_FACTOR, **kwargs)</code>","text":"<p>Process an image or list of images for instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>An image or a list of images for processing. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to use class-agnostic non-maximum suppression. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression. Defaults to 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>mask_decode_mode</code> <code>str</code> <p>Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".</p> <code>DEFAULT_MASK_DECODE_MODE</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections after non-maximum suppression. Defaults to 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the processed images. Defaults to False.</p> <code>False</code> <code>tradeoff_factor</code> <code>float</code> <p>Tradeoff factor used when <code>mask_decode_mode</code> is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.</p> <code>DEFAULT_TRADEOFF_FACTOR</code> <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to customize the inference process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]</code> <p>Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.</p> <p>Raises:</p> Type Description <code>InvalidMaskDecodeArgument</code> <p>If an invalid <code>mask_decode_mode</code> is provided or if the <code>tradeoff_factor</code> is outside the allowed range.</p> Notes <ul> <li>Processes input images and normalizes them.</li> <li>Makes predictions using the ONNX runtime.</li> <li>Applies non-maximum suppression to the predictions.</li> <li>Decodes the masks according to the specified mode.</li> </ul> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = False,\n    confidence: float = DEFAULT_CONFIDENCE,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    mask_decode_mode: str = DEFAULT_MASK_DECODE_MODE,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    tradeoff_factor: float = DEFAULT_TRADEOFF_FACTOR,\n    **kwargs,\n) -&gt; Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]:\n\"\"\"\n    Process an image or list of images for instance segmentation.\n\n    Args:\n        image (Any): An image or a list of images for processing.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n        iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n        mask_decode_mode (str, optional): Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".\n        max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the processed images. Defaults to False.\n        tradeoff_factor (float, optional): Tradeoff factor used when `mask_decode_mode` is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        **kwargs: Additional parameters to customize the inference process.\n\n    Returns:\n        Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.\n\n    Raises:\n        InvalidMaskDecodeArgument: If an invalid `mask_decode_mode` is provided or if the `tradeoff_factor` is outside the allowed range.\n\n    Notes:\n        - Processes input images and normalizes them.\n        - Makes predictions using the ONNX runtime.\n        - Applies non-maximum suppression to the predictions.\n        - Decodes the masks according to the specified mode.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        iou_threshold=iou_threshold,\n        mask_decode_mode=mask_decode_mode,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        tradeoff_factor=tradeoff_factor,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, masks, img_dims, class_filter=[], **kwargs)</code>","text":"<p>Create instance segmentation inference response objects for the provided predictions and masks.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>List of prediction data, one for each image.</p> required <code>masks</code> <code>List[List[List[float]]]</code> <p>List of masks corresponding to the predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>List of image dimensions corresponding to the processed images.</p> required <code>class_filter</code> <code>List[str]</code> <p>List of class names to filter predictions by. Defaults to an empty list (no filtering).</p> <code>[]</code> <p>Returns:</p> Type Description <code>Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]</code> <p>Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.</p> Notes <ul> <li>For each image, constructs an <code>InstanceSegmentationInferenceResponse</code> object.</li> <li>Each response contains a list of <code>InstanceSegmentationPrediction</code> objects.</li> </ul> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[List[float]]],\n    masks: List[List[List[float]]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: List[str] = [],\n    **kwargs,\n) -&gt; Union[\n    InstanceSegmentationInferenceResponse,\n    List[InstanceSegmentationInferenceResponse],\n]:\n\"\"\"\n    Create instance segmentation inference response objects for the provided predictions and masks.\n\n    Args:\n        predictions (List[List[List[float]]]): List of prediction data, one for each image.\n        masks (List[List[List[float]]]): List of masks corresponding to the predictions.\n        img_dims (List[Tuple[int, int]]): List of image dimensions corresponding to the processed images.\n        class_filter (List[str], optional): List of class names to filter predictions by. Defaults to an empty list (no filtering).\n\n    Returns:\n        Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.\n\n    Notes:\n        - For each image, constructs an `InstanceSegmentationInferenceResponse` object.\n        - Each response contains a list of `InstanceSegmentationPrediction` objects.\n    \"\"\"\n    responses = [\n        InstanceSegmentationInferenceResponse(\n            predictions=[\n                InstanceSegmentationPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n                for pred, mask in zip(batch_predictions, batch_masks)\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, (batch_predictions, batch_masks) in enumerate(\n            zip(predictions, masks)\n        )\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Runs inference on the ONNX model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>The preprocessed image(s) to run inference on.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Runs inference on the ONNX model.\n\n    Args:\n        img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError(\"predict must be implemented by a subclass\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/","title":"keypoints_detection_base","text":""},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel","title":"<code>KeypointsDetectionBaseOnnxRoboflowInferenceModel</code>","text":"<p>             Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model. This class implements an object detection specific infer method.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>class KeypointsDetectionBaseOnnxRoboflowInferenceModel(\n    ObjectDetectionBaseOnnxRoboflowInferenceModel\n):\n\"\"\"Roboflow ONNX Object detection model. This class implements an object detection specific infer method.\"\"\"\n\n    task_type = \"keypoint-detection\"\n\n    def __init__(self, model_id: str, *args, **kwargs):\n        super().__init__(model_id, *args, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n        Returns:\n            list: A list of filenames specific to ONNX models.\n        \"\"\"\n        return [\"environment.json\", \"class_names.txt\", \"keypoints_metadata.json\"]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[KeypointsDetectionInferenceResponse]:\n\"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n            max_candidates (int): Maximum number of candidate detections. Default is 3000.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[KeypointsDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        number_of_classes = len(self.get_class_names)\n        num_masks = predictions.shape[2] - 5 - number_of_classes\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            num_masks=num_masks,\n        )\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions=predictions,\n            infer_shape=infer_shape,\n            img_dims=img_dims,\n            preproc=self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        predictions = post_process_keypoints(\n            predictions=predictions,\n            keypoints_start_index=-num_masks,\n            infer_shape=infer_shape,\n            img_dims=img_dims,\n            preproc=self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[KeypointsDetectionInferenceResponse]:\n\"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.\n        \"\"\"\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n        keypoint_confidence_threshold = 0.0\n        if \"request\" in kwargs:\n            keypoint_confidence_threshold = kwargs[\"request\"].keypoint_confidence\n        responses = [\n            KeypointsDetectionInferenceResponse(\n                predictions=[\n                    KeypointsPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                            \"keypoints\": model_keypoints_to_response(\n                                keypoints_metadata=self.keypoints_metadata,\n                                keypoints=pred[7:],\n                                predicted_object_class_id=int(pred[6]),\n                                keypoint_confidence_threshold=keypoint_confidence_threshold,\n                            ),\n                        }\n                    )\n                    for pred in batch_predictions\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n\n    def keypoints_count(self) -&gt; int:\n        raise NotImplementedError\n\n    def validate_model_classes(self) -&gt; None:\n        num_keypoints = self.keypoints_count()\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            len_prediction=output_shape[2], keypoints=num_keypoints\n        )\n        if num_classes != self.num_classes:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Returns the list of files to be downloaded from the inference bucket for ONNX model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of filenames specific to ONNX models.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n    Returns:\n        list: A list of filenames specific to ONNX models.\n    \"\"\"\n    return [\"environment.json\", \"class_names.txt\", \"keypoints_metadata.json\"]\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[KeypointsDetectionInferenceResponse]</code> <p>List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[KeypointsDetectionInferenceResponse]:\n\"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.\n    \"\"\"\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n    keypoint_confidence_threshold = 0.0\n    if \"request\" in kwargs:\n        keypoint_confidence_threshold = kwargs[\"request\"].keypoint_confidence\n    responses = [\n        KeypointsDetectionInferenceResponse(\n            predictions=[\n                KeypointsPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                        \"keypoints\": model_keypoints_to_response(\n                            keypoints_metadata=self.keypoints_metadata,\n                            keypoints=pred[7:],\n                            predicted_object_class_id=int(pred[6]),\n                            keypoint_confidence_threshold=keypoint_confidence_threshold,\n                        ),\n                    }\n                )\n                for pred in batch_predictions\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to apply class-agnostic non-max suppression. Default is False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression. Default is 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Default is 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[KeypointsDetectionInferenceResponse]</code> <p>List[KeypointsDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[KeypointsDetectionInferenceResponse]:\n\"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n        max_candidates (int): Maximum number of candidate detections. Default is 3000.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[KeypointsDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    number_of_classes = len(self.get_class_names)\n    num_masks = predictions.shape[2] - 5 - number_of_classes\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=confidence,\n        iou_thresh=iou_threshold,\n        class_agnostic=class_agnostic_nms,\n        max_detections=max_detections,\n        max_candidate_detections=max_candidates,\n        num_masks=num_masks,\n    )\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions=predictions,\n        infer_shape=infer_shape,\n        img_dims=img_dims,\n        preproc=self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    predictions = post_process_keypoints(\n        predictions=predictions,\n        keypoints_start_index=-num_masks,\n        infer_shape=infer_shape,\n        img_dims=img_dims,\n        preproc=self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/","title":"object_detection_base","text":""},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel","title":"<code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code>","text":"<p>             Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model. This class implements an object detection specific infer method.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>class ObjectDetectionBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX Object detection model. This class implements an object detection specific infer method.\"\"\"\n\n    task_type = \"object-detection\"\n    box_format = \"xywh\"\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        fix_batch_size: bool = False,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n\"\"\"\n        Runs object detection inference on one or multiple images and returns the detections.\n\n        Args:\n            image (Any): The input image or a list of images to process.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n            iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n            fix_batch_size (bool, optional): If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.\n            max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the processed images along with the predictions. Defaults to False.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If `return_image_dims` is True, it will return a tuple with predictions and image dimensions.\n\n        Raises:\n            ValueError: If batching is not enabled for the model and more than one image is passed for processing.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            iou_threshold=iou_threshold,\n            fix_batch_size=fix_batch_size,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n\"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n        \"\"\"\n\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n\n        predictions = predictions[\n            : len(img_dims)\n        ]  # If the batch size was fixed we have empty preds at the end\n        responses = [\n            ObjectDetectionInferenceResponse(\n                predictions=[\n                    ObjectDetectionPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                        }\n                    )\n                    for pred in batch_predictions\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n\"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n            max_candidates (int): Maximum number of candidate detections. Default is 3000.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            box_format=self.box_format,\n        )\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions,\n            infer_shape,\n            img_dims,\n            self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def preprocess(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        fix_batch_size: bool = False,\n        **kwargs,\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n\"\"\"Preprocesses an object detection inference request.\n\n        Args:\n            request (ObjectDetectionInferenceRequest): The request object containing images.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.\n        \"\"\"\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        img_in /= 255.0\n\n        if self.batching_enabled:\n            batch_padding = 0\n            if FIX_BATCH_SIZE or fix_batch_size:\n                if MAX_BATCH_SIZE == float(\"inf\"):\n                    logger.warn(\n                        \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                    )\n                    batch_padding = 0\n                else:\n                    batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n            if batch_padding &lt; 0:\n                raise ValueError(\n                    f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                    f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                    f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                    f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n                )\n            width_remainder = img_in.shape[2] % 32\n            height_remainder = img_in.shape[3] % 32\n            if width_remainder &gt; 0:\n                width_padding = 32 - (img_in.shape[2] % 32)\n            else:\n                width_padding = 0\n            if height_remainder &gt; 0:\n                height_padding = 32 - (img_in.shape[3] % 32)\n            else:\n                height_padding = 0\n            img_in = np.pad(\n                img_in,\n                ((0, batch_padding), (0, 0), (0, width_padding), (0, height_padding)),\n                \"constant\",\n            )\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"disable_preproc_static_crop\": disable_preproc_static_crop,\n            }\n        )\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Runs inference on the ONNX model.\n\n        Args:\n            img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n        Returns:\n            Tuple[np.ndarray]: The ONNX model predictions.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError(\"predict must be implemented by a subclass\")\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            output_shape[2], masks=0\n        )\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, iou_threshold=DEFAULT_IOU_THRESH, fix_batch_size=False, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Runs object detection inference on one or multiple images and returns the detections.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The input image or a list of images to process. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to use class-agnostic non-maximum suppression. Defaults to False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression. Defaults to 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>fix_batch_size</code> <code>bool</code> <p>If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.</p> <code>False</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections after non-maximum suppression. Defaults to 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the processed images along with the predictions. Defaults to False.</p> <code>False</code> <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>*args</code> <p>Variable length argument list.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If <code>return_image_dims</code> is True, it will return a tuple with predictions and image dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batching is not enabled for the model and more than one image is passed for processing.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    fix_batch_size: bool = False,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; Any:\n\"\"\"\n    Runs object detection inference on one or multiple images and returns the detections.\n\n    Args:\n        image (Any): The input image or a list of images to process.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n        iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n        fix_batch_size (bool, optional): If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.\n        max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the processed images along with the predictions. Defaults to False.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If `return_image_dims` is True, it will return a tuple with predictions and image dimensions.\n\n    Raises:\n        ValueError: If batching is not enabled for the model and more than one image is passed for processing.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        iou_threshold=iou_threshold,\n        fix_batch_size=fix_batch_size,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n\"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n    \"\"\"\n\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n\n    predictions = predictions[\n        : len(img_dims)\n    ]  # If the batch size was fixed we have empty preds at the end\n    responses = [\n        ObjectDetectionInferenceResponse(\n            predictions=[\n                ObjectDetectionPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n                for pred in batch_predictions\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to apply class-agnostic non-max suppression. Default is False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression. Default is 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Default is 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray, ...],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n\"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n        max_candidates (int): Maximum number of candidate detections. Default is 3000.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=confidence,\n        iou_thresh=iou_threshold,\n        class_agnostic=class_agnostic_nms,\n        max_detections=max_detections,\n        max_candidate_detections=max_candidates,\n        box_format=self.box_format,\n    )\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions,\n        infer_shape,\n        img_dims,\n        self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Runs inference on the ONNX model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>The preprocessed image(s) to run inference on.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: The ONNX model predictions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Runs inference on the ONNX model.\n\n    Args:\n        img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n    Returns:\n        Tuple[np.ndarray]: The ONNX model predictions.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError(\"predict must be implemented by a subclass\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.preprocess","title":"<code>preprocess(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, fix_batch_size=False, **kwargs)</code>","text":"<p>Preprocesses an object detection inference request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ObjectDetectionInferenceRequest</code> <p>The request object containing images.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, PreprocessReturnMetadata]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def preprocess(\n    self,\n    image: Any,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    fix_batch_size: bool = False,\n    **kwargs,\n) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n\"\"\"Preprocesses an object detection inference request.\n\n    Args:\n        request (ObjectDetectionInferenceRequest): The request object containing images.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.\n    \"\"\"\n    img_in, img_dims = self.load_image(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    img_in /= 255.0\n\n    if self.batching_enabled:\n        batch_padding = 0\n        if FIX_BATCH_SIZE or fix_batch_size:\n            if MAX_BATCH_SIZE == float(\"inf\"):\n                logger.warn(\n                    \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                )\n                batch_padding = 0\n            else:\n                batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n        if batch_padding &lt; 0:\n            raise ValueError(\n                f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n            )\n        width_remainder = img_in.shape[2] % 32\n        height_remainder = img_in.shape[3] % 32\n        if width_remainder &gt; 0:\n            width_padding = 32 - (img_in.shape[2] % 32)\n        else:\n            width_padding = 0\n        if height_remainder &gt; 0:\n            height_padding = 32 - (img_in.shape[3] % 32)\n        else:\n            height_padding = 0\n        img_in = np.pad(\n            img_in,\n            ((0, batch_padding), (0, 0), (0, width_padding), (0, height_padding)),\n            \"constant\",\n        )\n\n    return img_in, PreprocessReturnMetadata(\n        {\n            \"img_dims\": img_dims,\n            \"disable_preproc_static_crop\": disable_preproc_static_crop,\n        }\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/","title":"roboflow","text":""},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowCoreModel","title":"<code>OnnxRoboflowCoreModel</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> <p>Roboflow Inference Model that operates using an ONNX model file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class OnnxRoboflowCoreModel(RoboflowCoreModel):\n\"\"\"Roboflow Inference Model that operates using an ONNX model file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel","title":"<code>OnnxRoboflowInferenceModel</code>","text":"<p>             Bases: <code>RoboflowInferenceModel</code></p> <p>Roboflow Inference Model that operates using an ONNX model file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class OnnxRoboflowInferenceModel(RoboflowInferenceModel):\n\"\"\"Roboflow Inference Model that operates using an ONNX model file.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        onnxruntime_execution_providers: List[\n            str\n        ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n        *args,\n        **kwargs,\n    ):\n\"\"\"Initializes the OnnxRoboflowInferenceModel instance.\n\n        Args:\n            model_id (str): The identifier for the specific ONNX model.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(model_id, *args, **kwargs)\n        if self.load_weights or not self.has_model_metadata:\n            self.onnxruntime_execution_providers = onnxruntime_execution_providers\n            for ep in self.onnxruntime_execution_providers:\n                if ep == \"TensorrtExecutionProvider\":\n                    ep = (\n                        \"TensorrtExecutionProvider\",\n                        {\n                            \"trt_engine_cache_enable\": True,\n                            \"trt_engine_cache_path\": os.path.join(\n                                TENSORRT_CACHE_PATH, self.endpoint\n                            ),\n                            \"trt_fp16_enable\": True,\n                        },\n                    )\n        self.initialize_model()\n        self.image_loader_threadpool = ThreadPoolExecutor(max_workers=None)\n        try:\n            self.validate_model()\n        except ModelArtefactError as e:\n            logger.error(f\"Unable to validate model artifacts, clearing cache: {e}\")\n            self.clear_cache()\n            raise ModelArtefactError from e\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n\"\"\"Runs inference on given data.\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        input_elements = calculate_input_elements(input_value=image)\n        max_batch_size = MAX_BATCH_SIZE if self.batching_enabled else self.batch_size\n        if (input_elements == 1) or (max_batch_size == float(\"inf\")):\n            return super().infer(image, **kwargs)\n        logger.debug(\n            f\"Inference will be executed in batches, as there is {input_elements} input elements and \"\n            f\"maximum batch size for a model is set to: {max_batch_size}\"\n        )\n        inference_results = []\n        for batch_input in create_batches(sequence=image, batch_size=max_batch_size):\n            batch_inference_results = super().infer(batch_input, **kwargs)\n            inference_results.append(batch_inference_results)\n        return self.merge_inference_results(inference_results=inference_results)\n\n    def merge_inference_results(self, inference_results: List[Any]) -&gt; Any:\n        return list(itertools.chain(*inference_results))\n\n    def validate_model(self) -&gt; None:\n        if MODEL_VALIDATION_DISABLED:\n            logger.debug(\"Model validation disabled.\")\n            return None\n        logger.debug(\"Starting model validation\")\n        if not self.load_weights:\n            return\n        try:\n            assert self.onnx_session is not None\n        except AssertionError as e:\n            raise ModelArtefactError(\n                \"ONNX session not initialized. Check that the model weights are available.\"\n            ) from e\n        try:\n            self.run_test_inference()\n        except Exception as e:\n            raise ModelArtefactError(f\"Unable to run test inference. Cause: {e}\") from e\n        try:\n            self.validate_model_classes()\n        except Exception as e:\n            raise ModelArtefactError(\n                f\"Unable to validate model classes. Cause: {e}\"\n            ) from e\n        logger.debug(\"Model validation finished\")\n\n    def run_test_inference(self) -&gt; None:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        logger.debug(f\"Running test inference. Image size: {test_image.shape}\")\n        result = self.infer(test_image)\n        logger.debug(f\"Test inference finished.\")\n        return result\n\n    def get_model_output_shape(self) -&gt; Tuple[int, int, int]:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        logger.debug(f\"Getting model output shape. Image size: {test_image.shape}\")\n        test_image, _ = self.preprocess(test_image)\n        output = self.predict(test_image)[0]\n        logger.debug(f\"Model output shape test finished.\")\n        return output.shape\n\n    def validate_model_classes(self) -&gt; None:\n        pass\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n        Returns:\n            list: A list of filenames specific to ONNX models.\n        \"\"\"\n        return [\"environment.json\", \"class_names.txt\"]\n\n    def initialize_model(self) -&gt; None:\n\"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n        logger.debug(\"Getting model artefacts\")\n        self.get_model_artifacts()\n        logger.debug(\"Creating inference session\")\n        if self.load_weights or not self.has_model_metadata:\n            t1_session = perf_counter()\n            # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n            providers = self.onnxruntime_execution_providers\n            if not self.load_weights:\n                providers = [\"OpenVINOExecutionProvider\", \"CPUExecutionProvider\"]\n            try:\n                self.onnx_session = onnxruntime.InferenceSession(\n                    self.cache_file(self.weights_file),\n                    providers=providers,\n                )\n            except Exception as e:\n                self.clear_cache()\n                raise ModelArtefactError(\n                    f\"Unable to load ONNX session. Cause: {e}\"\n                ) from e\n            logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n            if REQUIRED_ONNX_PROVIDERS:\n                available_providers = onnxruntime.get_available_providers()\n                for provider in REQUIRED_ONNX_PROVIDERS:\n                    if provider not in available_providers:\n                        raise OnnxProviderNotAvailable(\n                            f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                        )\n\n            inputs = self.onnx_session.get_inputs()[0]\n            input_shape = inputs.shape\n            self.batch_size = input_shape[0]\n            self.img_size_h = input_shape[2]\n            self.img_size_w = input_shape[3]\n            self.input_name = inputs.name\n            if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n                if \"resize\" in self.preproc:\n                    self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                    self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n                else:\n                    self.img_size_h = 640\n                    self.img_size_w = 640\n\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n\n            model_metadata = {\n                \"batch_size\": self.batch_size,\n                \"img_size_h\": self.img_size_h,\n                \"img_size_w\": self.img_size_w,\n            }\n            logger.debug(f\"Writing model metadata to memcache\")\n            self.write_model_metadata_to_memcache(model_metadata)\n            if not self.load_weights:  # had to load weights to get metadata\n                del self.onnx_session\n        else:\n            if not self.has_model_metadata:\n                raise ValueError(\n                    \"This should be unreachable, should get weights if we don't have model metadata\"\n                )\n            logger.debug(f\"Loading model metadata from memcache\")\n            metadata = self.model_metadata_from_memcache()\n            self.batch_size = metadata[\"batch_size\"]\n            self.img_size_h = metadata[\"img_size_h\"]\n            self.img_size_w = metadata[\"img_size_w\"]\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n        logger.debug(\"Model initialisation finished.\")\n\n    def load_image(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        if isinstance(image, list):\n            preproc_image = partial(\n                self.preproc_image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n            )\n            imgs_with_dims = self.image_loader_threadpool.map(preproc_image, image)\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in, img_dims = self.preproc_image(\n                image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n            )\n            img_dims = [img_dims]\n        return img_in, img_dims\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Returns the file containing the ONNX model weights.\n\n        Returns:\n            str: The file path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Returns the file containing the ONNX model weights.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path to the weights file.</p>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.__init__","title":"<code>__init__(model_id, onnxruntime_execution_providers=get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS), *args, **kwargs)</code>","text":"<p>Initializes the OnnxRoboflowInferenceModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier for the specific ONNX model.</p> required <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    onnxruntime_execution_providers: List[\n        str\n    ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n    *args,\n    **kwargs,\n):\n\"\"\"Initializes the OnnxRoboflowInferenceModel instance.\n\n    Args:\n        model_id (str): The identifier for the specific ONNX model.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(model_id, *args, **kwargs)\n    if self.load_weights or not self.has_model_metadata:\n        self.onnxruntime_execution_providers = onnxruntime_execution_providers\n        for ep in self.onnxruntime_execution_providers:\n            if ep == \"TensorrtExecutionProvider\":\n                ep = (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": os.path.join(\n                            TENSORRT_CACHE_PATH, self.endpoint\n                        ),\n                        \"trt_fp16_enable\": True,\n                    },\n                )\n    self.initialize_model()\n    self.image_loader_threadpool = ThreadPoolExecutor(max_workers=None)\n    try:\n        self.validate_model()\n    except ModelArtefactError as e:\n        logger.error(f\"Unable to validate model artifacts, clearing cache: {e}\")\n        self.clear_cache()\n        raise ModelArtefactError from e\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Returns the list of files to be downloaded from the inference bucket for ONNX model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of filenames specific to ONNX models.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n    Returns:\n        list: A list of filenames specific to ONNX models.\n    \"\"\"\n    return [\"environment.json\", \"class_names.txt\"]\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Runs inference on given data. - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n\"\"\"Runs inference on given data.\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    input_elements = calculate_input_elements(input_value=image)\n    max_batch_size = MAX_BATCH_SIZE if self.batching_enabled else self.batch_size\n    if (input_elements == 1) or (max_batch_size == float(\"inf\")):\n        return super().infer(image, **kwargs)\n    logger.debug(\n        f\"Inference will be executed in batches, as there is {input_elements} input elements and \"\n        f\"maximum batch size for a model is set to: {max_batch_size}\"\n    )\n    inference_results = []\n    for batch_input in create_batches(sequence=image, batch_size=max_batch_size):\n        batch_inference_results = super().infer(batch_input, **kwargs)\n        inference_results.append(batch_inference_results)\n    return self.merge_inference_results(inference_results=inference_results)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.initialize_model","title":"<code>initialize_model()</code>","text":"<p>Initializes the ONNX model, setting up the inference session and other necessary properties.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def initialize_model(self) -&gt; None:\n\"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n    logger.debug(\"Getting model artefacts\")\n    self.get_model_artifacts()\n    logger.debug(\"Creating inference session\")\n    if self.load_weights or not self.has_model_metadata:\n        t1_session = perf_counter()\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        providers = self.onnxruntime_execution_providers\n        if not self.load_weights:\n            providers = [\"OpenVINOExecutionProvider\", \"CPUExecutionProvider\"]\n        try:\n            self.onnx_session = onnxruntime.InferenceSession(\n                self.cache_file(self.weights_file),\n                providers=providers,\n            )\n        except Exception as e:\n            self.clear_cache()\n            raise ModelArtefactError(\n                f\"Unable to load ONNX session. Cause: {e}\"\n            ) from e\n        logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        inputs = self.onnx_session.get_inputs()[0]\n        input_shape = inputs.shape\n        self.batch_size = input_shape[0]\n        self.img_size_h = input_shape[2]\n        self.img_size_w = input_shape[3]\n        self.input_name = inputs.name\n        if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n            if \"resize\" in self.preproc:\n                self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n            else:\n                self.img_size_h = 640\n                self.img_size_w = 640\n\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n\n        model_metadata = {\n            \"batch_size\": self.batch_size,\n            \"img_size_h\": self.img_size_h,\n            \"img_size_w\": self.img_size_w,\n        }\n        logger.debug(f\"Writing model metadata to memcache\")\n        self.write_model_metadata_to_memcache(model_metadata)\n        if not self.load_weights:  # had to load weights to get metadata\n            del self.onnx_session\n    else:\n        if not self.has_model_metadata:\n            raise ValueError(\n                \"This should be unreachable, should get weights if we don't have model metadata\"\n            )\n        logger.debug(f\"Loading model metadata from memcache\")\n        metadata = self.model_metadata_from_memcache()\n        self.batch_size = metadata[\"batch_size\"]\n        self.img_size_h = metadata[\"img_size_h\"]\n        self.img_size_w = metadata[\"img_size_w\"]\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n    logger.debug(\"Model initialisation finished.\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel","title":"<code>RoboflowCoreModel</code>","text":"<p>             Bases: <code>RoboflowInferenceModel</code></p> <p>Base Roboflow inference model (Inherits from CvModel since all Roboflow models are CV models currently).</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class RoboflowCoreModel(RoboflowInferenceModel):\n\"\"\"Base Roboflow inference model (Inherits from CvModel since all Roboflow models are CV models currently).\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        api_key=None,\n    ):\n\"\"\"Initializes the RoboflowCoreModel instance.\n\n        Args:\n            model_id (str): The identifier for the specific model.\n            api_key ([type], optional): The API key for authentication. Defaults to None.\n        \"\"\"\n        super().__init__(model_id, api_key=api_key)\n        self.download_weights()\n\n    def download_weights(self) -&gt; None:\n\"\"\"Downloads the model weights from the configured source.\n\n        This method includes handling for AWS access keys and error handling.\n        \"\"\"\n        infer_bucket_files = self.get_infer_bucket_file_list()\n        if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n            logger.debug(\"Model artifacts already downloaded, loading from cache\")\n            return None\n        if is_model_artefacts_bucket_available():\n            self.download_model_artefacts_from_s3()\n            return None\n        self.download_model_from_roboflow_api()\n\n    def download_model_from_roboflow_api(self) -&gt; None:\n        api_data = get_roboflow_model_data(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.CORE_MODEL,\n            device_id=self.device_id,\n        )\n        if \"weights\" not in api_data:\n            raise ModelArtefactError(\n                f\"`weights` key not available in Roboflow API response while downloading model weights.\"\n            )\n        for weights_url_key in api_data[\"weights\"]:\n            weights_url = api_data[\"weights\"][weights_url_key]\n            t1 = perf_counter()\n            model_weights_response = get_from_url(weights_url, json_response=False)\n            filename = weights_url.split(\"?\")[0].split(\"/\")[-1]\n            save_bytes_in_cache(\n                content=model_weights_response.content,\n                file=filename,\n                model_id=self.endpoint,\n            )\n            if perf_counter() - t1 &gt; 120:\n                logger.debug(\n                    \"Weights download took longer than 120 seconds, refreshing API request\"\n                )\n                api_data = get_roboflow_model_data(\n                    api_key=self.api_key,\n                    model_id=self.endpoint,\n                    endpoint_type=ModelEndpointType.CORE_MODEL,\n                    device_id=self.device_id,\n                )\n\n    def get_device_id(self) -&gt; str:\n\"\"\"Returns the device ID associated with this model.\n\n        Returns:\n            str: The device ID.\n        \"\"\"\n        return self.device_id\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Abstract method to get the list of files to be downloaded from the inference bucket.\n\n        Raises:\n            NotImplementedError: This method must be implemented in subclasses.\n\n        Returns:\n            List[str]: A list of filenames.\n        \"\"\"\n        raise NotImplementedError(\n            \"get_infer_bucket_file_list not implemented for OnnxRoboflowCoreModel\"\n        )\n\n    def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n\"\"\"Abstract method to preprocess an image.\n\n        Raises:\n            NotImplementedError: This method must be implemented in subclasses.\n\n        Returns:\n            Image.Image: The preprocessed PIL image.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".preprocess_image\")\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Abstract property representing the file containing the model weights. For core models, all model artifacts are handled through get_infer_bucket_file_list method.\"\"\"\n        return None\n\n    @property\n    def model_artifact_bucket(self):\n        return CORE_MODEL_BUCKET\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Abstract property representing the file containing the model weights. For core models, all model artifacts are handled through get_infer_bucket_file_list method.</p>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.__init__","title":"<code>__init__(model_id, api_key=None)</code>","text":"<p>Initializes the RoboflowCoreModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier for the specific model.</p> required <code>api_key</code> <code>[type]</code> <p>The API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    api_key=None,\n):\n\"\"\"Initializes the RoboflowCoreModel instance.\n\n    Args:\n        model_id (str): The identifier for the specific model.\n        api_key ([type], optional): The API key for authentication. Defaults to None.\n    \"\"\"\n    super().__init__(model_id, api_key=api_key)\n    self.download_weights()\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.download_weights","title":"<code>download_weights()</code>","text":"<p>Downloads the model weights from the configured source.</p> <p>This method includes handling for AWS access keys and error handling.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def download_weights(self) -&gt; None:\n\"\"\"Downloads the model weights from the configured source.\n\n    This method includes handling for AWS access keys and error handling.\n    \"\"\"\n    infer_bucket_files = self.get_infer_bucket_file_list()\n    if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n        logger.debug(\"Model artifacts already downloaded, loading from cache\")\n        return None\n    if is_model_artefacts_bucket_available():\n        self.download_model_artefacts_from_s3()\n        return None\n    self.download_model_from_roboflow_api()\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.get_device_id","title":"<code>get_device_id()</code>","text":"<p>Returns the device ID associated with this model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The device ID.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_device_id(self) -&gt; str:\n\"\"\"Returns the device ID associated with this model.\n\n    Returns:\n        str: The device ID.\n    \"\"\"\n    return self.device_id\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Abstract method to get the list of files to be downloaded from the inference bucket.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in subclasses.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of filenames.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Abstract method to get the list of files to be downloaded from the inference bucket.\n\n    Raises:\n        NotImplementedError: This method must be implemented in subclasses.\n\n    Returns:\n        List[str]: A list of filenames.\n    \"\"\"\n    raise NotImplementedError(\n        \"get_infer_bucket_file_list not implemented for OnnxRoboflowCoreModel\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Abstract method to preprocess an image.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in subclasses.</p> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The preprocessed PIL image.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n\"\"\"Abstract method to preprocess an image.\n\n    Raises:\n        NotImplementedError: This method must be implemented in subclasses.\n\n    Returns:\n        Image.Image: The preprocessed PIL image.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".preprocess_image\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel","title":"<code>RoboflowInferenceModel</code>","text":"<p>             Bases: <code>Model</code></p> <p>Base Roboflow inference model.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class RoboflowInferenceModel(Model):\n\"\"\"Base Roboflow inference model.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        cache_dir_root=MODEL_CACHE_DIR,\n        api_key=None,\n        load_weights=True,\n    ):\n\"\"\"\n        Initialize the RoboflowInferenceModel object.\n\n        Args:\n            model_id (str): The unique identifier for the model.\n            cache_dir_root (str, optional): The root directory for the cache. Defaults to MODEL_CACHE_DIR.\n            api_key (str, optional): API key for authentication. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.load_weights = load_weights\n        self.metrics = {\"num_inferences\": 0, \"avg_inference_time\": 0.0}\n        self.api_key = api_key if api_key else API_KEY\n        model_id = resolve_roboflow_model_alias(model_id=model_id)\n        self.dataset_id, self.version_id = model_id.split(\"/\")\n        self.endpoint = model_id\n        self.device_id = GLOBAL_DEVICE_ID\n        self.cache_dir = os.path.join(cache_dir_root, self.endpoint)\n        self.keypoints_metadata: Optional[dict] = None\n        initialise_cache(model_id=self.endpoint)\n\n    def cache_file(self, f: str) -&gt; str:\n\"\"\"Get the cache file path for a given file.\n\n        Args:\n            f (str): Filename.\n\n        Returns:\n            str: Full path to the cached file.\n        \"\"\"\n        return get_cache_file_path(file=f, model_id=self.endpoint)\n\n    def clear_cache(self) -&gt; None:\n\"\"\"Clear the cache directory.\"\"\"\n        clear_cache(model_id=self.endpoint)\n\n    def draw_predictions(\n        self,\n        inference_request: InferenceRequest,\n        inference_response: InferenceResponse,\n    ) -&gt; bytes:\n\"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n        Args:\n            inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n            inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n        Returns:\n            str: A base64 encoded image string\n        \"\"\"\n        return draw_detection_predictions(\n            inference_request=inference_request,\n            inference_response=inference_response,\n            colors=self.colors,\n        )\n\n    @property\n    def get_class_names(self):\n        return self.class_names\n\n    def get_device_id(self) -&gt; str:\n\"\"\"\n        Get the device identifier on which the model is deployed.\n\n        Returns:\n            str: Device identifier.\n        \"\"\"\n        return self.device_id\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Get a list of inference bucket files.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n\n        Returns:\n            List[str]: A list of inference bucket files.\n        \"\"\"\n        raise NotImplementedError(\n            self.__class__.__name__ + \".get_infer_bucket_file_list\"\n        )\n\n    @property\n    def cache_key(self):\n        return f\"metadata:{self.endpoint}\"\n\n    @staticmethod\n    def model_metadata_from_memcache_endpoint(endpoint):\n        model_metadata = cache.get(f\"metadata:{endpoint}\")\n        return model_metadata\n\n    def model_metadata_from_memcache(self):\n        model_metadata = cache.get(self.cache_key)\n        return model_metadata\n\n    def write_model_metadata_to_memcache(self, metadata):\n        cache.set(\n            self.cache_key, metadata, expire=MODEL_METADATA_CACHE_EXPIRATION_TIMEOUT\n        )\n\n    @property\n    def has_model_metadata(self):\n        return self.model_metadata_from_memcache() is not None\n\n    def get_model_artifacts(self) -&gt; None:\n\"\"\"Fetch or load the model artifacts.\n\n        Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.\n        \"\"\"\n        self.cache_model_artefacts()\n        self.load_model_artifacts_from_cache()\n\n    def cache_model_artefacts(self) -&gt; None:\n        infer_bucket_files = self.get_all_required_infer_bucket_file()\n        if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n            return None\n        if is_model_artefacts_bucket_available():\n            self.download_model_artefacts_from_s3()\n            return None\n        self.download_model_artifacts_from_roboflow_api()\n\n    def get_all_required_infer_bucket_file(self) -&gt; List[str]:\n        infer_bucket_files = self.get_infer_bucket_file_list()\n        infer_bucket_files.append(self.weights_file)\n        logger.debug(f\"List of files required to load model: {infer_bucket_files}\")\n        return [f for f in infer_bucket_files if f is not None]\n\n    def download_model_artefacts_from_s3(self) -&gt; None:\n        try:\n            logger.debug(\"Downloading model artifacts from S3\")\n            infer_bucket_files = self.get_all_required_infer_bucket_file()\n            cache_directory = get_cache_dir()\n            s3_keys = [f\"{self.endpoint}/{file}\" for file in infer_bucket_files]\n            download_s3_files_to_directory(\n                bucket=self.model_artifact_bucket,\n                keys=s3_keys,\n                target_dir=cache_directory,\n                s3_client=S3_CLIENT,\n            )\n        except Exception as error:\n            raise ModelArtefactError(\n                f\"Could not obtain model artefacts from S3 with keys {s3_keys}. Cause: {error}\"\n            ) from error\n\n    @property\n    def model_artifact_bucket(self):\n        return INFER_BUCKET\n\n    def download_model_artifacts_from_roboflow_api(self) -&gt; None:\n        logger.debug(\"Downloading model artifacts from Roboflow API\")\n        api_data = get_roboflow_model_data(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.ORT,\n            device_id=self.device_id,\n        )\n        if \"ort\" not in api_data.keys():\n            raise ModelArtefactError(\n                \"Could not find `ort` key in roboflow API model description response.\"\n            )\n        api_data = api_data[\"ort\"]\n        if \"classes\" in api_data:\n            save_text_lines_in_cache(\n                content=api_data[\"classes\"],\n                file=\"class_names.txt\",\n                model_id=self.endpoint,\n            )\n        if \"model\" not in api_data:\n            raise ModelArtefactError(\n                \"Could not find `model` key in roboflow API model description response.\"\n            )\n        if \"environment\" not in api_data:\n            raise ModelArtefactError(\n                \"Could not find `environment` key in roboflow API model description response.\"\n            )\n        environment = get_from_url(api_data[\"environment\"])\n        model_weights_response = get_from_url(api_data[\"model\"], json_response=False)\n        save_bytes_in_cache(\n            content=model_weights_response.content,\n            file=self.weights_file,\n            model_id=self.endpoint,\n        )\n        if \"colors\" in api_data:\n            environment[\"COLORS\"] = api_data[\"colors\"]\n        save_json_in_cache(\n            content=environment,\n            file=\"environment.json\",\n            model_id=self.endpoint,\n        )\n        if \"keypoints_metadata\" in api_data:\n            # TODO: make sure backend provides that\n            save_json_in_cache(\n                content=api_data[\"keypoints_metadata\"],\n                file=\"keypoints_metadata.json\",\n                model_id=self.endpoint,\n            )\n\n    def load_model_artifacts_from_cache(self) -&gt; None:\n        logger.debug(\"Model artifacts already downloaded, loading model from cache\")\n        infer_bucket_files = self.get_all_required_infer_bucket_file()\n        if \"environment.json\" in infer_bucket_files:\n            self.environment = load_json_from_cache(\n                file=\"environment.json\",\n                model_id=self.endpoint,\n                object_pairs_hook=OrderedDict,\n            )\n        if \"class_names.txt\" in infer_bucket_files:\n            self.class_names = load_text_file_from_cache(\n                file=\"class_names.txt\",\n                model_id=self.endpoint,\n                split_lines=True,\n                strip_white_chars=True,\n            )\n        else:\n            self.class_names = get_class_names_from_environment_file(\n                environment=self.environment\n            )\n        self.colors = get_color_mapping_from_environment(\n            environment=self.environment,\n            class_names=self.class_names,\n        )\n        if \"keypoints_metadata.json\" in infer_bucket_files:\n            self.keypoints_metadata = parse_keypoints_metadata(\n                load_json_from_cache(\n                    file=\"keypoints_metadata.json\",\n                    model_id=self.endpoint,\n                    object_pairs_hook=OrderedDict,\n                )\n            )\n        self.num_classes = len(self.class_names)\n        if \"PREPROCESSING\" not in self.environment:\n            raise ModelArtefactError(\n                \"Could not find `PREPROCESSING` key in environment file.\"\n            )\n        if issubclass(type(self.environment[\"PREPROCESSING\"]), dict):\n            self.preproc = self.environment[\"PREPROCESSING\"]\n        else:\n            self.preproc = json.loads(self.environment[\"PREPROCESSING\"])\n        if self.preproc.get(\"resize\"):\n            self.resize_method = self.preproc[\"resize\"].get(\"format\", \"Stretch to\")\n            if self.resize_method not in [\n                \"Stretch to\",\n                \"Fit (black edges) in\",\n                \"Fit (white edges) in\",\n            ]:\n                self.resize_method = \"Stretch to\"\n        else:\n            self.resize_method = \"Stretch to\"\n        logger.debug(f\"Resize method is '{self.resize_method}'\")\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    def initialize_model(self) -&gt; None:\n\"\"\"Initialize the model.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".initialize_model\")\n\n    def preproc_image(\n        self,\n        image: Union[Any, InferenceRequestImage],\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n\"\"\"\n        Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n        Args:\n            image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n        \"\"\"\n        np_image, is_bgr = load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient\n            or \"auto-orient\" not in self.preproc.keys()\n            or DISABLE_PREPROC_AUTO_ORIENT,\n        )\n        preprocessed_image, img_dims = self.preprocess_image(\n            np_image,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        if self.resize_method == \"Stretch to\":\n            resized = cv2.resize(\n                preprocessed_image, (self.img_size_w, self.img_size_h), cv2.INTER_CUBIC\n            )\n        elif self.resize_method == \"Fit (black edges) in\":\n            resized = letterbox_image(\n                preprocessed_image, (self.img_size_w, self.img_size_h)\n            )\n        elif self.resize_method == \"Fit (white edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(255, 255, 255),\n            )\n\n        if is_bgr:\n            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        img_in = np.transpose(resized, (2, 0, 1))\n        img_in = img_in.astype(np.float32)\n        img_in = np.expand_dims(img_in, axis=0)\n\n        return img_in, img_dims\n\n    def preprocess_image(\n        self,\n        image: np.ndarray,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n\"\"\"\n        Preprocesses the given image using specified preprocessing steps.\n\n        Args:\n            image (Image.Image): The PIL image to preprocess.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Image.Image: The preprocessed PIL image.\n        \"\"\"\n        return prepare(\n            image,\n            self.preproc,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Abstract property representing the file containing the model weights.\n\n        Raises:\n            NotImplementedError: This property must be implemented in subclasses.\n\n        Returns:\n            str: The file path to the weights file.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".weights_file\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Abstract property representing the file containing the model weights.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This property must be implemented in subclasses.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path to the weights file.</p>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.__init__","title":"<code>__init__(model_id, cache_dir_root=MODEL_CACHE_DIR, api_key=None, load_weights=True)</code>","text":"<p>Initialize the RoboflowInferenceModel object.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The unique identifier for the model.</p> required <code>cache_dir_root</code> <code>str</code> <p>The root directory for the cache. Defaults to MODEL_CACHE_DIR.</p> <code>MODEL_CACHE_DIR</code> <code>api_key</code> <code>str</code> <p>API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    cache_dir_root=MODEL_CACHE_DIR,\n    api_key=None,\n    load_weights=True,\n):\n\"\"\"\n    Initialize the RoboflowInferenceModel object.\n\n    Args:\n        model_id (str): The unique identifier for the model.\n        cache_dir_root (str, optional): The root directory for the cache. Defaults to MODEL_CACHE_DIR.\n        api_key (str, optional): API key for authentication. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.load_weights = load_weights\n    self.metrics = {\"num_inferences\": 0, \"avg_inference_time\": 0.0}\n    self.api_key = api_key if api_key else API_KEY\n    model_id = resolve_roboflow_model_alias(model_id=model_id)\n    self.dataset_id, self.version_id = model_id.split(\"/\")\n    self.endpoint = model_id\n    self.device_id = GLOBAL_DEVICE_ID\n    self.cache_dir = os.path.join(cache_dir_root, self.endpoint)\n    self.keypoints_metadata: Optional[dict] = None\n    initialise_cache(model_id=self.endpoint)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.cache_file","title":"<code>cache_file(f)</code>","text":"<p>Get the cache file path for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>Filename.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Full path to the cached file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def cache_file(self, f: str) -&gt; str:\n\"\"\"Get the cache file path for a given file.\n\n    Args:\n        f (str): Filename.\n\n    Returns:\n        str: Full path to the cached file.\n    \"\"\"\n    return get_cache_file_path(file=f, model_id=self.endpoint)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache directory.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def clear_cache(self) -&gt; None:\n\"\"\"Clear the cache directory.\"\"\"\n    clear_cache(model_id=self.endpoint)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw predictions from an inference response onto the original image provided by an inference request</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>ObjectDetectionInferenceRequest</code> <p>The inference request containing the image on which to draw predictions</p> required <code>inference_response</code> <code>ObjectDetectionInferenceResponse</code> <p>The inference response containing predictions to be drawn</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>bytes</code> <p>A base64 encoded image string</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def draw_predictions(\n    self,\n    inference_request: InferenceRequest,\n    inference_response: InferenceResponse,\n) -&gt; bytes:\n\"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n    Args:\n        inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n        inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n    Returns:\n        str: A base64 encoded image string\n    \"\"\"\n    return draw_detection_predictions(\n        inference_request=inference_request,\n        inference_response=inference_response,\n        colors=self.colors,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_device_id","title":"<code>get_device_id()</code>","text":"<p>Get the device identifier on which the model is deployed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Device identifier.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_device_id(self) -&gt; str:\n\"\"\"\n    Get the device identifier on which the model is deployed.\n\n    Returns:\n        str: Device identifier.\n    \"\"\"\n    return self.device_id\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get a list of inference bucket files.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of inference bucket files.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Get a list of inference bucket files.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n\n    Returns:\n        List[str]: A list of inference bucket files.\n    \"\"\"\n    raise NotImplementedError(\n        self.__class__.__name__ + \".get_infer_bucket_file_list\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_model_artifacts","title":"<code>get_model_artifacts()</code>","text":"<p>Fetch or load the model artifacts.</p> <p>Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_model_artifacts(self) -&gt; None:\n\"\"\"Fetch or load the model artifacts.\n\n    Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.\n    \"\"\"\n    self.cache_model_artefacts()\n    self.load_model_artifacts_from_cache()\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.initialize_model","title":"<code>initialize_model()</code>","text":"<p>Initialize the model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def initialize_model(self) -&gt; None:\n\"\"\"Initialize the model.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".initialize_model\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.preproc_image","title":"<code>preproc_image(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Any, InferenceRequestImage]</code> <p>An object containing information necessary to load the image for inference.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preproc_image(\n    self,\n    image: Union[Any, InferenceRequestImage],\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n\"\"\"\n    Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n    Args:\n        image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n    \"\"\"\n    np_image, is_bgr = load_image(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient\n        or \"auto-orient\" not in self.preproc.keys()\n        or DISABLE_PREPROC_AUTO_ORIENT,\n    )\n    preprocessed_image, img_dims = self.preprocess_image(\n        np_image,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    if self.resize_method == \"Stretch to\":\n        resized = cv2.resize(\n            preprocessed_image, (self.img_size_w, self.img_size_h), cv2.INTER_CUBIC\n        )\n    elif self.resize_method == \"Fit (black edges) in\":\n        resized = letterbox_image(\n            preprocessed_image, (self.img_size_w, self.img_size_h)\n        )\n    elif self.resize_method == \"Fit (white edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(255, 255, 255),\n        )\n\n    if is_bgr:\n        resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_in = np.transpose(resized, (2, 0, 1))\n    img_in = img_in.astype(np.float32)\n    img_in = np.expand_dims(img_in, axis=0)\n\n    return img_in, img_dims\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.preprocess_image","title":"<code>preprocess_image(image, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses the given image using specified preprocessing steps.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL image to preprocess.</p> required <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Image.Image: The preprocessed PIL image.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preprocess_image(\n    self,\n    image: np.ndarray,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n\"\"\"\n    Preprocesses the given image using specified preprocessing steps.\n\n    Args:\n        image (Image.Image): The PIL image to preprocess.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Image.Image: The preprocessed PIL image.\n    \"\"\"\n    return prepare(\n        image,\n        self.preproc,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/stubs/","title":"stubs","text":""},{"location":"docs/reference/inference/core/models/types/","title":"types","text":""},{"location":"docs/reference/inference/core/models/utils/batching/","title":"batching","text":""},{"location":"docs/reference/inference/core/models/utils/keypoints/","title":"keypoints","text":""},{"location":"docs/reference/inference/core/models/utils/keypoints/#inference.core.models.utils.keypoints.superset_keypoints_count","title":"<code>superset_keypoints_count(keypoints_metadata={})</code>","text":"<p>Returns the number of keypoints in the superset.</p> Source code in <code>inference/core/models/utils/keypoints.py</code> <pre><code>def superset_keypoints_count(keypoints_metadata={}) -&gt; int:\n\"\"\"Returns the number of keypoints in the superset.\"\"\"\n    max_keypoints = 0\n    for keypoints in keypoints_metadata.values():\n        if len(keypoints) &gt; max_keypoints:\n            max_keypoints = len(keypoints)\n    return max_keypoints\n</code></pre>"},{"location":"docs/reference/inference/core/models/utils/validate/","title":"validate","text":""},{"location":"docs/reference/inference/core/registries/base/","title":"base","text":""},{"location":"docs/reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry","title":"<code>ModelRegistry</code>","text":"<p>An object which is able to return model classes based on given model IDs and model types.</p> <p>Attributes:</p> Name Type Description <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>class ModelRegistry:\n\"\"\"An object which is able to return model classes based on given model IDs and model types.\n\n    Attributes:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n\n    def __init__(self, registry_dict) -&gt; None:\n\"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n        Args:\n            registry_dict (dict): A dictionary mapping model types to model classes.\n        \"\"\"\n        self.registry_dict = registry_dict\n\n    def get_model(self, model_type: str, model_id: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model type.\n\n        Args:\n            model_type (str): The type of the model to be retrieved.\n            model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n        Returns:\n            Model: The model class corresponding to the given model type.\n\n        Raises:\n            ModelNotRecognisedError: If the model_type is not found in the registry_dict.\n        \"\"\"\n        if model_type not in self.registry_dict:\n            raise ModelNotRecognisedError(\n                f\"Could not find model of type: {model_type} in configured registry.\"\n            )\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry.__init__","title":"<code>__init__(registry_dict)</code>","text":"<p>Initializes the ModelRegistry with the given dictionary of registered models.</p> <p>Parameters:</p> Name Type Description Default <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> required Source code in <code>inference/core/registries/base.py</code> <pre><code>def __init__(self, registry_dict) -&gt; None:\n\"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n    Args:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n    self.registry_dict = registry_dict\n</code></pre>"},{"location":"docs/reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry.get_model","title":"<code>get_model(model_type, model_id)</code>","text":"<p>Returns the model class based on the given model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of the model to be retrieved.</p> required <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved (unused in the current implementation).</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model type.</p> <p>Raises:</p> Type Description <code>ModelNotRecognisedError</code> <p>If the model_type is not found in the registry_dict.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>def get_model(self, model_type: str, model_id: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model type.\n\n    Args:\n        model_type (str): The type of the model to be retrieved.\n        model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n    Returns:\n        Model: The model class corresponding to the given model type.\n\n    Raises:\n        ModelNotRecognisedError: If the model_type is not found in the registry_dict.\n    \"\"\"\n    if model_type not in self.registry_dict:\n        raise ModelNotRecognisedError(\n            f\"Could not find model of type: {model_type} in configured registry.\"\n        )\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/roboflow/","title":"roboflow","text":""},{"location":"docs/reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry","title":"<code>RoboflowModelRegistry</code>","text":"<p>             Bases: <code>ModelRegistry</code></p> <p>A Roboflow-specific model registry which gets the model type using the model id, then returns a model class based on the model type.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>class RoboflowModelRegistry(ModelRegistry):\n\"\"\"A Roboflow-specific model registry which gets the model type using the model id,\n    then returns a model class based on the model type.\n    \"\"\"\n\n    def get_model(self, model_id: str, api_key: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model id and API key.\n\n        Args:\n            model_id (str): The ID of the model to be retrieved.\n            api_key (str): The API key used to authenticate.\n\n        Returns:\n            Model: The model class corresponding to the given model ID and type.\n\n        Raises:\n            ModelNotRecognisedError: If the model type is not supported or found.\n        \"\"\"\n        model_type = get_model_type(model_id, api_key)\n        if model_type not in self.registry_dict:\n            raise ModelNotRecognisedError(f\"Model type not supported: {model_type}\")\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry.get_model","title":"<code>get_model(model_id, api_key)</code>","text":"<p>Returns the model class based on the given model id and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model ID and type.</p> <p>Raises:</p> Type Description <code>ModelNotRecognisedError</code> <p>If the model type is not supported or found.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model(self, model_id: str, api_key: str) -&gt; Model:\n\"\"\"Returns the model class based on the given model id and API key.\n\n    Args:\n        model_id (str): The ID of the model to be retrieved.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        Model: The model class corresponding to the given model ID and type.\n\n    Raises:\n        ModelNotRecognisedError: If the model type is not supported or found.\n    \"\"\"\n    model_type = get_model_type(model_id, api_key)\n    if model_type not in self.registry_dict:\n        raise ModelNotRecognisedError(f\"Model type not supported: {model_type}\")\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.get_model_type","title":"<code>get_model_type(model_id, api_key=None)</code>","text":"<p>Retrieves the model type based on the given model ID and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[TaskType, ModelType]</code> <p>The project task type and the model type.</p> <p>Raises:</p> Type Description <code>WorkspaceLoadError</code> <p>If the workspace could not be loaded or if the API key is invalid.</p> <code>DatasetLoadError</code> <p>If the dataset could not be loaded due to invalid ID, workspace ID or version ID.</p> <code>MissingDefaultModelError</code> <p>If default model is not configured and API does not provide this info</p> <code>MalformedRoboflowAPIResponseError</code> <p>Roboflow API responds in invalid format.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model_type(\n    model_id: str,\n    api_key: Optional[str] = None,\n) -&gt; Tuple[TaskType, ModelType]:\n\"\"\"Retrieves the model type based on the given model ID and API key.\n\n    Args:\n        model_id (str): The ID of the model.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        tuple: The project task type and the model type.\n\n    Raises:\n        WorkspaceLoadError: If the workspace could not be loaded or if the API key is invalid.\n        DatasetLoadError: If the dataset could not be loaded due to invalid ID, workspace ID or version ID.\n        MissingDefaultModelError: If default model is not configured and API does not provide this info\n        MalformedRoboflowAPIResponseError: Roboflow API responds in invalid format.\n    \"\"\"\n    model_id = resolve_roboflow_model_alias(model_id=model_id)\n    dataset_id, version_id = get_model_id_chunks(model_id=model_id)\n    if dataset_id in GENERIC_MODELS:\n        logger.debug(f\"Loading generic model: {dataset_id}.\")\n        return GENERIC_MODELS[dataset_id]\n    cached_metadata = get_model_metadata_from_cache(\n        dataset_id=dataset_id, version_id=version_id\n    )\n    if cached_metadata is not None:\n        return cached_metadata[0], cached_metadata[1]\n    if version_id == STUB_VERSION_ID:\n        if api_key is None:\n            raise MissingApiKeyError(\n                \"Stub model version provided but no API key was provided. API key is required to load stub models.\"\n            )\n        workspace_id = get_roboflow_workspace(api_key=api_key)\n        project_task_type = get_roboflow_dataset_type(\n            api_key=api_key, workspace_id=workspace_id, dataset_id=dataset_id\n        )\n        model_type = \"stub\"\n        save_model_metadata_in_cache(\n            dataset_id=dataset_id,\n            version_id=version_id,\n            project_task_type=project_task_type,\n            model_type=model_type,\n        )\n        return project_task_type, model_type\n    api_data = get_roboflow_model_data(\n        api_key=api_key,\n        model_id=model_id,\n        endpoint_type=ModelEndpointType.ORT,\n        device_id=GLOBAL_DEVICE_ID,\n    ).get(\"ort\")\n    if api_data is None:\n        raise ModelArtefactError(\"Error loading model artifacts from Roboflow API.\")\n    # some older projects do not have type field - hence defaulting\n    project_task_type = api_data.get(\"type\", \"object-detection\")\n    model_type = api_data.get(\"modelType\")\n    if model_type is None or model_type == \"ort\":\n        # some very old model versions do not have modelType reported - and API respond in a generic way -\n        # then we shall attempt using default model for given task type\n        model_type = MODEL_TYPE_DEFAULTS.get(project_task_type)\n    if model_type is None or project_task_type is None:\n        raise ModelArtefactError(\"Error loading model artifacts from Roboflow API.\")\n    save_model_metadata_in_cache(\n        dataset_id=dataset_id,\n        version_id=version_id,\n        project_task_type=project_task_type,\n        model_type=model_type,\n    )\n\n    return project_task_type, model_type\n</code></pre>"},{"location":"docs/reference/inference/core/utils/drawing/","title":"drawing","text":""},{"location":"docs/reference/inference/core/utils/environment/","title":"environment","text":""},{"location":"docs/reference/inference/core/utils/environment/#inference.core.utils.environment.safe_env_to_type","title":"<code>safe_env_to_type(variable_name, default_value=None, type_constructor=None)</code>","text":"<p>Converts env variable to specified type, but only if variable is set - otherwise default is returned. If <code>type_constructor</code> is not given - value of type str will be returned.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def safe_env_to_type(\n    variable_name: str,\n    default_value: Optional[T] = None,\n    type_constructor: Optional[Union[Type[T], Callable[[str], T]]] = None,\n) -&gt; Optional[T]:\n\"\"\"\n    Converts env variable to specified type, but only if variable is set - otherwise default is returned.\n    If `type_constructor` is not given - value of type str will be returned.\n    \"\"\"\n    if variable_name not in os.environ:\n        return default_value\n    variable_value = os.environ[variable_name]\n    if type_constructor is None:\n        return variable_value\n    return type_constructor(variable_value)\n</code></pre>"},{"location":"docs/reference/inference/core/utils/environment/#inference.core.utils.environment.safe_split_value","title":"<code>safe_split_value(value, delimiter=',')</code>","text":"<p>Splits a separated environment variable into a list.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The environment variable value to be split.</p> required <code>delimiter(str)</code> <p>Delimiter to be used</p> required <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list or None: The split values as a list, or None if the input is None.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def safe_split_value(value: Optional[str], delimiter: str = \",\") -&gt; Optional[List[str]]:\n\"\"\"\n    Splits a separated environment variable into a list.\n\n    Args:\n        value (str): The environment variable value to be split.\n        delimiter(str): Delimiter to be used\n\n    Returns:\n        list or None: The split values as a list, or None if the input is None.\n    \"\"\"\n    if value is None:\n        return None\n    else:\n        return value.split(delimiter)\n</code></pre>"},{"location":"docs/reference/inference/core/utils/environment/#inference.core.utils.environment.str2bool","title":"<code>str2bool(value)</code>","text":"<p>Converts an environment variable to a boolean value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str or bool</code> <p>The environment variable value to be converted.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>The converted boolean value.</p> <p>Raises:</p> Type Description <code>InvalidEnvironmentVariableError</code> <p>If the value is not 'true', 'false', or a boolean.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def str2bool(value: Any) -&gt; bool:\n\"\"\"\n    Converts an environment variable to a boolean value.\n\n    Args:\n        value (str or bool): The environment variable value to be converted.\n\n    Returns:\n        bool: The converted boolean value.\n\n    Raises:\n        InvalidEnvironmentVariableError: If the value is not 'true', 'false', or a boolean.\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n    if not issubclass(type(value), str):\n        raise InvalidEnvironmentVariableError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n    if value.lower() == \"true\":\n        return True\n    elif value.lower() == \"false\":\n        return False\n    else:\n        raise InvalidEnvironmentVariableError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/file_system/","title":"file_system","text":""},{"location":"docs/reference/inference/core/utils/function/","title":"function","text":""},{"location":"docs/reference/inference/core/utils/hash/","title":"hash","text":""},{"location":"docs/reference/inference/core/utils/image_utils/","title":"image_utils","text":""},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.attempt_loading_image_from_string","title":"<code>attempt_loading_image_from_string(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Attempt to load an image from a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, bytes, bytearray, _IOBase]</code> <p>The image data in string format.</p> required <code>cv_imread_flags</code> <code>int</code> <p>OpenCV flags used for image reading.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: A tuple of the loaded image in numpy array format and a boolean flag indicating if the image is in BGR format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def attempt_loading_image_from_string(\n    value: Union[str, bytes, bytearray, _IOBase],\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n\"\"\"\n    Attempt to load an image from a string.\n\n    Args:\n        value (Union[str, bytes, bytearray, _IOBase]): The image data in string format.\n        cv_imread_flags (int): OpenCV flags used for image reading.\n\n    Returns:\n        Tuple[np.ndarray, bool]: A tuple of the loaded image in numpy array format and a boolean flag indicating if the image is in BGR format.\n    \"\"\"\n    try:\n        return load_image_base64(value=value, cv_imread_flags=cv_imread_flags), True\n    except:\n        pass\n    try:\n        return (\n            load_image_from_encoded_bytes(value=value, cv_imread_flags=cv_imread_flags),\n            True,\n        )\n    except:\n        pass\n    try:\n        return (\n            load_image_from_buffer(value=value, cv_imread_flags=cv_imread_flags),\n            True,\n        )\n    except:\n        pass\n    try:\n        return load_image_from_numpy_str(value=value), True\n    except InvalidNumpyInput as error:\n        raise InputFormatInferenceFailed(\n            message=\"Input image format could not be inferred from string.\",\n            public_message=\"Input image format could not be inferred from string.\",\n        ) from error\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.choose_image_decoding_flags","title":"<code>choose_image_decoding_flags(disable_preproc_auto_orient)</code>","text":"<p>Choose the appropriate OpenCV image decoding flags.</p> <p>Parameters:</p> Name Type Description Default <code>disable_preproc_auto_orient</code> <code>bool</code> <p>Flag to disable preprocessing auto-orientation.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>OpenCV image decoding flags.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def choose_image_decoding_flags(disable_preproc_auto_orient: bool) -&gt; int:\n\"\"\"Choose the appropriate OpenCV image decoding flags.\n\n    Args:\n        disable_preproc_auto_orient (bool): Flag to disable preprocessing auto-orientation.\n\n    Returns:\n        int: OpenCV image decoding flags.\n    \"\"\"\n    cv_imread_flags = cv2.IMREAD_COLOR\n    if disable_preproc_auto_orient:\n        cv_imread_flags = cv_imread_flags | cv2.IMREAD_IGNORE_ORIENTATION\n    return cv_imread_flags\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.convert_gray_image_to_bgr","title":"<code>convert_gray_image_to_bgr(image)</code>","text":"<p>Convert a grayscale image to BGR format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The grayscale image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The converted BGR image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def convert_gray_image_to_bgr(image: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Convert a grayscale image to BGR format.\n\n    Args:\n        image (np.ndarray): The grayscale image.\n\n    Returns:\n        np.ndarray: The converted BGR image.\n    \"\"\"\n\n    if len(image.shape) == 2 or image.shape[2] == 1:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    return image\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.encode_image_to_jpeg_bytes","title":"<code>encode_image_to_jpeg_bytes(image, jpeg_quality=90)</code>","text":"<p>Encode a numpy image to JPEG format in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <code>jpeg_quality</code> <code>int</code> <p>Quality of the JPEG image.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The JPEG encoded image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def encode_image_to_jpeg_bytes(image: np.ndarray, jpeg_quality: int = 90) -&gt; bytes:\n\"\"\"\n    Encode a numpy image to JPEG format in bytes.\n\n    Args:\n        image (np.ndarray): The numpy array representing an image.\n        jpeg_quality (int): Quality of the JPEG image.\n\n    Returns:\n        bytes: The JPEG encoded image.\n    \"\"\"\n    encoding_param = [int(cv2.IMWRITE_JPEG_QUALITY), jpeg_quality]\n    _, img_encoded = cv2.imencode(\".jpg\", image, encoding_param)\n    return np.array(img_encoded).tobytes()\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.extract_image_payload_and_type","title":"<code>extract_image_payload_and_type(value)</code>","text":"<p>Extract the image payload and type from the given value.</p> <p>This function supports different types of image inputs (e.g., InferenceRequestImage, dict, etc.) and extracts the relevant data and image type for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value which can be an image or information to derive the image.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Optional[ImageType]]</code> <p>Tuple[Any, Optional[ImageType]]: A tuple containing the extracted image data and the corresponding image type.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def extract_image_payload_and_type(value: Any) -&gt; Tuple[Any, Optional[ImageType]]:\n\"\"\"Extract the image payload and type from the given value.\n\n    This function supports different types of image inputs (e.g., InferenceRequestImage, dict, etc.)\n    and extracts the relevant data and image type for further processing.\n\n    Args:\n        value (Any): The input value which can be an image or information to derive the image.\n\n    Returns:\n        Tuple[Any, Optional[ImageType]]: A tuple containing the extracted image data and the corresponding image type.\n    \"\"\"\n    image_type = None\n    if issubclass(type(value), InferenceRequestImage):\n        image_type = value.type\n        value = value.value\n    elif issubclass(type(value), dict):\n        image_type = value.get(\"type\")\n        value = value.get(\"value\")\n    allowed_payload_types = {e.value for e in ImageType}\n    if image_type is None:\n        return value, image_type\n    if image_type.lower() not in allowed_payload_types:\n        raise InvalidImageTypeDeclared(\n            message=f\"Declared image type: {image_type.lower()} which is not in allowed types: {allowed_payload_types}.\",\n            public_message=\"Image declaration contains not recognised image type.\",\n        )\n    return value, ImageType(image_type.lower())\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image","title":"<code>load_image(value, disable_preproc_auto_orient=False)</code>","text":"<p>Loads an image based on the specified type and value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Image value which could be an instance of InferenceRequestImage, a dict with 'type' and 'value' keys, or inferred based on the value's content.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Image.Image: The loaded PIL image, converted to RGB.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the specified image type is not supported.</p> <code>InvalidNumpyInput</code> <p>If the numpy input method is used and the input data is invalid.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image(\n    value: Any,\n    disable_preproc_auto_orient: bool = False,\n) -&gt; Tuple[np.ndarray, bool]:\n\"\"\"Loads an image based on the specified type and value.\n\n    Args:\n        value (Any): Image value which could be an instance of InferenceRequestImage,\n            a dict with 'type' and 'value' keys, or inferred based on the value's content.\n\n    Returns:\n        Image.Image: The loaded PIL image, converted to RGB.\n\n    Raises:\n        NotImplementedError: If the specified image type is not supported.\n        InvalidNumpyInput: If the numpy input method is used and the input data is invalid.\n    \"\"\"\n    cv_imread_flags = choose_image_decoding_flags(\n        disable_preproc_auto_orient=disable_preproc_auto_orient\n    )\n    value, image_type = extract_image_payload_and_type(value=value)\n    if image_type is not None:\n        np_image, is_bgr = load_image_with_known_type(\n            value=value,\n            image_type=image_type,\n            cv_imread_flags=cv_imread_flags,\n        )\n    else:\n        np_image, is_bgr = load_image_with_inferred_type(\n            value, cv_imread_flags=cv_imread_flags\n        )\n    np_image = convert_gray_image_to_bgr(image=np_image)\n    logger.debug(f\"Loaded inference image. Shape: {getattr(np_image, 'shape', None)}\")\n    return np_image, is_bgr\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_base64","title":"<code>load_image_base64(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a base64 encoded string using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Base64 encoded string representing the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The loaded image as a numpy array.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_base64(\n    value: Union[str, bytes], cv_imread_flags=cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n\"\"\"Loads an image from a base64 encoded string using OpenCV.\n\n    Args:\n        value (str): Base64 encoded string representing the image.\n\n    Returns:\n        np.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    # New routes accept images via json body (str), legacy routes accept bytes which need to be decoded as strings\n    if not isinstance(value, str):\n        value = value.decode(\"utf-8\")\n    value = BASE64_DATA_TYPE_PATTERN.sub(\"\", value)\n    value = pybase64.b64decode(value)\n    image_np = np.frombuffer(value, np.uint8)\n    result = cv2.imdecode(image_np, cv_imread_flags)\n    if result is None:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Malformed base64 input image.\",\n        )\n    return result\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_buffer","title":"<code>load_image_from_buffer(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a multipart-encoded input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Multipart-encoded input representing the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_buffer(\n    value: _IOBase,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; np.ndarray:\n\"\"\"Loads an image from a multipart-encoded input.\n\n    Args:\n        value (Any): Multipart-encoded input representing the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n    \"\"\"\n    value.seek(0)\n    image_np = np.frombuffer(value.read(), np.uint8)\n    result = cv2.imdecode(image_np, cv_imread_flags)\n    if result is None:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from buffer.\",\n            public_message=\"Could not decode bytes into image.\",\n        )\n    return result\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_encoded_bytes","title":"<code>load_image_from_encoded_bytes(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image from encoded bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The byte sequence representing the image.</p> required <code>cv_imread_flags</code> <code>int</code> <p>OpenCV flags used for image reading.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The loaded image as a numpy array.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_encoded_bytes(\n    value: bytes, cv_imread_flags: int = cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n\"\"\"\n    Load an image from encoded bytes.\n\n    Args:\n        value (bytes): The byte sequence representing the image.\n        cv_imread_flags (int): OpenCV flags used for image reading.\n\n    Returns:\n        np.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    image_np = np.asarray(bytearray(value), dtype=np.uint8)\n    image = cv2.imdecode(image_np, cv_imread_flags)\n    if image is None:\n        raise InputImageLoadError(\n            message=f\"Could not decode bytes as image.\",\n            public_message=\"Data is not image.\",\n        )\n    return image\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_numpy_str","title":"<code>load_image_from_numpy_str(value)</code>","text":"<p>Loads an image from a numpy array string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[bytes, str]</code> <p>Base64 string or byte sequence representing the pickled numpy array of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> <p>Raises:</p> Type Description <code>InvalidNumpyInput</code> <p>If the numpy data is invalid.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_numpy_str(value: Union[bytes, str]) -&gt; np.ndarray:\n\"\"\"Loads an image from a numpy array string.\n\n    Args:\n        value (Union[bytes, str]): Base64 string or byte sequence representing the pickled numpy array of the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n\n    Raises:\n        InvalidNumpyInput: If the numpy data is invalid.\n    \"\"\"\n    try:\n        if isinstance(value, str):\n            value = pybase64.b64decode(value)\n        data = pickle.loads(value)\n    except (EOFError, TypeError, pickle.UnpicklingError, binascii.Error) as error:\n        raise InvalidNumpyInput(\n            message=f\"Could not unpickle image data. Cause: {error}\",\n            public_message=\"Could not deserialize pickle payload.\",\n        ) from error\n    validate_numpy_image(data=data)\n    return data\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_url","title":"<code>load_image_from_url(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a given URL.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>URL of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_url(\n    value: str, cv_imread_flags: int = cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n\"\"\"Loads an image from a given URL.\n\n    Args:\n        value (str): URL of the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n    \"\"\"\n    try:\n        response = requests.get(value, stream=True)\n        api_key_safe_raise_for_status(response=response)\n        return load_image_from_encoded_bytes(\n            value=response.content, cv_imread_flags=cv_imread_flags\n        )\n    except (RequestException, ConnectionError) as error:\n        raise InputImageLoadError(\n            message=f\"Could not load image from url: {value}. Details: {error}\",\n            public_message=\"Data pointed by URL could not be decoded into image.\",\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_with_inferred_type","title":"<code>load_image_with_inferred_type(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image by inferring its type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The image data.</p> required <code>cv_imread_flags</code> <code>int</code> <p>Flags used for OpenCV's imread function.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: Loaded image as a numpy array and a boolean indicating if the image is in BGR format.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the image type could not be inferred.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_with_inferred_type(\n    value: Any,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n\"\"\"Load an image by inferring its type.\n\n    Args:\n        value (Any): The image data.\n        cv_imread_flags (int): Flags used for OpenCV's imread function.\n\n    Returns:\n        Tuple[np.ndarray, bool]: Loaded image as a numpy array and a boolean indicating if the image is in BGR format.\n\n    Raises:\n        NotImplementedError: If the image type could not be inferred.\n    \"\"\"\n    if isinstance(value, (np.ndarray, np.generic)):\n        validate_numpy_image(data=value)\n        return value, True\n    elif isinstance(value, Image.Image):\n        return np.asarray(value.convert(\"RGB\")), False\n    elif isinstance(value, str) and (value.startswith(\"http\")):\n        return load_image_from_url(value=value, cv_imread_flags=cv_imread_flags), True\n    elif isinstance(value, str) and os.path.isfile(value):\n        return cv2.imread(value, cv_imread_flags), True\n    else:\n        return attempt_loading_image_from_string(\n            value=value, cv_imread_flags=cv_imread_flags\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_with_known_type","title":"<code>load_image_with_known_type(value, image_type, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image using the known image type.</p> <p>Supports various image types (e.g., NUMPY, PILLOW, etc.) and loads them into a numpy array format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The image data.</p> required <code>image_type</code> <code>ImageType</code> <p>The type of the image.</p> required <code>cv_imread_flags</code> <code>int</code> <p>Flags used for OpenCV's imread function.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: A tuple of the loaded image as a numpy array and a boolean indicating if the image is in BGR format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_with_known_type(\n    value: Any,\n    image_type: ImageType,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n\"\"\"Load an image using the known image type.\n\n    Supports various image types (e.g., NUMPY, PILLOW, etc.) and loads them into a numpy array format.\n\n    Args:\n        value (Any): The image data.\n        image_type (ImageType): The type of the image.\n        cv_imread_flags (int): Flags used for OpenCV's imread function.\n\n    Returns:\n        Tuple[np.ndarray, bool]: A tuple of the loaded image as a numpy array and a boolean indicating if the image is in BGR format.\n    \"\"\"\n    if image_type is ImageType.NUMPY and not ALLOW_NUMPY_INPUT:\n        raise InvalidImageTypeDeclared(\n            message=f\"NumPy image type is not supported in this configuration of `inference`.\",\n            public_message=f\"NumPy image type is not supported in this configuration of `inference`.\",\n        )\n    loader = IMAGE_LOADERS[image_type]\n    is_bgr = True if image_type is not ImageType.PILLOW else False\n    image = loader(value, cv_imread_flags)\n    return image, is_bgr\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.np_image_to_base64","title":"<code>np_image_to_base64(image)</code>","text":"<p>Convert a numpy image to a base64 encoded byte string.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The base64 encoded image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def np_image_to_base64(image: np.ndarray) -&gt; bytes:\n\"\"\"\n    Convert a numpy image to a base64 encoded byte string.\n\n    Args:\n        image (np.ndarray): The numpy array representing an image.\n\n    Returns:\n        bytes: The base64 encoded image.\n    \"\"\"\n    image = Image.fromarray(image)\n    with BytesIO() as buffer:\n        image = image.convert(\"RGB\")\n        image.save(buffer, format=\"JPEG\")\n        buffer.seek(0)\n        return buffer.getvalue()\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.validate_numpy_image","title":"<code>validate_numpy_image(data)</code>","text":"<p>Validate if the provided data is a valid numpy image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <p>Raises:</p> Type Description <code>InvalidNumpyInput</code> <p>If the provided data is not a valid numpy image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def validate_numpy_image(data: np.ndarray) -&gt; None:\n\"\"\"\n    Validate if the provided data is a valid numpy image.\n\n    Args:\n        data (np.ndarray): The numpy array representing an image.\n\n    Raises:\n        InvalidNumpyInput: If the provided data is not a valid numpy image.\n    \"\"\"\n    if not issubclass(type(data), np.ndarray):\n        raise InvalidNumpyInput(\n            message=f\"Data provided as input could not be decoded into np.ndarray object.\",\n            public_message=f\"Data provided as input could not be decoded into np.ndarray object.\",\n        )\n    if len(data.shape) != 3 and len(data.shape) != 2:\n        raise InvalidNumpyInput(\n            message=f\"For image given as np.ndarray expected 2 or 3 dimensions, got {len(data.shape)} dimensions.\",\n            public_message=f\"For image given as np.ndarray expected 2 or 3 dimensions.\",\n        )\n    if data.shape[-1] != 3 and data.shape[-1] != 1:\n        raise InvalidNumpyInput(\n            message=f\"For image given as np.ndarray expected 1 or 3 channels, got {data.shape[-1]} channels.\",\n            public_message=\"For image given as np.ndarray expected 1 or 3 channels.\",\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.xyxy_to_xywh","title":"<code>xyxy_to_xywh(xyxy)</code>","text":"<p>Convert bounding box format from (xmin, ymin, xmax, ymax) to (xcenter, ycenter, width, height).</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>List[int]</code> <p>List containing the coordinates in (xmin, ymin, xmax, ymax) format.</p> required <p>Returns:</p> Type Description <p>List[int]: List containing the converted coordinates in (xcenter, ycenter, width, height) format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def xyxy_to_xywh(xyxy):\n\"\"\"\n    Convert bounding box format from (xmin, ymin, xmax, ymax) to (xcenter, ycenter, width, height).\n\n    Args:\n        xyxy (List[int]): List containing the coordinates in (xmin, ymin, xmax, ymax) format.\n\n    Returns:\n        List[int]: List containing the converted coordinates in (xcenter, ycenter, width, height) format.\n    \"\"\"\n    x_temp = (xyxy[0] + xyxy[2]) / 2\n    y_temp = (xyxy[1] + xyxy[3]) / 2\n    w_temp = abs(xyxy[0] - xyxy[2])\n    h_temp = abs(xyxy[1] - xyxy[3])\n\n    return [int(x_temp), int(y_temp), int(w_temp), int(h_temp)]\n</code></pre>"},{"location":"docs/reference/inference/core/utils/notebooks/","title":"notebooks","text":""},{"location":"docs/reference/inference/core/utils/onnx/","title":"onnx","text":""},{"location":"docs/reference/inference/core/utils/onnx/#inference.core.utils.onnx.get_onnxruntime_execution_providers","title":"<code>get_onnxruntime_execution_providers(value)</code>","text":"<p>Extracts the ONNX runtime execution providers from the given string.</p> <p>The input string is expected to be a comma-separated list, possibly enclosed within square brackets and containing single quotes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string containing the list of ONNX runtime execution providers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings representing each execution provider.</p> Source code in <code>inference/core/utils/onnx.py</code> <pre><code>def get_onnxruntime_execution_providers(value: str) -&gt; List[str]:\n\"\"\"Extracts the ONNX runtime execution providers from the given string.\n\n    The input string is expected to be a comma-separated list, possibly enclosed\n    within square brackets and containing single quotes.\n\n    Args:\n        value (str): The string containing the list of ONNX runtime execution providers.\n\n    Returns:\n        List[str]: A list of strings representing each execution provider.\n    \"\"\"\n    if len(value) == 0:\n        return []\n    value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\" \", \"\")\n    return value.split(\",\")\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/","title":"postprocess","text":""},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.cosine_similarity","title":"<code>cosine_similarity(a, b)</code>","text":"<p>Compute the cosine similarity between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>Vector A.</p> required <code>b</code> <code>ndarray</code> <p>Vector B.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>Union[number, ndarray]</code> <p>Cosine similarity between vectors A and B.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; Union[np.number, np.ndarray]:\n\"\"\"\n    Compute the cosine similarity between two vectors.\n\n    Args:\n        a (np.ndarray): Vector A.\n        b (np.ndarray): Vector B.\n\n    Returns:\n        float: Cosine similarity between vectors A and B.\n    \"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.crop_mask","title":"<code>crop_mask(masks, boxes)</code>","text":"<p>\"Crop\" predicted masks by zeroing out everything not in the predicted bbox. Vectorized by Chong (thanks Chong).</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def crop_mask(masks: np.ndarray, boxes: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    \"Crop\" predicted masks by zeroing out everything not in the predicted bbox.\n    Vectorized by Chong (thanks Chong).\n\n    Args:\n        - masks should be a size [h, w, n] tensor of masks\n        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n    \"\"\"\n\n    n, h, w = masks.shape\n    x1, y1, x2, y2 = np.split(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)\n    r = np.arange(w, dtype=x1.dtype)[None, None, :]  # rows shape(1,w,1)\n    c = np.arange(h, dtype=x1.dtype)[None, :, None]  # cols shape(h,1,1)\n\n    masks = masks * ((r &gt;= x1) * (r &lt; x2) * (c &gt;= y1) * (c &lt; y2))\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.get_static_crop_dimensions","title":"<code>get_static_crop_dimensions(orig_shape, preproc, disable_preproc_static_crop=False)</code>","text":"<p>Generates a transformation based on preprocessing configuration.</p> <p>Parameters:</p> Name Type Description Default <code>orig_shape</code> <code>tuple</code> <p>The original shape of the object (e.g., image) - (height, width).</p> required <code>preproc</code> <code>dict</code> <p>Preprocessing configuration dictionary, containing information such as static cropping.</p> required <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tuple[int, int], Tuple[int, int]]</code> <p>A tuple containing the shift in the x and y directions, and the updated original shape after cropping.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def get_static_crop_dimensions(\n    orig_shape: Tuple[int, int],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n\"\"\"\n    Generates a transformation based on preprocessing configuration.\n\n    Args:\n        orig_shape (tuple): The original shape of the object (e.g., image) - (height, width).\n        preproc (dict): Preprocessing configuration dictionary, containing information such as static cropping.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        tuple: A tuple containing the shift in the x and y directions, and the updated original shape after cropping.\n    \"\"\"\n    try:\n        if static_crop_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        ):\n            x_min, y_min, x_max, y_max = standardise_static_crop(\n                static_crop_config=preproc[STATIC_CROP_KEY]\n            )\n        else:\n            x_min, y_min, x_max, y_max = 0, 0, 1, 1\n        crop_shift_x, crop_shift_y = (\n            round(x_min * orig_shape[1]),\n            round(y_min * orig_shape[0]),\n        )\n        cropped_percent_x = x_max - x_min\n        cropped_percent_y = y_max - y_min\n        orig_shape = (\n            round(orig_shape[0] * cropped_percent_y),\n            round(orig_shape[1] * cropped_percent_x),\n        )\n        return (crop_shift_x, crop_shift_y), orig_shape\n    except KeyError as error:\n        raise PostProcessingError(\n            f\"Could not find a proper configuration key {error} in post-processing.\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.mask2poly","title":"<code>mask2poly(mask)</code>","text":"<p>Find contours in the mask and return them as a float32 array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contours represented as a float32 array.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def mask2poly(mask: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Find contours in the mask and return them as a float32 array.\n\n    Args:\n        mask (np.ndarray): A binary mask.\n\n    Returns:\n        np.ndarray: Contours represented as a float32 array.\n    \"\"\"\n    contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n    if contours:\n        contours = np.array(\n            contours[np.array([len(x) for x in contours]).argmax()]\n        ).reshape(-1, 2)\n    else:\n        contours = np.zeros((0, 2))\n    return contours.astype(\"float32\")\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.masks2poly","title":"<code>masks2poly(masks)</code>","text":"<p>Converts binary masks to polygonal segments.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[ndarray]</code> <p>A list of segments, where each segment is obtained by converting the corresponding mask.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def masks2poly(masks: np.ndarray) -&gt; List[np.ndarray]:\n\"\"\"Converts binary masks to polygonal segments.\n\n    Args:\n        masks (numpy.ndarray): A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.\n\n    Returns:\n        list: A list of segments, where each segment is obtained by converting the corresponding mask.\n    \"\"\"\n    segments = []\n    masks = (masks * 255.0).astype(np.uint8)\n    for mask in masks:\n        segments.append(mask2poly(mask))\n    return segments\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_bboxes","title":"<code>post_process_bboxes(predictions, infer_shape, img_dims, preproc, disable_preproc_static_crop=False, resize_method='Stretch to')</code>","text":"<p>Postprocesses each patch of detections by scaling them to the original image coordinates and by shifting them based on a static crop preproc (if applied).</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>The predictions output from NMS, indices are: batch x prediction x [x1, y1, x2, y2, ...].</p> required <code>infer_shape</code> <code>Tuple[int, int]</code> <p>The shape of the inference image.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>The dimensions of the original image for each batch, indices are: batch x [height, width].</p> required <code>preproc</code> <code>dict</code> <p>Preprocessing configuration dictionary.</p> required <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>resize_method</code> <code>str</code> <p>Resize method for image. Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <p>Returns:</p> Type Description <code>List[List[List[float]]]</code> <p>List[List[List[float]]]: The scaled and shifted predictions, indices are: batch x prediction x [x1, y1, x2, y2, ...].</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_bboxes(\n    predictions: List[List[List[float]]],\n    infer_shape: Tuple[int, int],\n    img_dims: List[Tuple[int, int]],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[List[float]]]:\n\"\"\"\n    Postprocesses each patch of detections by scaling them to the original image coordinates and by shifting them based on a static crop preproc (if applied).\n\n    Args:\n        predictions (List[List[List[float]]]): The predictions output from NMS, indices are: batch x prediction x [x1, y1, x2, y2, ...].\n        infer_shape (Tuple[int, int]): The shape of the inference image.\n        img_dims (List[Tuple[int, int]]): The dimensions of the original image for each batch, indices are: batch x [height, width].\n        preproc (dict): Preprocessing configuration dictionary.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        resize_method (str, optional): Resize method for image. Defaults to \"Stretch to\".\n\n    Returns:\n        List[List[List[float]]]: The scaled and shifted predictions, indices are: batch x prediction x [x1, y1, x2, y2, ...].\n    \"\"\"\n\n    # Get static crop params\n    scaled_predictions = []\n    # Loop through batches\n    for i, batch_predictions in enumerate(predictions):\n        if len(batch_predictions) == 0:\n            scaled_predictions.append([])\n            continue\n        np_batch_predictions = np.array(batch_predictions)\n        # Get bboxes from predictions (x1,y1,x2,y2)\n        predicted_bboxes = np_batch_predictions[:, :4]\n        (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n            img_dims[i],\n            preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if resize_method == \"Stretch to\":\n            predicted_bboxes = stretch_bboxes(\n                predicted_bboxes=predicted_bboxes,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        elif (\n            resize_method == \"Fit (black edges) in\"\n            or resize_method == \"Fit (white edges) in\"\n        ):\n            predicted_bboxes = undo_image_padding_for_predicted_boxes(\n                predicted_bboxes=predicted_bboxes,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        predicted_bboxes = clip_boxes_coordinates(\n            predicted_bboxes=predicted_bboxes,\n            origin_shape=origin_shape,\n        )\n        predicted_bboxes = shift_bboxes(\n            bboxes=predicted_bboxes,\n            shift_x=crop_shift_x,\n            shift_y=crop_shift_y,\n        )\n        np_batch_predictions[:, :4] = predicted_bboxes\n        scaled_predictions.append(np_batch_predictions.tolist())\n    return scaled_predictions\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_keypoints","title":"<code>post_process_keypoints(predictions, keypoints_start_index, infer_shape, img_dims, preproc, disable_preproc_static_crop=False, resize_method='Stretch to')</code>","text":"<p>Scales and shifts keypoints based on the given image shapes and preprocessing method.</p> <p>This function performs polygon scaling and shifting based on the specified resizing method and pre-processing steps. The polygons are transformed according to the ratio and padding between two images.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>predictions from model</p> required <code>keypoints_start_index</code> <code>int</code> <p>offset in the 3rd dimension pointing where in the prediction start keypoints [(x, y, cfg), ...] for each keypoint class</p> required <code>img_dims</code> <code>list of (tuple of int</code> <p>Shape of the source image (height, width).</p> required <code>infer_shape</code> <code>tuple of int</code> <p>Shape of the target image (height, width).</p> required <code>preproc</code> <code>object</code> <p>Preprocessing details used for generating the transformation.</p> required <code>resize_method</code> <code>str</code> <p>Resizing method, either \"Stretch to\", \"Fit (black edges) in\", or \"Fit (white edges) in\". Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>flag to disable static crop</p> <code>False</code> <p>Returns:     list of list of list: predictions with post-processed keypoints</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_keypoints(\n    predictions: List[List[List[float]]],\n    keypoints_start_index: int,\n    infer_shape: Tuple[int, int],\n    img_dims: List[Tuple[int, int]],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[List[float]]]:\n\"\"\"Scales and shifts keypoints based on the given image shapes and preprocessing method.\n\n    This function performs polygon scaling and shifting based on the specified resizing method and\n    pre-processing steps. The polygons are transformed according to the ratio and padding between two images.\n\n    Args:\n        predictions: predictions from model\n        keypoints_start_index: offset in the 3rd dimension pointing where in the prediction start keypoints [(x, y, cfg), ...] for each keypoint class\n        img_dims list of (tuple of int): Shape of the source image (height, width).\n        infer_shape (tuple of int): Shape of the target image (height, width).\n        preproc (object): Preprocessing details used for generating the transformation.\n        resize_method (str, optional): Resizing method, either \"Stretch to\", \"Fit (black edges) in\", or \"Fit (white edges) in\". Defaults to \"Stretch to\".\n        disable_preproc_static_crop: flag to disable static crop\n    Returns:\n        list of list of list: predictions with post-processed keypoints\n    \"\"\"\n    # Get static crop params\n    scaled_predictions = []\n    # Loop through batches\n    for i, batch_predictions in enumerate(predictions):\n        if len(batch_predictions) == 0:\n            scaled_predictions.append([])\n            continue\n        np_batch_predictions = np.array(batch_predictions)\n        keypoints = np_batch_predictions[:, keypoints_start_index:]\n        (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n            img_dims[i],\n            preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if resize_method == \"Stretch to\":\n            keypoints = stretch_keypoints(\n                keypoints=keypoints,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        elif (\n            resize_method == \"Fit (black edges) in\"\n            or resize_method == \"Fit (white edges) in\"\n        ):\n            keypoints = undo_image_padding_for_predicted_keypoints(\n                keypoints=keypoints,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        keypoints = clip_keypoints_coordinates(\n            keypoints=keypoints, origin_shape=origin_shape\n        )\n        keypoints = shift_keypoints(\n            keypoints=keypoints, shift_x=crop_shift_x, shift_y=crop_shift_y\n        )\n        np_batch_predictions[:, keypoints_start_index:] = keypoints\n        scaled_predictions.append(np_batch_predictions.tolist())\n    return scaled_predictions\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_polygons","title":"<code>post_process_polygons(origin_shape, polys, infer_shape, preproc, resize_method='Stretch to')</code>","text":"<p>Scales and shifts polygons based on the given image shapes and preprocessing method.</p> <p>This function performs polygon scaling and shifting based on the specified resizing method and pre-processing steps. The polygons are transformed according to the ratio and padding between two images.</p> <p>Parameters:</p> Name Type Description Default <code>origin_shape</code> <code>tuple of int</code> <p>Shape of the source image (height, width).</p> required <code>infer_shape</code> <code>tuple of int</code> <p>Shape of the target image (height, width).</p> required <code>polys</code> <code>list of list of tuple</code> <p>List of polygons, where each polygon is represented by a list of (x, y) coordinates.</p> required <code>preproc</code> <code>object</code> <p>Preprocessing details used for generating the transformation.</p> required <code>resize_method</code> <code>str</code> <p>Resizing method, either \"Stretch to\", \"Fit (black edges) in\", or \"Fit (white edges) in\". Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <p>Returns:</p> Type Description <code>List[List[Tuple[float, float]]]</code> <p>list of list of tuple: A list of shifted and scaled polygons.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_polygons(\n    origin_shape: Tuple[int, int],\n    polys: List[List[Tuple[float, float]]],\n    infer_shape: Tuple[int, int],\n    preproc: dict,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[Tuple[float, float]]]:\n\"\"\"Scales and shifts polygons based on the given image shapes and preprocessing method.\n\n    This function performs polygon scaling and shifting based on the specified resizing method and\n    pre-processing steps. The polygons are transformed according to the ratio and padding between two images.\n\n    Args:\n        origin_shape (tuple of int): Shape of the source image (height, width).\n        infer_shape (tuple of int): Shape of the target image (height, width).\n        polys (list of list of tuple): List of polygons, where each polygon is represented by a list of (x, y) coordinates.\n        preproc (object): Preprocessing details used for generating the transformation.\n        resize_method (str, optional): Resizing method, either \"Stretch to\", \"Fit (black edges) in\", or \"Fit (white edges) in\". Defaults to \"Stretch to\".\n\n    Returns:\n        list of list of tuple: A list of shifted and scaled polygons.\n    \"\"\"\n    (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n        origin_shape, preproc\n    )\n    new_polys = []\n    if resize_method == \"Stretch to\":\n        width_ratio = origin_shape[1] / infer_shape[1]\n        height_ratio = origin_shape[0] / infer_shape[0]\n        new_polys = scale_polygons(\n            polygons=polys,\n            x_scale=width_ratio,\n            y_scale=height_ratio,\n        )\n    elif resize_method in {\"Fit (black edges) in\", \"Fit (white edges) in\"}:\n        new_polys = undo_image_padding_for_predicted_polygons(\n            polygons=polys,\n            infer_shape=infer_shape,\n            origin_shape=origin_shape,\n        )\n    shifted_polys = []\n    for poly in new_polys:\n        poly = [(p[0] + crop_shift_x, p[1] + crop_shift_y) for p in poly]\n        shifted_polys.append(poly)\n    return shifted_polys\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_accurate","title":"<code>process_mask_accurate(protos, masks_in, bboxes, shape)</code>","text":"<p>Returns masks that are the size of the original image.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_accurate(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n) -&gt; np.ndarray:\n\"\"\"Returns masks that are the size of the original image.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n\n    # Order = 1 -&gt; bilinear\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=0)\n    masks = masks.transpose((1, 2, 0))\n    masks = cv2.resize(masks, (shape[1], shape[0]), cv2.INTER_LINEAR)\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=2)\n    masks = masks.transpose((2, 0, 1))\n    masks = crop_mask(masks, bboxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_fast","title":"<code>process_mask_fast(protos, masks_in, bboxes, shape)</code>","text":"<p>Returns masks in their original size.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_fast(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n) -&gt; np.ndarray:\n\"\"\"Returns masks in their original size.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    ih, iw = shape\n    c, mh, mw = protos.shape  # CHW\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n    down_sampled_boxes = scale_bboxes(\n        bboxes=deepcopy(bboxes),\n        scale_x=mw / iw,\n        scale_y=mh / ih,\n    )\n    masks = crop_mask(masks, down_sampled_boxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_tradeoff","title":"<code>process_mask_tradeoff(protos, masks_in, bboxes, shape, tradeoff_factor)</code>","text":"<p>Returns masks that are the size of the original image with a tradeoff factor applied.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <code>tradeoff_factor</code> <code>float</code> <p>Tradeoff factor for resizing masks.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_tradeoff(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n    tradeoff_factor: float,\n) -&gt; np.ndarray:\n\"\"\"Returns masks that are the size of the original image with a tradeoff factor applied.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n        tradeoff_factor (float): Tradeoff factor for resizing masks.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    c, mh, mw = protos.shape  # CHW\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n\n    # Order = 1 -&gt; bilinear\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=0)\n    masks = masks.transpose((1, 2, 0))\n    ih, iw = shape\n    h = int(mh * (1 - tradeoff_factor) + ih * tradeoff_factor)\n    w = int(mw * (1 - tradeoff_factor) + iw * tradeoff_factor)\n    size = (h, w)\n    if tradeoff_factor != 0:\n        masks = cv2.resize(masks, size, cv2.INTER_LINEAR)\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=2)\n    masks = masks.transpose((2, 0, 1))\n    c, mh, mw = masks.shape\n    down_sampled_boxes = scale_bboxes(\n        bboxes=deepcopy(bboxes),\n        scale_x=mw / iw,\n        scale_y=mh / ih,\n    )\n    masks = crop_mask(masks, down_sampled_boxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.sigmoid","title":"<code>sigmoid(x)</code>","text":"<p>Computes the sigmoid function for the given input.</p> <p>The sigmoid function is defined as: f(x) = 1 / (1 + exp(-x))</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float or ndarray</code> <p>Input value or array for which the sigmoid function is to be computed.</p> required <p>Returns:</p> Type Description <code>Union[float, number, ndarray]</code> <p>float or numpy.ndarray: The computed sigmoid value(s).</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def sigmoid(x: Union[float, np.ndarray]) -&gt; Union[float, np.number, np.ndarray]:\n\"\"\"Computes the sigmoid function for the given input.\n\n    The sigmoid function is defined as:\n    f(x) = 1 / (1 + exp(-x))\n\n    Args:\n        x (float or numpy.ndarray): Input value or array for which the sigmoid function is to be computed.\n\n    Returns:\n        float or numpy.ndarray: The computed sigmoid value(s).\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n</code></pre>"},{"location":"docs/reference/inference/core/utils/preprocess/","title":"preprocess","text":""},{"location":"docs/reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.letterbox_image","title":"<code>letterbox_image(image, desired_size, color=(0, 0, 0))</code>","text":"<p>Resize and pad image to fit the desired size, preserving its aspect ratio.</p> <p>Parameters: - image: numpy array representing the image. - desired_size: tuple (width, height) representing the target dimensions. - color: tuple (B, G, R) representing the color to pad with.</p> <p>Returns: - letterboxed image.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def letterbox_image(\n    image: np.ndarray,\n    desired_size: Tuple[int, int],\n    color: Tuple[int, int, int] = (0, 0, 0),\n) -&gt; np.ndarray:\n\"\"\"\n    Resize and pad image to fit the desired size, preserving its aspect ratio.\n\n    Parameters:\n    - image: numpy array representing the image.\n    - desired_size: tuple (width, height) representing the target dimensions.\n    - color: tuple (B, G, R) representing the color to pad with.\n\n    Returns:\n    - letterboxed image.\n    \"\"\"\n    resized_img = resize_image_keeping_aspect_ratio(\n        image=image,\n        desired_size=desired_size,\n    )\n    new_height, new_width = resized_img.shape[:2]\n    top_padding = (desired_size[1] - new_height) // 2\n    bottom_padding = desired_size[1] - new_height - top_padding\n    left_padding = (desired_size[0] - new_width) // 2\n    right_padding = desired_size[0] - new_width - left_padding\n    return cv2.copyMakeBorder(\n        resized_img,\n        top_padding,\n        bottom_padding,\n        left_padding,\n        right_padding,\n        cv2.BORDER_CONSTANT,\n        value=color,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.prepare","title":"<code>prepare(image, preproc, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Prepares an image by applying a series of preprocessing steps defined in the <code>preproc</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input PIL image object.</p> required <code>preproc</code> <code>dict</code> <p>Dictionary containing preprocessing steps. Example: {     \"resize\": {\"enabled\": true, \"width\": 416, \"height\": 416, \"format\": \"Stretch to\"},     \"static-crop\": {\"y_min\": 25, \"x_max\": 75, \"y_max\": 75, \"enabled\": true, \"x_min\": 25},     \"auto-orient\": {\"enabled\": true},     \"grayscale\": {\"enabled\": true},     \"contrast\": {\"enabled\": true, \"type\": \"Adaptive Equalization\"} }</p> required <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>PIL.Image.Image: The preprocessed image object.</p> <code>tuple</code> <code>Tuple[int, int]</code> <p>The dimensions of the image.</p> Note <p>The function uses global flags like <code>DISABLE_PREPROC_AUTO_ORIENT</code>, <code>DISABLE_PREPROC_STATIC_CROP</code>, etc. to conditionally enable or disable certain preprocessing steps.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def prepare(\n    image: np.ndarray,\n    preproc,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n\"\"\"\n    Prepares an image by applying a series of preprocessing steps defined in the `preproc` dictionary.\n\n    Args:\n        image (PIL.Image.Image): The input PIL image object.\n        preproc (dict): Dictionary containing preprocessing steps. Example:\n            {\n                \"resize\": {\"enabled\": true, \"width\": 416, \"height\": 416, \"format\": \"Stretch to\"},\n                \"static-crop\": {\"y_min\": 25, \"x_max\": 75, \"y_max\": 75, \"enabled\": true, \"x_min\": 25},\n                \"auto-orient\": {\"enabled\": true},\n                \"grayscale\": {\"enabled\": true},\n                \"contrast\": {\"enabled\": true, \"type\": \"Adaptive Equalization\"}\n            }\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        PIL.Image.Image: The preprocessed image object.\n        tuple: The dimensions of the image.\n\n    Note:\n        The function uses global flags like `DISABLE_PREPROC_AUTO_ORIENT`, `DISABLE_PREPROC_STATIC_CROP`, etc.\n        to conditionally enable or disable certain preprocessing steps.\n    \"\"\"\n    try:\n        h, w = image.shape[0:2]\n        img_dims = (h, w)\n        if static_crop_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        ):\n            image = take_static_crop(\n                image=image, crop_parameters=preproc[STATIC_CROP_KEY]\n            )\n        if contrast_adjustments_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_contrast=disable_preproc_contrast,\n        ):\n            adjustment_type = ContrastAdjustmentType(preproc[CONTRAST_KEY][TYPE_KEY])\n            image = apply_contrast_adjustment(\n                image=image, adjustment_type=adjustment_type\n            )\n        if grayscale_conversion_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n        ):\n            image = apply_grayscale_conversion(image=image)\n        return image, img_dims\n    except KeyError as error:\n        raise PreProcessingError(\n            f\"Pre-processing of image failed due to misconfiguration. Missing key: {error}.\"\n        ) from error\n</code></pre>"},{"location":"docs/reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.resize_image_keeping_aspect_ratio","title":"<code>resize_image_keeping_aspect_ratio(image, desired_size)</code>","text":"<p>Resize reserving its aspect ratio.</p> <p>Parameters: - image: numpy array representing the image. - desired_size: tuple (width, height) representing the target dimensions.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def resize_image_keeping_aspect_ratio(\n    image: np.ndarray,\n    desired_size: Tuple[int, int],\n) -&gt; np.ndarray:\n\"\"\"\n    Resize reserving its aspect ratio.\n\n    Parameters:\n    - image: numpy array representing the image.\n    - desired_size: tuple (width, height) representing the target dimensions.\n    \"\"\"\n    img_ratio = image.shape[1] / image.shape[0]\n    desired_ratio = desired_size[0] / desired_size[1]\n\n    # Determine the new dimensions\n    if img_ratio &gt;= desired_ratio:\n        # Resize by width\n        new_width = desired_size[0]\n        new_height = int(desired_size[0] / img_ratio)\n    else:\n        # Resize by height\n        new_height = desired_size[1]\n        new_width = int(desired_size[1] * img_ratio)\n\n    # Resize the image to new dimensions\n    return cv2.resize(image, (new_width, new_height))\n</code></pre>"},{"location":"docs/reference/inference/core/utils/requests/","title":"requests","text":""},{"location":"docs/reference/inference/core/utils/roboflow/","title":"roboflow","text":""},{"location":"docs/reference/inference/core/utils/s3/","title":"s3","text":""},{"location":"docs/reference/inference/core/utils/url_utils/","title":"url_utils","text":""},{"location":"docs/reference/inference/core/utils/visualisation/","title":"visualisation","text":""},{"location":"docs/reference/inference/enterprise/device_manager/container_service/","title":"container_service","text":""},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.InferServerContainer","title":"<code>InferServerContainer</code>  <code>dataclass</code>","text":"Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>@dataclass\nclass InferServerContainer:\n    status: str\n    id: str\n    port: int\n    host: str\n    startup_time: float\n    version: str\n\n    def __init__(self, docker_container, details):\n        self.container = docker_container\n        self.status = details.get(\"status\")\n        self.id = details.get(\"uuid\")\n        self.port = details.get(\"port\")\n        self.host = details.get(\"host\")\n        self.version = details.get(\"version\")\n        t = details.get(\"startup_time_ts\").split(\".\")[0]\n        self.startup_time = (\n            datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S\").timestamp()\n            if t is not None\n            else datetime.now().timestamp()\n        )\n\n    def kill(self):\n        try:\n            self.container.kill()\n            return True, None\n        except Exception as e:\n            logger.error(e)\n            return False, None\n\n    def restart(self):\n        try:\n            self.container.restart()\n            return True, None\n        except Exception as e:\n            logger.error(e)\n            return False, None\n\n    def stop(self):\n        try:\n            self.container.stop()\n            return True, None\n        except Exception as e:\n            logger.error(e)\n            return False, None\n\n    def start(self):\n        try:\n            self.container.start()\n            return True, None\n        except Exception as e:\n            logger.error(e)\n            return False, None\n\n    def inspect(self):\n        try:\n            info = requests.get(f\"http://{self.host}:{self.port}/info\").json()\n            return True, info\n        except Exception as e:\n            logger.error(e)\n            return False, None\n\n    def snapshot(self):\n        try:\n            snapshot = self.get_latest_inferred_images()\n            snapshot.update({\"container_id\": self.id})\n            return True, snapshot\n        except Exception as e:\n            logger.error(e)\n            return False, None\n\n    def get_latest_inferred_images(self, max=4):\n\"\"\"\n        Retrieve the latest inferred images and associated information for this container.\n\n        This method fetches the most recent inferred images within the time interval defined by METRICS_INTERVAL.\n\n        Args:\n            max (int, optional): The maximum number of inferred images to retrieve.\n                Defaults to 4.\n\n        Returns:\n            dict: A dictionary where each key represents a model ID associated with this\n            container, and the corresponding value is a list of dictionaries containing\n            information about the latest inferred images. Each dictionary has the following keys:\n            - \"image\" (str): The base64-encoded image data.\n            - \"dimensions\" (dict): Image dimensions (width and height).\n            - \"predictions\" (list): A list of predictions or results associated with the image.\n\n        Notes:\n            - This method uses the global constant METRICS_INTERVAL to specify the time interval.\n        \"\"\"\n\n        now = time.time()\n        start = now - METRICS_INTERVAL\n        api_keys = get_cache_model_items().get(self.id, dict()).keys()\n        model_ids = []\n        for api_key in api_keys:\n            mids = get_cache_model_items().get(self.id, dict()).get(api_key, [])\n            model_ids.extend(mids)\n        num_images = 0\n        latest_inferred_images = dict()\n        for model_id in model_ids:\n            if num_images &gt;= max:\n                break\n            latest_reqs = cache.zrangebyscore(\n                f\"inference:{self.id}:{model_id}\", min=start, max=now\n            )\n            for req in latest_reqs:\n                images = req[\"request\"][\"image\"]\n                image_dims = req.get(\"response\", {}).get(\"image\", dict())\n                predictions = req.get(\"response\", {}).get(\"predictions\", [])\n                if images is None or len(images) == 0:\n                    continue\n                if type(images) is not list:\n                    images = [images]\n                for image in images:\n                    value = None\n                    if image[\"type\"] == \"base64\":\n                        value = image[\"value\"]\n                    else:\n                        loaded_image = load_image_rgb(image)\n                        image_bytes = loaded_image.tobytes()\n                        image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n                        value = image_base64\n                    if latest_inferred_images.get(model_id) is None:\n                        latest_inferred_images[model_id] = []\n                    inference = dict(\n                        image=value, dimensions=image_dims, predictions=predictions\n                    )\n                    latest_inferred_images[model_id].append(inference)\n                    num_images += 1\n        return latest_inferred_images\n\n    def get_startup_config(self):\n\"\"\"\n        Get the startup configuration for this container.\n\n        Returns:\n            dict: A dictionary containing the startup configuration for this container.\n        \"\"\"\n        env_vars = self.container.attrs.get(\"Config\", {}).get(\"Env\", {})\n        port_bindings = self.container.attrs.get(\"HostConfig\", {}).get(\n            \"PortBindings\", {}\n        )\n        detached = self.container.attrs.get(\"HostConfig\", {}).get(\"Detached\", False)\n        image = self.container.attrs.get(\"Config\", {}).get(\"Image\", \"\")\n        privileged = self.container.attrs.get(\"HostConfig\", {}).get(\"Privileged\", False)\n        labels = self.container.attrs.get(\"Config\", {}).get(\"Labels\", {})\n        env = []\n        for var in env_vars:\n            name, value = var.split(\"=\")\n            env.append(f\"{name}={value}\")\n        return {\n            \"env\": env,\n            \"port_bindings\": port_bindings,\n            \"detach\": detached,\n            \"image\": image,\n            \"privileged\": privileged,\n            \"labels\": labels,\n            # TODO: add device requests\n        }\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.InferServerContainer.get_latest_inferred_images","title":"<code>get_latest_inferred_images(max=4)</code>","text":"<p>Retrieve the latest inferred images and associated information for this container.</p> <p>This method fetches the most recent inferred images within the time interval defined by METRICS_INTERVAL.</p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>int</code> <p>The maximum number of inferred images to retrieve. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key represents a model ID associated with this</p> <p>container, and the corresponding value is a list of dictionaries containing</p> <p>information about the latest inferred images. Each dictionary has the following keys:</p> <ul> <li>\"image\" (str): The base64-encoded image data.</li> </ul> <ul> <li>\"dimensions\" (dict): Image dimensions (width and height).</li> </ul> <ul> <li>\"predictions\" (list): A list of predictions or results associated with the image.</li> </ul> Notes <ul> <li>This method uses the global constant METRICS_INTERVAL to specify the time interval.</li> </ul> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def get_latest_inferred_images(self, max=4):\n\"\"\"\n    Retrieve the latest inferred images and associated information for this container.\n\n    This method fetches the most recent inferred images within the time interval defined by METRICS_INTERVAL.\n\n    Args:\n        max (int, optional): The maximum number of inferred images to retrieve.\n            Defaults to 4.\n\n    Returns:\n        dict: A dictionary where each key represents a model ID associated with this\n        container, and the corresponding value is a list of dictionaries containing\n        information about the latest inferred images. Each dictionary has the following keys:\n        - \"image\" (str): The base64-encoded image data.\n        - \"dimensions\" (dict): Image dimensions (width and height).\n        - \"predictions\" (list): A list of predictions or results associated with the image.\n\n    Notes:\n        - This method uses the global constant METRICS_INTERVAL to specify the time interval.\n    \"\"\"\n\n    now = time.time()\n    start = now - METRICS_INTERVAL\n    api_keys = get_cache_model_items().get(self.id, dict()).keys()\n    model_ids = []\n    for api_key in api_keys:\n        mids = get_cache_model_items().get(self.id, dict()).get(api_key, [])\n        model_ids.extend(mids)\n    num_images = 0\n    latest_inferred_images = dict()\n    for model_id in model_ids:\n        if num_images &gt;= max:\n            break\n        latest_reqs = cache.zrangebyscore(\n            f\"inference:{self.id}:{model_id}\", min=start, max=now\n        )\n        for req in latest_reqs:\n            images = req[\"request\"][\"image\"]\n            image_dims = req.get(\"response\", {}).get(\"image\", dict())\n            predictions = req.get(\"response\", {}).get(\"predictions\", [])\n            if images is None or len(images) == 0:\n                continue\n            if type(images) is not list:\n                images = [images]\n            for image in images:\n                value = None\n                if image[\"type\"] == \"base64\":\n                    value = image[\"value\"]\n                else:\n                    loaded_image = load_image_rgb(image)\n                    image_bytes = loaded_image.tobytes()\n                    image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n                    value = image_base64\n                if latest_inferred_images.get(model_id) is None:\n                    latest_inferred_images[model_id] = []\n                inference = dict(\n                    image=value, dimensions=image_dims, predictions=predictions\n                )\n                latest_inferred_images[model_id].append(inference)\n                num_images += 1\n    return latest_inferred_images\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.InferServerContainer.get_startup_config","title":"<code>get_startup_config()</code>","text":"<p>Get the startup configuration for this container.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the startup configuration for this container.</p> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def get_startup_config(self):\n\"\"\"\n    Get the startup configuration for this container.\n\n    Returns:\n        dict: A dictionary containing the startup configuration for this container.\n    \"\"\"\n    env_vars = self.container.attrs.get(\"Config\", {}).get(\"Env\", {})\n    port_bindings = self.container.attrs.get(\"HostConfig\", {}).get(\n        \"PortBindings\", {}\n    )\n    detached = self.container.attrs.get(\"HostConfig\", {}).get(\"Detached\", False)\n    image = self.container.attrs.get(\"Config\", {}).get(\"Image\", \"\")\n    privileged = self.container.attrs.get(\"HostConfig\", {}).get(\"Privileged\", False)\n    labels = self.container.attrs.get(\"Config\", {}).get(\"Labels\", {})\n    env = []\n    for var in env_vars:\n        name, value = var.split(\"=\")\n        env.append(f\"{name}={value}\")\n    return {\n        \"env\": env,\n        \"port_bindings\": port_bindings,\n        \"detach\": detached,\n        \"image\": image,\n        \"privileged\": privileged,\n        \"labels\": labels,\n        # TODO: add device requests\n    }\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.get_container_by_id","title":"<code>get_container_by_id(id)</code>","text":"<p>Gets an inference server container by its id</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>string</code> <p>The id of the container</p> required <p>Returns:</p> Name Type Description <code>container</code> <p>The container object if found, None otherwise</p> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def get_container_by_id(id):\n\"\"\"\n    Gets an inference server container by its id\n\n    Args:\n        id (string): The id of the container\n\n    Returns:\n        container: The container object if found, None otherwise\n    \"\"\"\n    containers = get_inference_containers()\n    for c in containers:\n        if c.id == id:\n            return c\n    return None\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.get_container_ids","title":"<code>get_container_ids()</code>","text":"<p>Gets the ids of the inference server containers</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of container ids</p> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def get_container_ids():\n\"\"\"\n    Gets the ids of the inference server containers\n\n    Returns:\n        list: A list of container ids\n    \"\"\"\n    containers = get_inference_containers()\n    return [c.id for c in containers]\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.get_inference_containers","title":"<code>get_inference_containers()</code>","text":"<p>Discovers inference server containers running on the host and parses their information into a list of InferServerContainer objects</p> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def get_inference_containers():\n\"\"\"\n    Discovers inference server containers running on the host\n    and parses their information into a list of InferServerContainer objects\n    \"\"\"\n    client = docker.from_env()\n    containers = client.containers.list()\n    inference_containers = []\n    for c in containers:\n        if is_inference_server_container(c):\n            details = parse_container_info(c)\n            info = {}\n            try:\n                info = requests.get(\n                    f\"http://{details['host']}:{details['port']}/info\", timeout=3\n                ).json()\n            except Exception as e:\n                logger.error(f\"Failed to get info from container {c.id} {details} {e}\")\n            details.update(info)\n            infer_container = InferServerContainer(c, details)\n            if len(inference_containers) == 0:\n                inference_containers.append(infer_container)\n                continue\n            for ic in inference_containers:\n                if ic.id == infer_container.id:\n                    continue\n                inference_containers.append(infer_container)\n    return inference_containers\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.is_inference_server_container","title":"<code>is_inference_server_container(container)</code>","text":"<p>Checks if a container is an inference server container</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>any</code> <p>A container object from the Docker SDK</p> required <p>Returns:</p> Name Type Description <code>boolean</code> <p>True if the container is an inference server container, False otherwise</p> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def is_inference_server_container(container):\n\"\"\"\n    Checks if a container is an inference server container\n\n    Args:\n        container (any): A container object from the Docker SDK\n\n    Returns:\n        boolean: True if the container is an inference server container, False otherwise\n    \"\"\"\n    image_tags = container.image.tags\n    for t in image_tags:\n        if t.startswith(\"roboflow/roboflow-inference-server\"):\n            return True\n    return False\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/container_service/#inference.enterprise.device_manager.container_service.parse_container_info","title":"<code>parse_container_info(c)</code>","text":"<p>Parses the container information into a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>any</code> <p>Docker SDK Container object</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the container information</p> Source code in <code>inference/enterprise/device_manager/container_service.py</code> <pre><code>def parse_container_info(c):\n\"\"\"\n    Parses the container information into a dictionary\n\n    Args:\n        c (any): Docker SDK Container object\n\n    Returns:\n        dict: A dictionary containing the container information\n    \"\"\"\n    env = c.attrs.get(\"Config\", {}).get(\"Env\", {})\n    info = {\"container_id\": c.id, \"port\": 9001, \"host\": \"0.0.0.0\"}\n    for var in env:\n        if var.startswith(\"PORT=\"):\n            info[\"port\"] = var.split(\"=\")[1]\n        elif var.startswith(\"HOST=\"):\n            info[\"host\"] = var.split(\"=\")[1]\n    status = c.attrs.get(\"State\", {}).get(\"Status\")\n    if status:\n        info[\"status\"] = status\n    container_name = c.attrs.get(\"Name\")\n    if container_name:\n        info[\"container_name_on_host\"] = container_name\n    startup_time = c.attrs.get(\"State\", {}).get(\"StartedAt\")\n    if startup_time:\n        info[\"startup_time_ts\"] = startup_time\n    return info\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/device_manager/","title":"device_manager","text":""},{"location":"docs/reference/inference/enterprise/device_manager/helpers/","title":"helpers","text":""},{"location":"docs/reference/inference/enterprise/device_manager/helpers/#inference.enterprise.device_manager.helpers.get_cache_model_items","title":"<code>get_cache_model_items()</code>","text":"<p>Retrieve and organize cached model items within a specified time interval.</p> <p>This method queries a cache for model items and retrieves those that fall within the time interval defined by the global constant METRICS_INTERVAL. It organizes the retrieved items into a hierarchical dictionary structure for efficient access.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing model items organized by server ID, API key,</p> <p>and model ID. The structure is as follows:</p> <ul> <li>Keys: Server IDs associated with models.</li> <li>Sub-keys: API keys associated with models on the server.</li> <li>Values: Lists of model IDs associated with each API key on the server.</li> </ul> Notes <ul> <li>This method relies on a cache system for storing and retrieving model items.</li> <li>It uses the global constant METRICS_INTERVAL to specify the time interval.</li> </ul> Source code in <code>inference/enterprise/device_manager/helpers.py</code> <pre><code>def get_cache_model_items():\n\"\"\"\n    Retrieve and organize cached model items within a specified time interval.\n\n    This method queries a cache for model items and retrieves those that fall\n    within the time interval defined by the global constant METRICS_INTERVAL.\n    It organizes the retrieved items into a hierarchical dictionary structure\n    for efficient access.\n\n    Returns:\n        dict: A dictionary containing model items organized by server ID, API key,\n        and model ID. The structure is as follows:\n        - Keys: Server IDs associated with models.\n          - Sub-keys: API keys associated with models on the server.\n            - Values: Lists of model IDs associated with each API key on the server.\n\n    Notes:\n        - This method relies on a cache system for storing and retrieving model items.\n        - It uses the global constant METRICS_INTERVAL to specify the time interval.\n    \"\"\"\n    now = time.time()\n    start = now - METRICS_INTERVAL\n    models = cache.zrangebyscore(\"models\", min=start, max=now)\n    model_items = dict()\n    for model in models:\n        server_id, api_key, model_id = model.split(\":\")\n        if server_id not in model_items:\n            model_items[server_id] = dict()\n        if api_key not in model_items[server_id]:\n            model_items[server_id][api_key] = []\n        model_items[server_id][api_key].append(model_id)\n    return model_items\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/metrics_service/","title":"metrics_service","text":""},{"location":"docs/reference/inference/enterprise/device_manager/metrics_service/#inference.enterprise.device_manager.metrics_service.aggregate_device_stats","title":"<code>aggregate_device_stats()</code>","text":"<p>Aggregate statistics for the device.</p> Source code in <code>inference/enterprise/device_manager/metrics_service.py</code> <pre><code>def aggregate_device_stats():\n\"\"\"\n    Aggregate statistics for the device.\n    \"\"\"\n    window_start_timestamp = str(int(time.time()))\n    all_data = {\n        \"api_key\": API_KEY,\n        \"timestamp\": window_start_timestamp,\n        \"device\": {\n            \"id\": GLOBAL_DEVICE_ID,\n            \"name\": GLOBAL_DEVICE_ID,\n            \"type\": f\"roboflow-device-manager=={__version__}\",\n            \"tags\": TAGS,\n            \"system_info\": get_system_info(),\n            \"containers\": build_container_stats(),\n        },\n    }\n    return all_data\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/metrics_service/#inference.enterprise.device_manager.metrics_service.aggregate_model_stats","title":"<code>aggregate_model_stats(container_id)</code>","text":"<p>Aggregate statistics for models within a specified container.</p> <p>This function retrieves and aggregates performance metrics for all models associated with the given container within a specified time interval.</p> <p>Parameters:</p> Name Type Description Default <code>container_id</code> <code>str</code> <p>The unique identifier of the container for which model statistics are to be aggregated.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, where each dictionary represents a model's</p> <p>statistics with the following keys:</p> <ul> <li>\"dataset_id\" (str): The ID of the dataset associated with the model.</li> </ul> <ul> <li>\"version\" (str): The version of the model.</li> </ul> <ul> <li>\"api_key\" (str): The API key that was used to make an inference against this model</li> </ul> <ul> <li>\"metrics\" (dict): A dictionary containing performance metrics for the model:</li> <li>\"num_inferences\" (int): Number of inferences made</li> <li>\"num_errors\" (int): Number of errors</li> <li>\"avg_inference_time\" (float): Average inference time in seconds</li> </ul> Notes <ul> <li>The function calculates statistics over a time interval defined by   the global constant METRICS_INTERVAL, passed in when starting up the container.</li> </ul> Source code in <code>inference/enterprise/device_manager/metrics_service.py</code> <pre><code>def aggregate_model_stats(container_id):\n\"\"\"\n    Aggregate statistics for models within a specified container.\n\n    This function retrieves and aggregates performance metrics for all models\n    associated with the given container within a specified time interval.\n\n    Args:\n        container_id (str): The unique identifier of the container for which\n            model statistics are to be aggregated.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a model's\n        statistics with the following keys:\n        - \"dataset_id\" (str): The ID of the dataset associated with the model.\n        - \"version\" (str): The version of the model.\n        - \"api_key\" (str): The API key that was used to make an inference against this model\n        - \"metrics\" (dict): A dictionary containing performance metrics for the model:\n            - \"num_inferences\" (int): Number of inferences made\n            - \"num_errors\" (int): Number of errors\n            - \"avg_inference_time\" (float): Average inference time in seconds\n\n    Notes:\n        - The function calculates statistics over a time interval defined by\n          the global constant METRICS_INTERVAL, passed in when starting up the container.\n    \"\"\"\n    now = time.time()\n    start = now - METRICS_INTERVAL\n    models = []\n    api_keys = get_cache_model_items().get(container_id, dict()).keys()\n    for api_key in api_keys:\n        model_ids = get_cache_model_items().get(container_id, dict()).get(api_key, [])\n        for model_id in model_ids:\n            model = {\n                \"dataset_id\": model_id.split(\"/\")[0],\n                \"version\": model_id.split(\"/\")[1],\n                \"api_key\": api_key,\n                \"metrics\": get_model_metrics(\n                    container_id, model_id, min=start, max=now\n                ),\n            }\n            models.append(model)\n    return models\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/metrics_service/#inference.enterprise.device_manager.metrics_service.build_container_stats","title":"<code>build_container_stats()</code>","text":"<p>Build statistics for containers and their associated models.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, where each dictionary represents statistics</p> <p>for a container and its associated models with the following keys:</p> <ul> <li>\"uuid\" (str): The unique identifier (UUID) of the container.</li> </ul> <ul> <li>\"startup_time\" (float): The timestamp representing the container's startup time.</li> </ul> <ul> <li>\"models\" (list): A list of dictionaries representing statistics for each model associated with the container (see <code>aggregate_model_stats</code> for format).</li> </ul> Notes <ul> <li>This method relies on a singleton <code>container_service</code> for container information.</li> </ul> Source code in <code>inference/enterprise/device_manager/metrics_service.py</code> <pre><code>def build_container_stats():\n\"\"\"\n    Build statistics for containers and their associated models.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents statistics\n        for a container and its associated models with the following keys:\n        - \"uuid\" (str): The unique identifier (UUID) of the container.\n        - \"startup_time\" (float): The timestamp representing the container's startup time.\n        - \"models\" (list): A list of dictionaries representing statistics for each\n          model associated with the container (see `aggregate_model_stats` for format).\n\n    Notes:\n        - This method relies on a singleton `container_service` for container information.\n    \"\"\"\n    containers = []\n    for id in get_container_ids():\n        container = get_container_by_id(id)\n        if container:\n            container_stats = {}\n            models = aggregate_model_stats(id)\n            container_stats[\"uuid\"] = container.id\n            container_stats[\"version\"] = container.version\n            container_stats[\"startup_time\"] = container.startup_time\n            container_stats[\"models\"] = models\n            if container.status == \"running\" or container.status == \"restarting\":\n                container_stats[\"status\"] = \"running\"\n            elif container.status == \"exited\":\n                container_stats[\"status\"] = \"stopped\"\n            elif container.status == \"paused\":\n                container_stats[\"status\"] = \"idle\"\n            else:\n                container_stats[\"status\"] = \"processing\"\n            containers.append(container_stats)\n    return containers\n</code></pre>"},{"location":"docs/reference/inference/enterprise/device_manager/metrics_service/#inference.enterprise.device_manager.metrics_service.report_metrics_and_handle_commands","title":"<code>report_metrics_and_handle_commands()</code>","text":"<p>Report metrics to Roboflow.</p> <p>This function aggregates statistics for the device and its containers and sends them to Roboflow. If Roboflow sends back any commands, they are handled by the <code>handle_command</code> function.</p> Source code in <code>inference/enterprise/device_manager/metrics_service.py</code> <pre><code>def report_metrics_and_handle_commands():\n\"\"\"\n    Report metrics to Roboflow.\n\n    This function aggregates statistics for the device and its containers and\n    sends them to Roboflow. If Roboflow sends back any commands, they are\n    handled by the `handle_command` function.\n    \"\"\"\n    all_data = aggregate_device_stats()\n    logger.info(f\"Sending metrics to Roboflow {str(all_data)}.\")\n    res = requests.post(METRICS_URL, json=all_data)\n    api_key_safe_raise_for_status(response=res)\n    response = res.json()\n    for cmd in response.get(\"data\", []):\n        if cmd:\n            handle_command(cmd)\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/","title":"dispatch_manager","text":""},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker","title":"<code>ResultsChecker</code>","text":"<p>Class responsible for queuing asyncronous inference runs, keeping track of running requests, and awaiting their results.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>class ResultsChecker:\n\"\"\"\n    Class responsible for queuing asyncronous inference runs,\n    keeping track of running requests, and awaiting their results.\n    \"\"\"\n\n    def __init__(self, redis: Redis):\n        self.tasks: Dict[str, asyncio.Event] = {}\n        self.dones = dict()\n        self.errors = dict()\n        self.running = True\n        self.redis = redis\n        self.semaphore: BoundedSemaphore = BoundedSemaphore(NUM_PARALLEL_TASKS)\n\n    async def add_task(self, task_id: str, request: InferenceRequest):\n\"\"\"\n        Wait until there's available cylce to queue a task.\n        When there are cycles, add the task's id to a list to keep track of its results,\n        launch the preprocess celeryt task, set the task's status to in progress in redis.\n        \"\"\"\n        await self.semaphore.acquire()\n        self.tasks[task_id] = asyncio.Event()\n        preprocess.s(request.dict()).delay()\n\n    def get_result(self, task_id: str) -&gt; Any:\n\"\"\"\n        Check the done tasks and errored tasks for this task id.\n        \"\"\"\n        if task_id in self.dones:\n            return self.dones.pop(task_id)\n        elif task_id in self.errors:\n            message = self.errors.pop(task_id)\n            raise Exception(message)\n        else:\n            raise RuntimeError(\n                \"Task result not found in either success or error dict. Unreachable\"\n            )\n\n    async def loop(self):\n\"\"\"\n        Main loop. Check all in progress tasks for their status, and if their status is final,\n        (either failure or success) then add their results to the appropriate results dictionary.\n        \"\"\"\n        async with self.redis.pubsub() as pubsub:\n            await pubsub.subscribe(\"results\")\n            async for message in pubsub.listen():\n                if message[\"type\"] != \"message\":\n                    continue\n                message = orjson.loads(message[\"data\"])\n                task_id = message.pop(\"task_id\")\n                if task_id not in self.tasks:\n                    continue\n                self.semaphore.release()\n                status = message.pop(\"status\")\n                if status == FAILURE_STATE:\n                    self.errors[task_id] = message[\"payload\"]\n                elif status == SUCCESS_STATE:\n                    self.dones[task_id] = message[\"payload\"]\n                else:\n                    raise RuntimeError(\n                        \"Task result not found in possible states. Unreachable\"\n                    )\n                self.tasks[task_id].set()\n                await asyncio.sleep(0)\n\n    async def wait_for_response(self, key: str):\n        event = self.tasks[key]\n        await event.wait()\n        del self.tasks[key]\n        return self.get_result(key)\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.add_task","title":"<code>add_task(task_id, request)</code>  <code>async</code>","text":"<p>Wait until there's available cylce to queue a task. When there are cycles, add the task's id to a list to keep track of its results, launch the preprocess celeryt task, set the task's status to in progress in redis.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>async def add_task(self, task_id: str, request: InferenceRequest):\n\"\"\"\n    Wait until there's available cylce to queue a task.\n    When there are cycles, add the task's id to a list to keep track of its results,\n    launch the preprocess celeryt task, set the task's status to in progress in redis.\n    \"\"\"\n    await self.semaphore.acquire()\n    self.tasks[task_id] = asyncio.Event()\n    preprocess.s(request.dict()).delay()\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.get_result","title":"<code>get_result(task_id)</code>","text":"<p>Check the done tasks and errored tasks for this task id.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>def get_result(self, task_id: str) -&gt; Any:\n\"\"\"\n    Check the done tasks and errored tasks for this task id.\n    \"\"\"\n    if task_id in self.dones:\n        return self.dones.pop(task_id)\n    elif task_id in self.errors:\n        message = self.errors.pop(task_id)\n        raise Exception(message)\n    else:\n        raise RuntimeError(\n            \"Task result not found in either success or error dict. Unreachable\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.loop","title":"<code>loop()</code>  <code>async</code>","text":"<p>Main loop. Check all in progress tasks for their status, and if their status is final, (either failure or success) then add their results to the appropriate results dictionary.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>async def loop(self):\n\"\"\"\n    Main loop. Check all in progress tasks for their status, and if their status is final,\n    (either failure or success) then add their results to the appropriate results dictionary.\n    \"\"\"\n    async with self.redis.pubsub() as pubsub:\n        await pubsub.subscribe(\"results\")\n        async for message in pubsub.listen():\n            if message[\"type\"] != \"message\":\n                continue\n            message = orjson.loads(message[\"data\"])\n            task_id = message.pop(\"task_id\")\n            if task_id not in self.tasks:\n                continue\n            self.semaphore.release()\n            status = message.pop(\"status\")\n            if status == FAILURE_STATE:\n                self.errors[task_id] = message[\"payload\"]\n            elif status == SUCCESS_STATE:\n                self.dones[task_id] = message[\"payload\"]\n            else:\n                raise RuntimeError(\n                    \"Task result not found in possible states. Unreachable\"\n                )\n            self.tasks[task_id].set()\n            await asyncio.sleep(0)\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/entrypoint/","title":"entrypoint","text":""},{"location":"docs/reference/inference/enterprise/parallel/infer/","title":"infer","text":""},{"location":"docs/reference/inference/enterprise/parallel/infer/#inference.enterprise.parallel.infer.get_batch","title":"<code>get_batch(redis, model_names)</code>","text":"<p>Run a heuristic to select the best batch to infer on redis[Redis]: redis client model_names[List[str]]: list of models with nonzero number of requests returns:     Tuple[List[Dict], str]     List[Dict] represents a batch of request dicts     str is the model id</p> Source code in <code>inference/enterprise/parallel/infer.py</code> <pre><code>def get_batch(redis: Redis, model_names: List[str]) -&gt; Tuple[List[Dict], str]:\n\"\"\"\n    Run a heuristic to select the best batch to infer on\n    redis[Redis]: redis client\n    model_names[List[str]]: list of models with nonzero number of requests\n    returns:\n        Tuple[List[Dict], str]\n        List[Dict] represents a batch of request dicts\n        str is the model id\n    \"\"\"\n    batch_sizes = [\n        RoboflowInferenceModel.model_metadata_from_memcache_endpoint(m)[\"batch_size\"]\n        for m in model_names\n    ]\n    batch_sizes = [b if not isinstance(b, str) else BATCH_SIZE for b in batch_sizes]\n    batches = [\n        redis.zrange(f\"infer:{m}\", 0, b - 1, withscores=True)\n        for m, b in zip(model_names, batch_sizes)\n    ]\n    model_index = select_best_inference_batch(batches, batch_sizes)\n    batch = batches[model_index]\n    selected_model = model_names[model_index]\n    redis.zrem(f\"infer:{selected_model}\", *[b[0] for b in batch])\n    redis.hincrby(f\"requests\", selected_model, -len(batch))\n    batch = [orjson.loads(b[0]) for b in batch]\n    return batch, selected_model\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/infer/#inference.enterprise.parallel.infer.write_infer_arrays_and_launch_postprocess","title":"<code>write_infer_arrays_and_launch_postprocess(arrs, request, preproc_return_metadata)</code>","text":"<p>Write inference results to shared memory and launch the postprocessing task</p> Source code in <code>inference/enterprise/parallel/infer.py</code> <pre><code>def write_infer_arrays_and_launch_postprocess(\n    arrs: Tuple[np.ndarray, ...],\n    request: InferenceRequest,\n    preproc_return_metadata: Dict,\n):\n\"\"\"Write inference results to shared memory and launch the postprocessing task\"\"\"\n    shms = [shared_memory.SharedMemory(create=True, size=arr.nbytes) for arr in arrs]\n    with shm_manager(*shms):\n        shm_metadatas = []\n        for arr, shm in zip(arrs, shms):\n            shared = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)\n            shared[:] = arr[:]\n            shm_metadata = SharedMemoryMetadata(\n                shm_name=shm.name, array_shape=arr.shape, array_dtype=arr.dtype.name\n            )\n            shm_metadatas.append(asdict(shm_metadata))\n\n        postprocess.s(\n            tuple(shm_metadatas), request.dict(), preproc_return_metadata\n        ).delay()\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/parallel_http_api/","title":"parallel_http_api","text":""},{"location":"docs/reference/inference/enterprise/parallel/parallel_http_config/","title":"parallel_http_config","text":""},{"location":"docs/reference/inference/enterprise/parallel/tasks/","title":"tasks","text":""},{"location":"docs/reference/inference/enterprise/parallel/utils/","title":"utils","text":""},{"location":"docs/reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.SharedMemoryMetadata","title":"<code>SharedMemoryMetadata</code>  <code>dataclass</code>","text":"<p>Info needed to load array from shared memory</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@dataclass\nclass SharedMemoryMetadata:\n\"\"\"Info needed to load array from shared memory\"\"\"\n\n    shm_name: str\n    array_shape: List[int]\n    array_dtype: str\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.failure_handler","title":"<code>failure_handler(redis, *request_ids)</code>","text":"<p>Context manager that updates the status/results key in redis with exception info on failure.</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@contextmanager\ndef failure_handler(redis: Redis, *request_ids: str):\n\"\"\"\n    Context manager that updates the status/results key in redis with exception\n    info on failure.\n    \"\"\"\n    try:\n        yield\n    except Exception as error:\n        message = type(error).__name__ + \": \" + str(error)\n        for request_id in request_ids:\n            redis.publish(\n                \"results\",\n                json.dumps(\n                    {\"task_id\": request_id, \"status\": FAILURE_STATE, \"payload\": message}\n                ),\n            )\n        raise\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.shm_manager","title":"<code>shm_manager(*shms, unlink_on_success=False)</code>","text":"<p>Context manager that closes and frees shared memory objects.</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@contextmanager\ndef shm_manager(\n    *shms: Union[str, shared_memory.SharedMemory], unlink_on_success: bool = False\n):\n\"\"\"Context manager that closes and frees shared memory objects.\"\"\"\n    try:\n        loaded_shms = []\n        for shm in shms:\n            errors = []\n            try:\n                if isinstance(shm, str):\n                    shm = shared_memory.SharedMemory(name=shm)\n                loaded_shms.append(shm)\n            except BaseException as error:\n                errors.append(error)\n            if errors:\n                raise Exception(errors)\n\n        yield loaded_shms\n    except:\n        for shm in loaded_shms:\n            shm.close()\n            shm.unlink()\n        raise\n    else:\n        for shm in loaded_shms:\n            shm.close()\n            if unlink_on_success:\n                shm.unlink()\n</code></pre>"},{"location":"docs/reference/inference/enterprise/stream_management/api/app/","title":"app","text":""},{"location":"docs/reference/inference/enterprise/stream_management/api/entities/","title":"entities","text":""},{"location":"docs/reference/inference/enterprise/stream_management/api/errors/","title":"errors","text":""},{"location":"docs/reference/inference/enterprise/stream_management/api/stream_manager_client/","title":"stream_manager_client","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/app/","title":"app","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/communication/","title":"communication","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/entities/","title":"entities","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/errors/","title":"errors","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/inference_pipeline_manager/","title":"inference_pipeline_manager","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/serialisation/","title":"serialisation","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/tcp_server/","title":"tcp_server","text":""},{"location":"docs/reference/inference/enterprise/workflows/constants/","title":"constants","text":""},{"location":"docs/reference/inference/enterprise/workflows/errors/","title":"errors","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/core/","title":"core","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/entities/","title":"entities","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/execution_engine/","title":"execution_engine","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/execution_engine/#inference.enterprise.workflows.complier.execution_engine.execute_steps","title":"<code>execute_steps(steps, max_concurrent_steps, execution_graph, runtime_parameters, outputs_lookup, model_manager, api_key, step_execution_mode, active_learning_middleware, background_tasks)</code>  <code>async</code>","text":"<p>outputs_lookup is mutated while execution, only independent steps may be run together</p> Source code in <code>inference/enterprise/workflows/complier/execution_engine.py</code> <pre><code>async def execute_steps(\n    steps: List[str],\n    max_concurrent_steps: int,\n    execution_graph: DiGraph,\n    runtime_parameters: Dict[str, Any],\n    outputs_lookup: OutputsLookup,\n    model_manager: ModelManager,\n    api_key: Optional[str],\n    step_execution_mode: StepExecutionMode,\n    active_learning_middleware: WorkflowsActiveLearningMiddleware,\n    background_tasks: Optional[BackgroundTasks],\n) -&gt; Set[str]:\n\"\"\"outputs_lookup is mutated while execution, only independent steps may be run together\"\"\"\n    logger.info(f\"Executing steps: {steps}. Execution mode: {step_execution_mode}\")\n    nodes_to_discard = set()\n    steps_batches = list(make_batches(iterable=steps, batch_size=max_concurrent_steps))\n    for steps_batch in steps_batches:\n        logger.info(f\"Steps batch: {steps_batch}\")\n        coroutines = [\n            safe_execute_step(\n                step=step,\n                execution_graph=execution_graph,\n                runtime_parameters=runtime_parameters,\n                outputs_lookup=outputs_lookup,\n                model_manager=model_manager,\n                api_key=api_key,\n                step_execution_mode=step_execution_mode,\n                active_learning_middleware=active_learning_middleware,\n                background_tasks=background_tasks,\n            )\n            for step in steps_batch\n        ]\n        results = await asyncio.gather(*coroutines)\n        for result in results:\n            nodes_to_discard.update(result)\n    return nodes_to_discard\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/complier/flow_coordinator/","title":"flow_coordinator","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/graph_parser/","title":"graph_parser","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/graph_parser/#inference.enterprise.workflows.complier.graph_parser.verify_each_node_step_has_parent_in_the_same_branch","title":"<code>verify_each_node_step_has_parent_in_the_same_branch(execution_graph)</code>","text":"<p>Conditional branching creates a bit of mess, in terms of determining which steps to execute. Let's imagine graph:           / -&gt; B -&gt; C -&gt; D A -&gt; IF &lt;                           \\ -&gt; E -&gt; F -&gt; G -&gt; H where node G requires node C even though IF branched the execution. In other words - the problem emerges if a node of kind STEP has a parent (node from which it can be achieved) of kind STEP and this parent is in a different branch (point out that we allow for a single step to have multiple steps as input, but they must be at the same execution path - for instance if D requires an output from C and B - this is allowed). Additionally, we must prevent situation when outcomes of branches started by two or more condition steps merge with each other, as condition eval may result in contradictory execution (2).</p> <p>We need to detect that situation upfront, such that we can raise error of ambiguous execution path rather than run time-consuming computations that will end up in error.</p> <p>To detect problem, first we detect steps with more than one parent step. From those steps we trace what sequence of steps would lead to execution of problematic one. For each problematic node we take its parent nodes. Then, we analyse paths from those parent nodes in reversed topological order (from those nodes towards entry nodes of execution graph). While our analysis, on each path we denote <code>Condition</code> steps and result of condition evaluation that must have been observed in runtime, to reach the problematic node while graph execution in normal direction. If we detect that for any <code>Condition</code> step we would need to output both True and False (more than one registered next step of <code>Condition</code> step) - we raise error. To detect problem (2) - we only let number of different condition steps considered be the number of max condition steps in a single path from origin to parent of problematic step.</p> <p>Beware that the latter part of algorithm has quite bad time complexity in general case. Worst part of algorithm runs at O(V^4) - at least taking coarse, worst-case estimations. In fact, there is not so bad: * The number of step nodes with multiple parents that we loop over in main loop, reduces the number of steps we iterate through in inner loops, as we are dealing with DAG (with quite limited amount of edges) and for each multi-parent node takes at least two other nodes (to construct a suspicious group) - so expected number of iterations in main loop is low - let's say 1-3 for a real graph. * for any reasonable execution graph, the complexity should be acceptable.</p> Source code in <code>inference/enterprise/workflows/complier/graph_parser.py</code> <pre><code>def verify_each_node_step_has_parent_in_the_same_branch(\n    execution_graph: DiGraph,\n) -&gt; None:\n\"\"\"\n    Conditional branching creates a bit of mess, in terms of determining which\n    steps to execute.\n    Let's imagine graph:\n              / -&gt; B -&gt; C -&gt; D\n    A -&gt; IF &lt;             \\\n              \\ -&gt; E -&gt; F -&gt; G -&gt; H\n    where node G requires node C even though IF branched the execution. In other\n    words - the problem emerges if a node of kind STEP has a parent (node from which\n    it can be achieved) of kind STEP and this parent is in a different branch (point out that\n    we allow for a single step to have multiple steps as input, but they must be at the same\n    execution path - for instance if D requires an output from C and B - this is allowed).\n    Additionally, we must prevent situation when outcomes of branches started by two or more\n    condition steps merge with each other, as condition eval may result in contradictory\n    execution (2).\n\n\n    We need to detect that situation upfront, such that we can raise error of ambiguous execution path\n    rather than run time-consuming computations that will end up in error.\n\n    To detect problem, first we detect steps with more than one parent step.\n    From those steps we trace what sequence of steps would lead to execution of problematic one.\n    For each problematic node we take its parent nodes. Then, we analyse paths from\n    those parent nodes in reversed topological order (from those nodes towards entry nodes\n    of execution graph). While our analysis, on each path we denote `Condition` steps and\n    result of condition evaluation that must have been observed in runtime, to reach\n    the problematic node while graph execution in normal direction. If we detect that\n    for any `Condition` step we would need to output both True and False (more than one registered\n    next step of `Condition` step) - we raise error.\n    To detect problem (2) - we only let number of different condition steps considered be the number of\n    max condition steps in a single path from origin to parent of problematic step.\n\n    Beware that the latter part of algorithm has quite bad time complexity in general case.\n    Worst part of algorithm runs at O(V^4) - at least taking coarse, worst-case estimations.\n    In fact, there is not so bad:\n    * The number of step nodes with multiple parents that we loop over in main loop, reduces the number of\n    steps we iterate through in inner loops, as we are dealing with DAG (with quite limited amount of edges)\n    and for each multi-parent node takes at least two other nodes (to construct a suspicious group) -\n    so expected number of iterations in main loop is low - let's say 1-3 for a real graph.\n    * for any reasonable execution graph, the complexity should be acceptable.\n    \"\"\"\n    steps_with_more_than_one_parent = detect_steps_with_more_than_one_parent_step(\n        execution_graph=execution_graph\n    )  # O(V+E)\n    if len(steps_with_more_than_one_parent) == 0:\n        return None\n    reversed_steps_graph = construct_reversed_steps_graph(\n        execution_graph=execution_graph\n    )  # O(V+E)\n    reversed_topological_order = list(\n        nx.topological_sort(reversed_steps_graph)\n    )  # O(V+E)\n    for step in steps_with_more_than_one_parent:  # O(V)\n        verify_multi_parent_step_execution_paths(\n            reversed_steps_graph=reversed_steps_graph,\n            reversed_topological_order=reversed_topological_order,\n            step=step,\n        )\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/complier/runtime_input_validator/","title":"runtime_input_validator","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/utils/","title":"utils","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/validator/","title":"validator","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/steps_executors/active_learning_middlewares/","title":"active_learning_middlewares","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/steps_executors/auxiliary/","title":"auxiliary","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/steps_executors/constants/","title":"constants","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/steps_executors/models/","title":"models","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/steps_executors/types/","title":"types","text":""},{"location":"docs/reference/inference/enterprise/workflows/complier/steps_executors/utils/","title":"utils","text":""},{"location":"docs/reference/inference/enterprise/workflows/entities/base/","title":"base","text":""},{"location":"docs/reference/inference/enterprise/workflows/entities/inputs/","title":"inputs","text":""},{"location":"docs/reference/inference/enterprise/workflows/entities/outputs/","title":"outputs","text":""},{"location":"docs/reference/inference/enterprise/workflows/entities/steps/","title":"steps","text":""},{"location":"docs/reference/inference/enterprise/workflows/entities/steps/#inference.enterprise.workflows.entities.steps.StepInterface","title":"<code>StepInterface</code>","text":"<p>             Bases: <code>GraphNone</code></p> Source code in <code>inference/enterprise/workflows/entities/steps.py</code> <pre><code>class StepInterface(GraphNone, metaclass=ABCMeta):\n    @abstractmethod\n    def get_input_names(self) -&gt; Set[str]:\n\"\"\"\n        Supposed to give the name of all fields expected to represent inputs\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_output_names(self) -&gt; Set[str]:\n\"\"\"\n        Supposed to give the name of all fields expected to represent outputs to be referred by other steps\n        \"\"\"\n\n    @abstractmethod\n    def validate_field_selector(\n        self, field_name: str, input_step: GraphNone, index: Optional[int] = None\n    ) -&gt; None:\n\"\"\"\n        Supposed to validate the type of input is referred\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_field_binding(self, field_name: str, value: Any) -&gt; None:\n\"\"\"\n        Supposed to validate the type of value that is to be bounded with field as a result of graph\n        execution (values passed by client to invocation, as well as constructed during graph execution)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/entities/steps/#inference.enterprise.workflows.entities.steps.StepInterface.get_input_names","title":"<code>get_input_names()</code>  <code>abstractmethod</code>","text":"<p>Supposed to give the name of all fields expected to represent inputs</p> Source code in <code>inference/enterprise/workflows/entities/steps.py</code> <pre><code>@abstractmethod\ndef get_input_names(self) -&gt; Set[str]:\n\"\"\"\n    Supposed to give the name of all fields expected to represent inputs\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/entities/steps/#inference.enterprise.workflows.entities.steps.StepInterface.get_output_names","title":"<code>get_output_names()</code>  <code>abstractmethod</code>","text":"<p>Supposed to give the name of all fields expected to represent outputs to be referred by other steps</p> Source code in <code>inference/enterprise/workflows/entities/steps.py</code> <pre><code>@abstractmethod\ndef get_output_names(self) -&gt; Set[str]:\n\"\"\"\n    Supposed to give the name of all fields expected to represent outputs to be referred by other steps\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/entities/steps/#inference.enterprise.workflows.entities.steps.StepInterface.validate_field_binding","title":"<code>validate_field_binding(field_name, value)</code>  <code>abstractmethod</code>","text":"<p>Supposed to validate the type of value that is to be bounded with field as a result of graph execution (values passed by client to invocation, as well as constructed during graph execution)</p> Source code in <code>inference/enterprise/workflows/entities/steps.py</code> <pre><code>@abstractmethod\ndef validate_field_binding(self, field_name: str, value: Any) -&gt; None:\n\"\"\"\n    Supposed to validate the type of value that is to be bounded with field as a result of graph\n    execution (values passed by client to invocation, as well as constructed during graph execution)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/entities/steps/#inference.enterprise.workflows.entities.steps.StepInterface.validate_field_selector","title":"<code>validate_field_selector(field_name, input_step, index=None)</code>  <code>abstractmethod</code>","text":"<p>Supposed to validate the type of input is referred</p> Source code in <code>inference/enterprise/workflows/entities/steps.py</code> <pre><code>@abstractmethod\ndef validate_field_selector(\n    self, field_name: str, input_step: GraphNone, index: Optional[int] = None\n) -&gt; None:\n\"\"\"\n    Supposed to validate the type of input is referred\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/enterprise/workflows/entities/validators/","title":"validators","text":""},{"location":"docs/reference/inference/enterprise/workflows/entities/workflows_specification/","title":"workflows_specification","text":""},{"location":"docs/reference/inference/models/aliases/","title":"aliases","text":""},{"location":"docs/reference/inference/models/utils/","title":"utils","text":""},{"location":"docs/reference/inference/models/clip/clip_model/","title":"clip_model","text":""},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip","title":"<code>Clip</code>","text":"<p>             Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX ClipModel model.</p> <p>This class is responsible for handling the ONNX ClipModel model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>visual_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for visual inference.</p> <code>textual_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for textual inference.</p> <code>resolution</code> <code>int</code> <p>The resolution of the input image.</p> <code>clip_preprocess</code> <code>function</code> <p>Function to preprocess the image.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>class Clip(OnnxRoboflowCoreModel):\n\"\"\"Roboflow ONNX ClipModel model.\n\n    This class is responsible for handling the ONNX ClipModel model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        visual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for visual inference.\n        textual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for textual inference.\n        resolution (int): The resolution of the input image.\n        clip_preprocess (function): Function to preprocess the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        model_id: str = CLIP_MODEL_ID,\n        onnxruntime_execution_providers: List[\n            str\n        ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n        **kwargs,\n    ):\n\"\"\"Initializes the Clip with the given arguments and keyword arguments.\"\"\"\n        self.onnxruntime_execution_providers = onnxruntime_execution_providers\n        t1 = perf_counter()\n        super().__init__(*args, model_id=model_id, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n        self.visual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"visual.onnx\"),\n            providers=self.onnxruntime_execution_providers,\n        )\n\n        self.textual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"textual.onnx\"),\n            providers=self.onnxruntime_execution_providers,\n        )\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n        self.clip_preprocess = clip.clip._transform(self.resolution)\n        self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n        self.task_type = \"embedding\"\n\n    def compare(\n        self,\n        subject: Any,\n        prompt: Any,\n        subject_type: str = \"image\",\n        prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n        **kwargs,\n    ) -&gt; Union[List[float], Dict[str, float]]:\n\"\"\"\n        Compares the subject with the prompt to calculate similarity scores.\n\n        Args:\n            subject (Any): The subject data to be compared. Can be either an image or text.\n            prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n            subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n            prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.\n\n        Raises:\n            ValueError: If subject_type or prompt_type is neither \"image\" nor \"text\".\n            ValueError: If the number of prompts exceeds the maximum batch size.\n        \"\"\"\n\n        if subject_type == \"image\":\n            subject_embeddings = self.embed_image(subject)\n        elif subject_type == \"text\":\n            subject_embeddings = self.embed_text(subject)\n        else:\n            raise ValueError(\n                \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n            )\n\n        if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n            prompt_keys = prompt.keys()\n            prompt = [prompt[k] for k in prompt_keys]\n            prompt_obj = \"dict\"\n        else:\n            prompt = prompt\n            if not isinstance(prompt, list):\n                prompt = [prompt]\n            prompt_obj = \"list\"\n\n        if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n\n        if prompt_type == \"image\":\n            prompt_embeddings = self.embed_image(prompt)\n        elif prompt_type == \"text\":\n            prompt_embeddings = self.embed_text(prompt)\n        else:\n            raise ValueError(\n                \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n            )\n\n        similarities = [\n            cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n        ]\n\n        if prompt_obj == \"dict\":\n            similarities = dict(zip(prompt_keys, similarities))\n\n        return similarities\n\n    def make_compare_response(\n        self, similarities: Union[List[float], Dict[str, float]]\n    ) -&gt; ClipCompareResponse:\n\"\"\"\n        Creates a ClipCompareResponse object from the provided similarity data.\n\n        Args:\n            similarities (Union[List[float], Dict[str, float]]): A list or dictionary containing similarity scores.\n\n        Returns:\n            ClipCompareResponse: An instance of the ClipCompareResponse with the given similarity scores.\n\n        Example:\n            Assuming `ClipCompareResponse` expects a dictionary of string-float pairs:\n\n            &gt;&gt;&gt; make_compare_response({\"image1\": 0.98, \"image2\": 0.76})\n            ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})\n        \"\"\"\n        response = ClipCompareResponse(similarity=similarities)\n        return response\n\n    def embed_image(\n        self,\n        image: Any,\n        **kwargs,\n    ) -&gt; np.ndarray:\n\"\"\"\n        Embeds an image or a list of images using the Clip model.\n\n        Args:\n            image (Any): The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the image(s) as a numpy array.\n\n        Raises:\n            ValueError: If the number of images in the list exceeds the maximum batch size.\n\n        Notes:\n            The function measures performance using perf_counter and also has support for ONNX session to get embeddings.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(image, list):\n            if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n                )\n            imgs = [self.preproc_image(i) for i in image]\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in = self.preproc_image(image)\n\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n        return embeddings\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n        return (embeddings,)\n\n    def make_embed_image_response(\n        self, embeddings: np.ndarray\n    ) -&gt; ClipEmbeddingResponse:\n\"\"\"\n        Converts the given embeddings into a ClipEmbeddingResponse object.\n\n        Args:\n            embeddings (np.ndarray): A numpy array containing the embeddings for an image or images.\n\n        Returns:\n            ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n        Example:\n            &gt;&gt;&gt; embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n            &gt;&gt;&gt; make_embed_image_response(embeddings_array)\n            ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n        \"\"\"\n        response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n\n        return response\n\n    def embed_text(\n        self,\n        text: Union[str, List[str]],\n        **kwargs,\n    ) -&gt; np.ndarray:\n\"\"\"\n        Embeds a text or a list of texts using the Clip model.\n\n        Args:\n            text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the text or texts as a numpy array.\n\n        Raises:\n            ValueError: If the number of text strings in the list exceeds the maximum batch size.\n\n        Notes:\n            The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.\n        \"\"\"\n        if isinstance(text, list):\n            texts = text\n        else:\n            texts = [text]\n        results = []\n        for texts_batch in create_batches(\n            sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n        ):\n            tokenized_batch = clip.tokenize(texts_batch).numpy().astype(np.int32)\n            onnx_input_text = {\n                self.textual_onnx_session.get_inputs()[0].name: tokenized_batch\n            }\n            embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n            results.append(embeddings)\n        return np.concatenate(results, axis=0)\n\n    def make_embed_text_response(self, embeddings: np.ndarray) -&gt; ClipEmbeddingResponse:\n\"\"\"\n        Converts the given text embeddings into a ClipEmbeddingResponse object.\n\n        Args:\n            embeddings (np.ndarray): A numpy array containing the embeddings for a text or texts.\n\n        Returns:\n            ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n        Example:\n            &gt;&gt;&gt; embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n            &gt;&gt;&gt; make_embed_text_response(embeddings_array)\n            ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n        \"\"\"\n        response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\"textual.onnx\", \"visual.onnx\"]\n\n    def infer_from_request(\n        self, request: ClipInferenceRequest\n    ) -&gt; ClipEmbeddingResponse:\n\"\"\"Routes the request to the appropriate inference function.\n\n        Args:\n            request (ClipInferenceRequest): The request object containing the inference details.\n\n        Returns:\n            ClipEmbeddingResponse: The response object containing the embeddings.\n        \"\"\"\n        t1 = perf_counter()\n        if isinstance(request, ClipImageEmbeddingRequest):\n            infer_func = self.embed_image\n            make_response_func = self.make_embed_image_response\n        elif isinstance(request, ClipTextEmbeddingRequest):\n            infer_func = self.embed_text\n            make_response_func = self.make_embed_text_response\n        elif isinstance(request, ClipCompareRequest):\n            infer_func = self.compare\n            make_response_func = self.make_compare_response\n        else:\n            raise ValueError(\n                f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n            )\n        data = infer_func(**request.dict())\n        response = make_response_func(data)\n        response.time = perf_counter() - t1\n        return response\n\n    def make_response(self, embeddings, *args, **kwargs) -&gt; InferenceResponse:\n        return [self.make_embed_image_response(embeddings)]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        return [self.make_embed_image_response(predictions[0])]\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n\"\"\"Embeds an image\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        return super().infer(image, **kwargs)\n\n    def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n\"\"\"Preprocesses an inference request image.\n\n        Args:\n            image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n        Returns:\n            np.ndarray: A numpy array of the preprocessed image pixel data.\n        \"\"\"\n        pil_image = Image.fromarray(load_image_rgb(image))\n        preprocessed_image = self.clip_preprocess(pil_image)\n\n        img_in = np.expand_dims(preprocessed_image, axis=0)\n\n        return img_in.astype(np.float32)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        return self.preproc_image(image), PreprocessReturnMetadata({})\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.__init__","title":"<code>__init__(*args, model_id=CLIP_MODEL_ID, onnxruntime_execution_providers=get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS), **kwargs)</code>","text":"<p>Initializes the Clip with the given arguments and keyword arguments.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    model_id: str = CLIP_MODEL_ID,\n    onnxruntime_execution_providers: List[\n        str\n    ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n    **kwargs,\n):\n\"\"\"Initializes the Clip with the given arguments and keyword arguments.\"\"\"\n    self.onnxruntime_execution_providers = onnxruntime_execution_providers\n    t1 = perf_counter()\n    super().__init__(*args, model_id=model_id, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n    self.visual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"visual.onnx\"),\n        providers=self.onnxruntime_execution_providers,\n    )\n\n    self.textual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"textual.onnx\"),\n        providers=self.onnxruntime_execution_providers,\n    )\n\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n    self.clip_preprocess = clip.clip._transform(self.resolution)\n    self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n    self.task_type = \"embedding\"\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.compare","title":"<code>compare(subject, prompt, subject_type='image', prompt_type='text', **kwargs)</code>","text":"<p>Compares the subject with the prompt to calculate similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>Any</code> <p>The subject data to be compared. Can be either an image or text.</p> required <code>prompt</code> <code>Any</code> <p>The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.</p> required <code>subject_type</code> <code>str</code> <p>Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".</p> <code>'image'</code> <code>prompt_type</code> <code>Union[str, List[str], Dict[str, Any]]</code> <p>Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".</p> <code>'text'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Dict[str, float]]</code> <p>Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If subject_type or prompt_type is neither \"image\" nor \"text\".</p> <code>ValueError</code> <p>If the number of prompts exceeds the maximum batch size.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def compare(\n    self,\n    subject: Any,\n    prompt: Any,\n    subject_type: str = \"image\",\n    prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n    **kwargs,\n) -&gt; Union[List[float], Dict[str, float]]:\n\"\"\"\n    Compares the subject with the prompt to calculate similarity scores.\n\n    Args:\n        subject (Any): The subject data to be compared. Can be either an image or text.\n        prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n        subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n        prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.\n\n    Raises:\n        ValueError: If subject_type or prompt_type is neither \"image\" nor \"text\".\n        ValueError: If the number of prompts exceeds the maximum batch size.\n    \"\"\"\n\n    if subject_type == \"image\":\n        subject_embeddings = self.embed_image(subject)\n    elif subject_type == \"text\":\n        subject_embeddings = self.embed_text(subject)\n    else:\n        raise ValueError(\n            \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n        )\n\n    if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n        prompt_keys = prompt.keys()\n        prompt = [prompt[k] for k in prompt_keys]\n        prompt_obj = \"dict\"\n    else:\n        prompt = prompt\n        if not isinstance(prompt, list):\n            prompt = [prompt]\n        prompt_obj = \"list\"\n\n    if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n        raise ValueError(\n            f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n        )\n\n    if prompt_type == \"image\":\n        prompt_embeddings = self.embed_image(prompt)\n    elif prompt_type == \"text\":\n        prompt_embeddings = self.embed_text(prompt)\n    else:\n        raise ValueError(\n            \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n        )\n\n    similarities = [\n        cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n    ]\n\n    if prompt_obj == \"dict\":\n        similarities = dict(zip(prompt_keys, similarities))\n\n    return similarities\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.embed_image","title":"<code>embed_image(image, **kwargs)</code>","text":"<p>Embeds an image or a list of images using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the image(s) as a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of images in the list exceeds the maximum batch size.</p> Notes <p>The function measures performance using perf_counter and also has support for ONNX session to get embeddings.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def embed_image(\n    self,\n    image: Any,\n    **kwargs,\n) -&gt; np.ndarray:\n\"\"\"\n    Embeds an image or a list of images using the Clip model.\n\n    Args:\n        image (Any): The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the image(s) as a numpy array.\n\n    Raises:\n        ValueError: If the number of images in the list exceeds the maximum batch size.\n\n    Notes:\n        The function measures performance using perf_counter and also has support for ONNX session to get embeddings.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(image, list):\n        if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n        imgs = [self.preproc_image(i) for i in image]\n        img_in = np.concatenate(imgs, axis=0)\n    else:\n        img_in = self.preproc_image(image)\n\n    onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n    embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n    return embeddings\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.embed_text","title":"<code>embed_text(text, **kwargs)</code>","text":"<p>Embeds a text or a list of texts using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, List[str]]</code> <p>The text string or list of text strings to be embedded.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the text or texts as a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of text strings in the list exceeds the maximum batch size.</p> Notes <p>The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def embed_text(\n    self,\n    text: Union[str, List[str]],\n    **kwargs,\n) -&gt; np.ndarray:\n\"\"\"\n    Embeds a text or a list of texts using the Clip model.\n\n    Args:\n        text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the text or texts as a numpy array.\n\n    Raises:\n        ValueError: If the number of text strings in the list exceeds the maximum batch size.\n\n    Notes:\n        The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.\n    \"\"\"\n    if isinstance(text, list):\n        texts = text\n    else:\n        texts = [text]\n    results = []\n    for texts_batch in create_batches(\n        sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n    ):\n        tokenized_batch = clip.tokenize(texts_batch).numpy().astype(np.int32)\n        onnx_input_text = {\n            self.textual_onnx_session.get_inputs()[0].name: tokenized_batch\n        }\n        embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n        results.append(embeddings)\n    return np.concatenate(results, axis=0)\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\"textual.onnx\", \"visual.onnx\"]\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Embeds an image - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n\"\"\"Embeds an image\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    return super().infer(image, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Routes the request to the appropriate inference function.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipInferenceRequest</code> <p>The request object containing the inference details.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>The response object containing the embeddings.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def infer_from_request(\n    self, request: ClipInferenceRequest\n) -&gt; ClipEmbeddingResponse:\n\"\"\"Routes the request to the appropriate inference function.\n\n    Args:\n        request (ClipInferenceRequest): The request object containing the inference details.\n\n    Returns:\n        ClipEmbeddingResponse: The response object containing the embeddings.\n    \"\"\"\n    t1 = perf_counter()\n    if isinstance(request, ClipImageEmbeddingRequest):\n        infer_func = self.embed_image\n        make_response_func = self.make_embed_image_response\n    elif isinstance(request, ClipTextEmbeddingRequest):\n        infer_func = self.embed_text\n        make_response_func = self.make_embed_text_response\n    elif isinstance(request, ClipCompareRequest):\n        infer_func = self.compare\n        make_response_func = self.make_compare_response\n    else:\n        raise ValueError(\n            f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n        )\n    data = infer_func(**request.dict())\n    response = make_response_func(data)\n    response.time = perf_counter() - t1\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_compare_response","title":"<code>make_compare_response(similarities)</code>","text":"<p>Creates a ClipCompareResponse object from the provided similarity data.</p> <p>Parameters:</p> Name Type Description Default <code>similarities</code> <code>Union[List[float], Dict[str, float]]</code> <p>A list or dictionary containing similarity scores.</p> required <p>Returns:</p> Name Type Description <code>ClipCompareResponse</code> <code>ClipCompareResponse</code> <p>An instance of the ClipCompareResponse with the given similarity scores.</p> Example <p>Assuming <code>ClipCompareResponse</code> expects a dictionary of string-float pairs:</p> <p>make_compare_response({\"image1\": 0.98, \"image2\": 0.76}) ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_compare_response(\n    self, similarities: Union[List[float], Dict[str, float]]\n) -&gt; ClipCompareResponse:\n\"\"\"\n    Creates a ClipCompareResponse object from the provided similarity data.\n\n    Args:\n        similarities (Union[List[float], Dict[str, float]]): A list or dictionary containing similarity scores.\n\n    Returns:\n        ClipCompareResponse: An instance of the ClipCompareResponse with the given similarity scores.\n\n    Example:\n        Assuming `ClipCompareResponse` expects a dictionary of string-float pairs:\n\n        &gt;&gt;&gt; make_compare_response({\"image1\": 0.98, \"image2\": 0.76})\n        ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})\n    \"\"\"\n    response = ClipCompareResponse(similarity=similarities)\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_embed_image_response","title":"<code>make_embed_image_response(embeddings)</code>","text":"<p>Converts the given embeddings into a ClipEmbeddingResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>A numpy array containing the embeddings for an image or images.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.</p> Example <p>embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]]) make_embed_image_response(embeddings_array) ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_embed_image_response(\n    self, embeddings: np.ndarray\n) -&gt; ClipEmbeddingResponse:\n\"\"\"\n    Converts the given embeddings into a ClipEmbeddingResponse object.\n\n    Args:\n        embeddings (np.ndarray): A numpy array containing the embeddings for an image or images.\n\n    Returns:\n        ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n    Example:\n        &gt;&gt;&gt; embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n        &gt;&gt;&gt; make_embed_image_response(embeddings_array)\n        ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n    \"\"\"\n    response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_embed_text_response","title":"<code>make_embed_text_response(embeddings)</code>","text":"<p>Converts the given text embeddings into a ClipEmbeddingResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>A numpy array containing the embeddings for a text or texts.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.</p> Example <p>embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]]) make_embed_text_response(embeddings_array) ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_embed_text_response(self, embeddings: np.ndarray) -&gt; ClipEmbeddingResponse:\n\"\"\"\n    Converts the given text embeddings into a ClipEmbeddingResponse object.\n\n    Args:\n        embeddings (np.ndarray): A numpy array containing the embeddings for a text or texts.\n\n    Returns:\n        ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n    Example:\n        &gt;&gt;&gt; embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n        &gt;&gt;&gt; make_embed_text_response(embeddings_array)\n        ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n    \"\"\"\n    response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an inference request image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The object containing information necessary to load the image for inference.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of the preprocessed image pixel data.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n\"\"\"Preprocesses an inference request image.\n\n    Args:\n        image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n    Returns:\n        np.ndarray: A numpy array of the preprocessed image pixel data.\n    \"\"\"\n    pil_image = Image.fromarray(load_image_rgb(image))\n    preprocessed_image = self.clip_preprocess(pil_image)\n\n    img_in = np.expand_dims(preprocessed_image, axis=0)\n\n    return img_in.astype(np.float32)\n</code></pre>"},{"location":"docs/reference/inference/models/cogvlm/cogvlm/","title":"cogvlm","text":""},{"location":"docs/reference/inference/models/doctr/doctr_model/","title":"doctr_model","text":""},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR","title":"<code>DocTR</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTR(RoboflowCoreModel):\n    def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n\"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        self.api_key = kwargs.get(\"api_key\")\n        self.dataset_id = \"doctr\"\n        self.version_id = \"default\"\n        self.endpoint = model_id\n        model_id = model_id.lower()\n\n        os.environ[\"DOCTR_CACHE_DIR\"] = os.path.join(MODEL_CACHE_DIR, \"doctr_rec\")\n\n        self.det_model = DocTRDet(api_key=kwargs.get(\"api_key\"))\n        self.rec_model = DocTRRec(api_key=kwargs.get(\"api_key\"))\n\n        os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_rec/models/\", exist_ok=True)\n        os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_det/models/\", exist_ok=True)\n\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/doctr_det/db_resnet50/model.pt\",\n            f\"{MODEL_CACHE_DIR}/doctr_det/models/db_resnet50-ac60cadc.pt\",\n        )\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/doctr_rec/crnn_vgg16_bn/model.pt\",\n            f\"{MODEL_CACHE_DIR}/doctr_rec/models/crnn_vgg16_bn-9762b0b0.pt\",\n        )\n\n        self.model = ocr_predictor(\n            det_arch=self.det_model.version_id,\n            reco_arch=self.rec_model.version_id,\n            pretrained=True,\n        )\n        self.task_type = \"ocr\"\n\n    def clear_cache(self) -&gt; None:\n        self.det_model.clear_cache()\n        self.rec_model.clear_cache()\n\n    def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n\"\"\"\n        DocTR pre-processes images as part of its inference pipeline.\n\n        Thus, no preprocessing is required here.\n        \"\"\"\n        pass\n\n    def infer_from_request(\n        self, request: DoctrOCRInferenceRequest\n    ) -&gt; DoctrOCRInferenceResponse:\n        t1 = perf_counter()\n        result = self.infer(**request.dict())\n        return DoctrOCRInferenceResponse(\n            result=result,\n            time=perf_counter() - t1,\n        )\n\n    def infer(self, image: Any, **kwargs):\n\"\"\"\n        Run inference on a provided image.\n            - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n        Args:\n            request (DoctrOCRInferenceRequest): The inference request.\n\n        Returns:\n            DoctrOCRInferenceResponse: The inference response.\n        \"\"\"\n\n        img = load_image(image)\n\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n            image = Image.fromarray(img[0])\n\n            image.save(f.name)\n\n            doc = DocumentFile.from_images([f.name])\n\n            result = self.model(doc).export()\n\n            result = result[\"pages\"][0][\"blocks\"]\n\n            result = [\n                \" \".join([word[\"value\"] for word in line[\"words\"]])\n                for block in result\n                for line in block[\"lines\"]\n            ]\n\n            result = \" \".join(result)\n\n            return result\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.__init__","title":"<code>__init__(*args, model_id='doctr_rec/crnn_vgg16_bn', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n\"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    self.api_key = kwargs.get(\"api_key\")\n    self.dataset_id = \"doctr\"\n    self.version_id = \"default\"\n    self.endpoint = model_id\n    model_id = model_id.lower()\n\n    os.environ[\"DOCTR_CACHE_DIR\"] = os.path.join(MODEL_CACHE_DIR, \"doctr_rec\")\n\n    self.det_model = DocTRDet(api_key=kwargs.get(\"api_key\"))\n    self.rec_model = DocTRRec(api_key=kwargs.get(\"api_key\"))\n\n    os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_rec/models/\", exist_ok=True)\n    os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_det/models/\", exist_ok=True)\n\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/doctr_det/db_resnet50/model.pt\",\n        f\"{MODEL_CACHE_DIR}/doctr_det/models/db_resnet50-ac60cadc.pt\",\n    )\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/doctr_rec/crnn_vgg16_bn/model.pt\",\n        f\"{MODEL_CACHE_DIR}/doctr_rec/models/crnn_vgg16_bn-9762b0b0.pt\",\n    )\n\n    self.model = ocr_predictor(\n        det_arch=self.det_model.version_id,\n        reco_arch=self.rec_model.version_id,\n        pretrained=True,\n    )\n    self.task_type = \"ocr\"\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Run inference on a provided image.     - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>DoctrOCRInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Name Type Description <code>DoctrOCRInferenceResponse</code> <p>The inference response.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def infer(self, image: Any, **kwargs):\n\"\"\"\n    Run inference on a provided image.\n        - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n    Args:\n        request (DoctrOCRInferenceRequest): The inference request.\n\n    Returns:\n        DoctrOCRInferenceResponse: The inference response.\n    \"\"\"\n\n    img = load_image(image)\n\n    with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n        image = Image.fromarray(img[0])\n\n        image.save(f.name)\n\n        doc = DocumentFile.from_images([f.name])\n\n        result = self.model(doc).export()\n\n        result = result[\"pages\"][0][\"blocks\"]\n\n        result = [\n            \" \".join([word[\"value\"] for word in line[\"words\"]])\n            for block in result\n            for line in block[\"lines\"]\n        ]\n\n        result = \" \".join(result)\n\n        return result\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>DocTR pre-processes images as part of its inference pipeline.</p> <p>Thus, no preprocessing is required here.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n\"\"\"\n    DocTR pre-processes images as part of its inference pipeline.\n\n    Thus, no preprocessing is required here.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet","title":"<code>DocTRDet</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> <p>DocTR class for document Optical Character Recognition (OCR).</p> <p>Attributes:</p> Name Type Description <code>doctr</code> <p>The DocTR model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTRDet(RoboflowCoreModel):\n\"\"\"DocTR class for document Optical Character Recognition (OCR).\n\n    Attributes:\n        doctr: The DocTR model.\n        ort_session: ONNX runtime inference session.\n    \"\"\"\n\n    def __init__(self, *args, model_id: str = \"doctr_det/db_resnet50\", **kwargs):\n\"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        self.get_infer_bucket_file_list()\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet.__init__","title":"<code>__init__(*args, model_id='doctr_det/db_resnet50', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_det/db_resnet50\", **kwargs):\n\"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    self.get_infer_bucket_file_list()\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec","title":"<code>DocTRRec</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTRRec(RoboflowCoreModel):\n    def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n\"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        pass\n\n        self.get_infer_bucket_file_list()\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec.__init__","title":"<code>__init__(*args, model_id='doctr_rec/crnn_vgg16_bn', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n\"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    pass\n\n    self.get_infer_bucket_file_list()\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/","title":"gaze","text":""},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze","title":"<code>Gaze</code>","text":"<p>             Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX Gaze model.</p> <p>This class is responsible for handling the ONNX Gaze model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>gaze_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for gaze detection inference.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>class Gaze(OnnxRoboflowCoreModel):\n\"\"\"Roboflow ONNX Gaze model.\n\n    This class is responsible for handling the ONNX Gaze model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        gaze_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for gaze detection inference.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\"\"\"Initializes the Gaze with the given arguments and keyword arguments.\"\"\"\n\n        t1 = perf_counter()\n        super().__init__(*args, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n\n        # TODO: convert face detector (TensorflowLite) to ONNX model\n\n        self.gaze_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"L2CSNet_gaze360_resnet50_90bins.onnx\"),\n            providers=[\n                (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                    },\n                ),\n                \"CUDAExecutionProvider\",\n                \"OpenVINOExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        # init face detector\n        self.face_detector = mp.tasks.vision.FaceDetector.create_from_options(\n            mp.tasks.vision.FaceDetectorOptions(\n                base_options=mp.tasks.BaseOptions(\n                    model_asset_path=self.cache_file(\"mediapipe_face_detector.tflite\")\n                ),\n                running_mode=mp.tasks.vision.RunningMode.IMAGE,\n            )\n        )\n\n        # additional settings for gaze detection\n        self._gaze_transformations = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Resize(448),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n        self.task_type = \"gaze-detection\"\n        self.log(f\"GAZE model loaded in {perf_counter() - t1:.2f} seconds\")\n\n    def _crop_face_img(self, np_img: np.ndarray, face: Detection) -&gt; np.ndarray:\n\"\"\"Extract facial area in an image.\n\n        Args:\n            np_img (np.ndarray): The numpy image.\n            face (mediapipe.tasks.python.components.containers.detections.Detection): The detected face.\n\n        Returns:\n            np.ndarray: Cropped face image.\n        \"\"\"\n        # extract face area\n        bbox = face.bounding_box\n        x_min = bbox.origin_x\n        y_min = bbox.origin_y\n        x_max = bbox.origin_x + bbox.width\n        y_max = bbox.origin_y + bbox.height\n        face_img = np_img[y_min:y_max, x_min:x_max, :]\n        face_img = cv2.resize(face_img, (224, 224))\n        return face_img\n\n    def _detect_gaze(self, np_imgs: List[np.ndarray]) -&gt; List[Tuple[float, float]]:\n\"\"\"Detect faces and gazes in an image.\n\n        Args:\n            pil_imgs (List[np.ndarray]): The numpy image list, each image is a cropped facial image.\n\n        Returns:\n            List[Tuple[float, float]]: Yaw (radian) and Pitch (radian).\n        \"\"\"\n        ret = []\n        for i in range(0, len(np_imgs), GAZE_MAX_BATCH_SIZE):\n            img_batch = []\n            for j in range(i, min(len(np_imgs), i + GAZE_MAX_BATCH_SIZE)):\n                img = self._gaze_transformations(np_imgs[j])\n                img = np.expand_dims(img, axis=0).astype(np.float32)\n                img_batch.append(img)\n\n            img_batch = np.concatenate(img_batch, axis=0)\n            onnx_input_image = {self.gaze_onnx_session.get_inputs()[0].name: img_batch}\n            yaw, pitch = self.gaze_onnx_session.run(None, onnx_input_image)\n\n            for j in range(len(img_batch)):\n                ret.append((yaw[j], pitch[j]))\n\n        return ret\n\n    def _make_response(\n        self,\n        faces: List[Detection],\n        gazes: List[Tuple[float, float]],\n        imgW: int,\n        imgH: int,\n        time_total: float,\n        time_face_det: float = None,\n        time_gaze_det: float = None,\n    ) -&gt; GazeDetectionInferenceResponse:\n\"\"\"Prepare response object from detected faces and corresponding gazes.\n\n        Args:\n            faces (List[Detection]): The detected faces.\n            gazes (List[tuple(float, float)]): The detected gazes (yaw, pitch).\n            imgW (int): The width (px) of original image.\n            imgH (int): The height (px) of original image.\n            time_total (float): The processing time.\n            time_face_det (float): The processing time.\n            time_gaze_det (float): The processing time.\n\n        Returns:\n            GazeDetectionInferenceResponse: The response object including the detected faces and gazes info.\n        \"\"\"\n        predictions = []\n        for face, gaze in zip(faces, gazes):\n            landmarks = []\n            for keypoint in face.keypoints:\n                x = min(max(int(keypoint.x * imgW), 0), imgW - 1)\n                y = min(max(int(keypoint.y * imgH), 0), imgH - 1)\n                landmarks.append(Point(x=x, y=y))\n\n            bbox = face.bounding_box\n            x_center = bbox.origin_x + bbox.width / 2\n            y_center = bbox.origin_y + bbox.height / 2\n            score = face.categories[0].score\n\n            prediction = GazeDetectionPrediction(\n                face=FaceDetectionPrediction(\n                    x=x_center,\n                    y=y_center,\n                    width=bbox.width,\n                    height=bbox.height,\n                    confidence=score,\n                    class_name=\"face\",\n                    landmarks=landmarks,\n                ),\n                yaw=gaze[0],\n                pitch=gaze[1],\n            )\n            predictions.append(prediction)\n\n        response = GazeDetectionInferenceResponse(\n            predictions=predictions,\n            time=time_total,\n            time_face_det=time_face_det,\n            time_gaze_det=time_gaze_det,\n        )\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\n            \"mediapipe_face_detector.tflite\",\n            \"L2CSNet_gaze360_resnet50_90bins.onnx\",\n        ]\n\n    def infer_from_request(\n        self, request: GazeDetectionInferenceRequest\n    ) -&gt; List[GazeDetectionInferenceResponse]:\n\"\"\"Detect faces and gazes in image(s).\n\n        Args:\n            request (GazeDetectionInferenceRequest): The request object containing the image.\n\n        Returns:\n            List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.\n        \"\"\"\n        if isinstance(request.image, list):\n            if len(request.image) &gt; GAZE_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be inferred with gaze detection at one time is {GAZE_MAX_BATCH_SIZE}\"\n                )\n            imgs = request.image\n        else:\n            imgs = [request.image]\n\n        time_total = perf_counter()\n\n        # load pil images\n        num_img = len(imgs)\n        np_imgs = [load_image_rgb(img) for img in imgs]\n\n        # face detection\n        # TODO: face detection for batch\n        time_face_det = perf_counter()\n        faces = []\n        for np_img in np_imgs:\n            if request.do_run_face_detection:\n                mp_img = mp.Image(\n                    image_format=mp.ImageFormat.SRGB, data=np_img.astype(np.uint8)\n                )\n                faces_per_img = self.face_detector.detect(mp_img).detections\n            else:\n                faces_per_img = [\n                    Detection(\n                        bounding_box=BoundingBox(\n                            origin_x=0,\n                            origin_y=0,\n                            width=np_img.shape[1],\n                            height=np_img.shape[0],\n                        ),\n                        categories=[Category(score=1.0, category_name=\"face\")],\n                        keypoints=[],\n                    )\n                ]\n            faces.append(faces_per_img)\n        time_face_det = (perf_counter() - time_face_det) / num_img\n\n        # gaze detection\n        time_gaze_det = perf_counter()\n        face_imgs = []\n        for i, np_img in enumerate(np_imgs):\n            if request.do_run_face_detection:\n                face_imgs.extend(\n                    [self._crop_face_img(np_img, face) for face in faces[i]]\n                )\n            else:\n                face_imgs.append(cv2.resize(np_img, (224, 224)))\n        gazes = self._detect_gaze(face_imgs)\n        time_gaze_det = (perf_counter() - time_gaze_det) / num_img\n\n        time_total = (perf_counter() - time_total) / num_img\n\n        # prepare response\n        response = []\n        idx_gaze = 0\n        for i in range(len(np_imgs)):\n            imgH, imgW, _ = np_imgs[i].shape\n            faces_per_img = faces[i]\n            gazes_per_img = gazes[idx_gaze : idx_gaze + len(faces_per_img)]\n            response.append(\n                self._make_response(\n                    faces_per_img, gazes_per_img, imgW, imgH, time_total\n                )\n            )\n\n        return response\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the Gaze with the given arguments and keyword arguments.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Initializes the Gaze with the given arguments and keyword arguments.\"\"\"\n\n    t1 = perf_counter()\n    super().__init__(*args, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n\n    # TODO: convert face detector (TensorflowLite) to ONNX model\n\n    self.gaze_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"L2CSNet_gaze360_resnet50_90bins.onnx\"),\n        providers=[\n            (\n                \"TensorrtExecutionProvider\",\n                {\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                },\n            ),\n            \"CUDAExecutionProvider\",\n            \"OpenVINOExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    # init face detector\n    self.face_detector = mp.tasks.vision.FaceDetector.create_from_options(\n        mp.tasks.vision.FaceDetectorOptions(\n            base_options=mp.tasks.BaseOptions(\n                model_asset_path=self.cache_file(\"mediapipe_face_detector.tflite\")\n            ),\n            running_mode=mp.tasks.vision.RunningMode.IMAGE,\n        )\n    )\n\n    # additional settings for gaze detection\n    self._gaze_transformations = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Resize(448),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n            ),\n        ]\n    )\n    self.task_type = \"gaze-detection\"\n    self.log(f\"GAZE model loaded in {perf_counter() - t1:.2f} seconds\")\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\n        \"mediapipe_face_detector.tflite\",\n        \"L2CSNet_gaze360_resnet50_90bins.onnx\",\n    ]\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Detect faces and gazes in image(s).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GazeDetectionInferenceRequest</code> <p>The request object containing the image.</p> required <p>Returns:</p> Type Description <code>List[GazeDetectionInferenceResponse]</code> <p>List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def infer_from_request(\n    self, request: GazeDetectionInferenceRequest\n) -&gt; List[GazeDetectionInferenceResponse]:\n\"\"\"Detect faces and gazes in image(s).\n\n    Args:\n        request (GazeDetectionInferenceRequest): The request object containing the image.\n\n    Returns:\n        List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.\n    \"\"\"\n    if isinstance(request.image, list):\n        if len(request.image) &gt; GAZE_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be inferred with gaze detection at one time is {GAZE_MAX_BATCH_SIZE}\"\n            )\n        imgs = request.image\n    else:\n        imgs = [request.image]\n\n    time_total = perf_counter()\n\n    # load pil images\n    num_img = len(imgs)\n    np_imgs = [load_image_rgb(img) for img in imgs]\n\n    # face detection\n    # TODO: face detection for batch\n    time_face_det = perf_counter()\n    faces = []\n    for np_img in np_imgs:\n        if request.do_run_face_detection:\n            mp_img = mp.Image(\n                image_format=mp.ImageFormat.SRGB, data=np_img.astype(np.uint8)\n            )\n            faces_per_img = self.face_detector.detect(mp_img).detections\n        else:\n            faces_per_img = [\n                Detection(\n                    bounding_box=BoundingBox(\n                        origin_x=0,\n                        origin_y=0,\n                        width=np_img.shape[1],\n                        height=np_img.shape[0],\n                    ),\n                    categories=[Category(score=1.0, category_name=\"face\")],\n                    keypoints=[],\n                )\n            ]\n        faces.append(faces_per_img)\n    time_face_det = (perf_counter() - time_face_det) / num_img\n\n    # gaze detection\n    time_gaze_det = perf_counter()\n    face_imgs = []\n    for i, np_img in enumerate(np_imgs):\n        if request.do_run_face_detection:\n            face_imgs.extend(\n                [self._crop_face_img(np_img, face) for face in faces[i]]\n            )\n        else:\n            face_imgs.append(cv2.resize(np_img, (224, 224)))\n    gazes = self._detect_gaze(face_imgs)\n    time_gaze_det = (perf_counter() - time_gaze_det) / num_img\n\n    time_total = (perf_counter() - time_total) / num_img\n\n    # prepare response\n    response = []\n    idx_gaze = 0\n    for i in range(len(np_imgs)):\n        imgH, imgW, _ = np_imgs[i].shape\n        faces_per_img = faces[i]\n        gazes_per_img = gazes[idx_gaze : idx_gaze + len(faces_per_img)]\n        response.append(\n            self._make_response(\n                faces_per_img, gazes_per_img, imgW, imgH, time_total\n            )\n        )\n\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.L2C2Wrapper","title":"<code>L2C2Wrapper</code>","text":"<p>             Bases: <code>L2CS</code></p> <p>Roboflow L2CS Gaze detection model.</p> <p>This class is responsible for converting L2CS model to ONNX model. It is ONLY intended for internal usage.</p> Workflow <p>After training a L2CS model, create an instance of this wrapper class. Load the trained weights file, and save it as ONNX model.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>class L2C2Wrapper(L2CS):\n\"\"\"Roboflow L2CS Gaze detection model.\n\n    This class is responsible for converting L2CS model to ONNX model.\n    It is ONLY intended for internal usage.\n\n    Workflow:\n        After training a L2CS model, create an instance of this wrapper class.\n        Load the trained weights file, and save it as ONNX model.\n    \"\"\"\n\n    def __init__(self):\n        self.device = torch.device(\"cpu\")\n        self.num_bins = 90\n        super().__init__(\n            torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], self.num_bins\n        )\n        self._gaze_softmax = nn.Softmax(dim=1)\n        self._gaze_idx_tensor = torch.FloatTensor([i for i in range(90)]).to(\n            self.device\n        )\n\n    def forward(self, x):\n        idx_tensor = torch.stack(\n            [self._gaze_idx_tensor for i in range(x.shape[0])], dim=0\n        )\n        gaze_yaw, gaze_pitch = super().forward(x)\n\n        yaw_predicted = self._gaze_softmax(gaze_yaw)\n        yaw_radian = (\n            (torch.sum(yaw_predicted * idx_tensor, dim=1) * 4 - 180) * np.pi / 180\n        )\n\n        pitch_predicted = self._gaze_softmax(gaze_pitch)\n        pitch_radian = (\n            (torch.sum(pitch_predicted * idx_tensor, dim=1) * 4 - 180) * np.pi / 180\n        )\n\n        return yaw_radian, pitch_radian\n\n    def load_L2CS_model(\n        self,\n        file_path=f\"{MODEL_CACHE_DIR}/gaze/L2CS/L2CSNet_gaze360_resnet50_90bins.pkl\",\n    ):\n        super().load_state_dict(torch.load(file_path, map_location=self.device))\n        super().to(self.device)\n\n    def saveas_ONNX_model(\n        self,\n        file_path=f\"{MODEL_CACHE_DIR}/gaze/L2CS/L2CSNet_gaze360_resnet50_90bins.onnx\",\n    ):\n        dummy_input = torch.randn(1, 3, 448, 448)\n        dynamic_axes = {\n            \"input\": {0: \"batch_size\"},\n            \"output_yaw\": {0: \"batch_size\"},\n            \"output_pitch\": {0: \"batch_size\"},\n        }\n        torch.onnx.export(\n            self,\n            dummy_input,\n            file_path,\n            input_names=[\"input\"],\n            output_names=[\"output_yaw\", \"output_pitch\"],\n            dynamic_axes=dynamic_axes,\n            verbose=False,\n        )\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/l2cs/","title":"l2cs","text":""},{"location":"docs/reference/inference/models/gaze/l2cs/#inference.models.gaze.l2cs.L2CS","title":"<code>L2CS</code>","text":"<p>             Bases: <code>Module</code></p> <p>L2CS Gaze Detection Model.</p> <p>This class is responsible for performing gaze detection using the L2CS-Net model. Ref: https://github.com/Ahmednull/L2CS-Net</p> <p>Methods:</p> Name Description <code>forward</code> <p>Performs inference on the given image.</p> Source code in <code>inference/models/gaze/l2cs.py</code> <pre><code>class L2CS(nn.Module):\n\"\"\"L2CS Gaze Detection Model.\n\n    This class is responsible for performing gaze detection using the L2CS-Net model.\n    Ref: https://github.com/Ahmednull/L2CS-Net\n\n    Methods:\n        forward: Performs inference on the given image.\n    \"\"\"\n\n    def __init__(self, block, layers, num_bins):\n        self.inplanes = 64\n        super(L2CS, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.fc_yaw_gaze = nn.Linear(512 * block.expansion, num_bins)\n        self.fc_pitch_gaze = nn.Linear(512 * block.expansion, num_bins)\n\n        # Vestigial layer from previous experiments\n        self.fc_finetune = nn.Linear(512 * block.expansion + 3, 3)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n\n        # gaze\n        pre_yaw_gaze = self.fc_yaw_gaze(x)\n        pre_pitch_gaze = self.fc_pitch_gaze(x)\n        return pre_yaw_gaze, pre_pitch_gaze\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/","title":"grounding_dino","text":""},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO","title":"<code>GroundingDINO</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> <p>GroundingDINO class for zero-shot object detection.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The GroundingDINO model.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>class GroundingDINO(RoboflowCoreModel):\n\"\"\"GroundingDINO class for zero-shot object detection.\n\n    Attributes:\n        model: The GroundingDINO model.\n    \"\"\"\n\n    def __init__(\n        self, *args, model_id=\"grounding_dino/groundingdino_swint_ogc\", **kwargs\n    ):\n\"\"\"Initializes the GroundingDINO model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n        GROUNDING_DINO_CACHE_DIR = os.path.join(MODEL_CACHE_DIR, model_id)\n\n        GROUNDING_DINO_CONFIG_PATH = os.path.join(\n            GROUNDING_DINO_CACHE_DIR, \"GroundingDINO_SwinT_OGC.py\"\n        )\n\n        if not os.path.exists(GROUNDING_DINO_CACHE_DIR):\n            os.makedirs(GROUNDING_DINO_CACHE_DIR)\n\n        if not os.path.exists(GROUNDING_DINO_CONFIG_PATH):\n            url = \"https://raw.githubusercontent.com/roboflow/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n            urllib.request.urlretrieve(url, GROUNDING_DINO_CONFIG_PATH)\n\n        self.model = Model(\n            model_config_path=GROUNDING_DINO_CONFIG_PATH,\n            model_checkpoint_path=os.path.join(\n                GROUNDING_DINO_CACHE_DIR, \"groundingdino_swint_ogc.pth\"\n            ),\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        )\n        self.task_type = \"object-detection\"\n\n    def preproc_image(self, image: Any):\n\"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_bgr(image)\n        return np_image\n\n    def infer_from_request(\n        self,\n        request: GroundingDINOInferenceRequest,\n    ) -&gt; ObjectDetectionInferenceResponse:\n\"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        \"\"\"\n        result = self.infer(**request.dict())\n        return result\n\n    def infer(\n        self,\n        image: InferenceRequestImage,\n        text: list[str] = None,\n        class_filter: list = None,\n        box_threshold=0.5,\n        text_threshold=0.5,\n        class_agnostic_nms=CLASS_AGNOSTIC_NMS,\n        **kwargs\n    ):\n\"\"\"\n        Run inference on a provided image.\n            - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n        Args:\n            request (CVInferenceRequest): The inference request.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            GroundingDINOInferenceRequest: The inference response.\n        \"\"\"\n        t1 = perf_counter()\n        image = self.preproc_image(image)\n        img_dims = image.shape\n\n        detections = self.model.predict_with_classes(\n            image=image,\n            classes=text,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n        )\n\n        self.class_names = text\n\n        if class_agnostic_nms:\n            detections = detections.with_nms(class_agnostic=True)\n        else:\n            detections = detections.with_nms()\n\n        xywh_bboxes = [xyxy_to_xywh(detection) for detection in detections.xyxy]\n\n        t2 = perf_counter() - t1\n\n        responses = ObjectDetectionInferenceResponse(\n            predictions=[\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": xywh_bboxes[i][0],\n                        \"y\": xywh_bboxes[i][1],\n                        \"width\": xywh_bboxes[i][2],\n                        \"height\": xywh_bboxes[i][3],\n                        \"confidence\": detections.confidence[i],\n                        \"class\": self.class_names[int(detections.class_id[i])],\n                        \"class_id\": int(detections.class_id[i]),\n                    }\n                )\n                for i, pred in enumerate(detections.xyxy)\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n                and detections.class_id[i] is not None\n            ],\n            image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n            time=t2,\n        )\n        return responses\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"groundingdino_swint_ogc.pth\"]\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.__init__","title":"<code>__init__(*args, model_id='grounding_dino/groundingdino_swint_ogc', **kwargs)</code>","text":"<p>Initializes the GroundingDINO model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def __init__(\n    self, *args, model_id=\"grounding_dino/groundingdino_swint_ogc\", **kwargs\n):\n\"\"\"Initializes the GroundingDINO model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n\n    GROUNDING_DINO_CACHE_DIR = os.path.join(MODEL_CACHE_DIR, model_id)\n\n    GROUNDING_DINO_CONFIG_PATH = os.path.join(\n        GROUNDING_DINO_CACHE_DIR, \"GroundingDINO_SwinT_OGC.py\"\n    )\n\n    if not os.path.exists(GROUNDING_DINO_CACHE_DIR):\n        os.makedirs(GROUNDING_DINO_CACHE_DIR)\n\n    if not os.path.exists(GROUNDING_DINO_CONFIG_PATH):\n        url = \"https://raw.githubusercontent.com/roboflow/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n        urllib.request.urlretrieve(url, GROUNDING_DINO_CONFIG_PATH)\n\n    self.model = Model(\n        model_config_path=GROUNDING_DINO_CONFIG_PATH,\n        model_checkpoint_path=os.path.join(\n            GROUNDING_DINO_CACHE_DIR, \"groundingdino_swint_ogc.pth\"\n        ),\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    )\n    self.task_type = \"object-detection\"\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"groundingdino_swint_ogc.pth\"]\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.infer","title":"<code>infer(image, text=None, class_filter=None, box_threshold=0.5, text_threshold=0.5, class_agnostic_nms=CLASS_AGNOSTIC_NMS, **kwargs)</code>","text":"<p>Run inference on a provided image.     - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>CVInferenceRequest</code> <p>The inference request.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GroundingDINOInferenceRequest</code> <p>The inference response.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def infer(\n    self,\n    image: InferenceRequestImage,\n    text: list[str] = None,\n    class_filter: list = None,\n    box_threshold=0.5,\n    text_threshold=0.5,\n    class_agnostic_nms=CLASS_AGNOSTIC_NMS,\n    **kwargs\n):\n\"\"\"\n    Run inference on a provided image.\n        - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n    Args:\n        request (CVInferenceRequest): The inference request.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        GroundingDINOInferenceRequest: The inference response.\n    \"\"\"\n    t1 = perf_counter()\n    image = self.preproc_image(image)\n    img_dims = image.shape\n\n    detections = self.model.predict_with_classes(\n        image=image,\n        classes=text,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n    )\n\n    self.class_names = text\n\n    if class_agnostic_nms:\n        detections = detections.with_nms(class_agnostic=True)\n    else:\n        detections = detections.with_nms()\n\n    xywh_bboxes = [xyxy_to_xywh(detection) for detection in detections.xyxy]\n\n    t2 = perf_counter() - t1\n\n    responses = ObjectDetectionInferenceResponse(\n        predictions=[\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": xywh_bboxes[i][0],\n                    \"y\": xywh_bboxes[i][1],\n                    \"width\": xywh_bboxes[i][2],\n                    \"height\": xywh_bboxes[i][3],\n                    \"confidence\": detections.confidence[i],\n                    \"class\": self.class_names[int(detections.class_id[i])],\n                    \"class_id\": int(detections.class_id[i]),\n                }\n            )\n            for i, pred in enumerate(detections.xyxy)\n            if not class_filter\n            or self.class_names[int(pred[6])] in class_filter\n            and detections.class_id[i] is not None\n        ],\n        image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n        time=t2,\n    )\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def infer_from_request(\n    self,\n    request: GroundingDINOInferenceRequest,\n) -&gt; ObjectDetectionInferenceResponse:\n\"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    \"\"\"\n    result = self.infer(**request.dict())\n    return result\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def preproc_image(self, image: Any):\n\"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_bgr(image)\n    return np_image\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/","title":"segment_anything","text":""},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything","title":"<code>SegmentAnything</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> <p>SegmentAnything class for handling segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>sam</code> <p>The segmentation model.</p> <code>predictor</code> <p>The predictor for the segmentation model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> <code>embedding_cache</code> <p>Cache for embeddings.</p> <code>image_size_cache</code> <p>Cache for image sizes.</p> <code>embedding_cache_keys</code> <p>Keys for the embedding cache.</p> <code>low_res_logits_cache</code> <p>Cache for low resolution logits.</p> <code>segmentation_cache_keys</code> <p>Keys for the segmentation cache.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>class SegmentAnything(RoboflowCoreModel):\n\"\"\"SegmentAnything class for handling segmentation tasks.\n\n    Attributes:\n        sam: The segmentation model.\n        predictor: The predictor for the segmentation model.\n        ort_session: ONNX runtime inference session.\n        embedding_cache: Cache for embeddings.\n        image_size_cache: Cache for image sizes.\n        embedding_cache_keys: Keys for the embedding cache.\n        low_res_logits_cache: Cache for low resolution logits.\n        segmentation_cache_keys: Keys for the segmentation cache.\n    \"\"\"\n\n    def __init__(self, *args, model_id: str = f\"sam/{SAM_VERSION_ID}\", **kwargs):\n\"\"\"Initializes the SegmentAnything.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, model_id=model_id, **kwargs)\n        self.sam = sam_model_registry[self.version_id](\n            checkpoint=self.cache_file(\"encoder.pth\")\n        )\n        self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.predictor = SamPredictor(self.sam)\n        self.ort_session = onnxruntime.InferenceSession(\n            self.cache_file(\"decoder.onnx\"),\n            providers=[\n                \"CUDAExecutionProvider\",\n                \"OpenVINOExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n        self.embedding_cache = {}\n        self.image_size_cache = {}\n        self.embedding_cache_keys = []\n\n        self.low_res_logits_cache = {}\n        self.segmentation_cache_keys = []\n        self.task_type = \"unsupervised-segmentation\"\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: List of file names.\n        \"\"\"\n        return [\"encoder.pth\", \"decoder.onnx\"]\n\n    def embed_image(self, image: Any, image_id: Optional[str] = None, **kwargs):\n\"\"\"\n        Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n        the cached result will be returned.\n\n        Args:\n            image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n            image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                      with this ID. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                               and the second element is the shape (height, width) of the processed image.\n\n        Notes:\n            - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n\n        Example:\n            &gt;&gt;&gt; img_array = ... # some image array\n            &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n            (array([...]), (224, 224))\n        \"\"\"\n        if image_id and image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n            )\n        img_in = self.preproc_image(image)\n        self.predictor.set_image(img_in)\n        embedding = self.predictor.get_image_embedding().cpu().numpy()\n        if image_id:\n            self.embedding_cache[image_id] = embedding\n            self.image_size_cache[image_id] = img_in.shape[:2]\n            self.embedding_cache_keys.append(image_id)\n            if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.embedding_cache_keys.pop(0)\n                del self.embedding_cache[cache_key]\n                del self.image_size_cache[cache_key]\n        return (embedding, img_in.shape[:2])\n\n    def infer_from_request(self, request: SamInferenceRequest):\n\"\"\"Performs inference based on the request type.\n\n        Args:\n            request (SamInferenceRequest): The inference request.\n\n        Returns:\n            Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n        \"\"\"\n        t1 = perf_counter()\n        if isinstance(request, SamEmbeddingRequest):\n            embedding, _ = self.embed_image(**request.dict())\n            inference_time = perf_counter() - t1\n            if request.format == \"json\":\n                return SamEmbeddingResponse(\n                    embeddings=embedding.tolist(), time=inference_time\n                )\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.save(binary_vector, embedding)\n                binary_vector.seek(0)\n                return SamEmbeddingResponse(\n                    embeddings=binary_vector.getvalue(), time=inference_time\n                )\n        elif isinstance(request, SamSegmentationRequest):\n            masks, low_res_masks = self.segment_image(**request.dict())\n            if request.format == \"json\":\n                masks = masks &gt; self.predictor.model.mask_threshold\n                masks = masks2poly(masks)\n                low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n                low_res_masks = masks2poly(low_res_masks)\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.savez_compressed(\n                    binary_vector, masks=masks, low_res_masks=low_res_masks\n                )\n                binary_vector.seek(0)\n                binary_data = binary_vector.getvalue()\n                return binary_data\n            else:\n                raise ValueError(f\"Invalid format {request.format}\")\n\n            response = SamSegmentationResponse(\n                masks=[m.tolist() for m in masks],\n                low_res_masks=[m.tolist() for m in low_res_masks],\n                time=perf_counter() - t1,\n            )\n            return response\n\n    def preproc_image(self, image: InferenceRequestImage):\n\"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image\n\n    def segment_image(\n        self,\n        image: Any,\n        embeddings: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        embeddings_format: Optional[str] = \"json\",\n        has_mask_input: Optional[bool] = False,\n        image_id: Optional[str] = None,\n        mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n        mask_input_format: Optional[str] = \"json\",\n        orig_im_size: Optional[List[int]] = None,\n        point_coords: Optional[List[List[float]]] = [],\n        point_labels: Optional[List[int]] = [],\n        use_mask_input_cache: Optional[bool] = True,\n        **kwargs,\n    ):\n\"\"\"\n        Segments an image based on provided embeddings, points, masks, or cached results.\n        If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n        Args:\n            image (Any): The image to be segmented.\n            embeddings (Optional[Union[np.ndarray, List[List[float]]]]): The embeddings of the image.\n                Defaults to None, in which case the image is used to compute embeddings.\n            embeddings_format (Optional[str]): Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.\n            has_mask_input (Optional[bool]): Specifies whether mask input is provided. Defaults to False.\n            image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n            mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input mask for the image.\n            mask_input_format (Optional[str]): Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.\n            orig_im_size (Optional[List[int]]): Original size of the image when providing embeddings directly.\n            point_coords (Optional[List[List[float]]]): Coordinates of points in the image. Defaults to an empty list.\n            point_labels (Optional[List[int]]): Labels associated with the provided points. Defaults to an empty list.\n            use_mask_input_cache (Optional[bool]): Flag to determine if cached mask input should be used. Defaults to True.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image\n                                          and the second element is the low resolution segmentation masks.\n\n        Raises:\n            ValueError: If necessary inputs are missing or inconsistent.\n\n        Notes:\n            - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n              on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n        \"\"\"\n        if not embeddings:\n            if not image and not image_id:\n                raise ValueError(\n                    \"Must provide either image, cached image_id, or embeddings\"\n                )\n            elif image_id and not image and image_id not in self.embedding_cache:\n                raise ValueError(\n                    f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n                )\n            embedding, original_image_size = self.embed_image(\n                image=image, image_id=image_id\n            )\n        else:\n            if not orig_im_size:\n                raise ValueError(\n                    \"Must provide original image size if providing embeddings\"\n                )\n            original_image_size = orig_im_size\n            if embeddings_format == \"json\":\n                embedding = np.array(embeddings)\n            elif embeddings_format == \"binary\":\n                embedding = np.load(BytesIO(embeddings))\n\n        point_coords = point_coords\n        point_coords.append([0, 0])\n        point_coords = np.array(point_coords, dtype=np.float32)\n        point_coords = np.expand_dims(point_coords, axis=0)\n        point_coords = self.predictor.transform.apply_coords(\n            point_coords,\n            original_image_size,\n        )\n\n        point_labels = point_labels\n        point_labels.append(-1)\n        point_labels = np.array(point_labels, dtype=np.float32)\n        point_labels = np.expand_dims(point_labels, axis=0)\n\n        if has_mask_input:\n            if (\n                image_id\n                and image_id in self.low_res_logits_cache\n                and use_mask_input_cache\n            ):\n                mask_input = self.low_res_logits_cache[image_id]\n            elif not mask_input and (\n                not image_id or image_id not in self.low_res_logits_cache\n            ):\n                raise ValueError(\"Must provide either mask_input or cached image_id\")\n            else:\n                if mask_input_format == \"json\":\n                    polys = mask_input\n                    mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                    for i, poly in enumerate(polys):\n                        poly = ShapelyPolygon(poly)\n                        raster = rasterio.features.rasterize(\n                            [poly], out_shape=(256, 256)\n                        )\n                        mask_input[0, i, :, :] = raster\n                elif mask_input_format == \"binary\":\n                    binary_data = base64.b64decode(mask_input)\n                    mask_input = np.load(BytesIO(binary_data))\n        else:\n            mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n        ort_inputs = {\n            \"image_embeddings\": embedding.astype(np.float32),\n            \"point_coords\": point_coords.astype(np.float32),\n            \"point_labels\": point_labels,\n            \"mask_input\": mask_input.astype(np.float32),\n            \"has_mask_input\": (\n                np.zeros(1, dtype=np.float32)\n                if not has_mask_input\n                else np.ones(1, dtype=np.float32)\n            ),\n            \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n        }\n        masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n        if image_id:\n            self.low_res_logits_cache[image_id] = low_res_logits\n            if image_id not in self.segmentation_cache_keys:\n                self.segmentation_cache_keys.append(image_id)\n            if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.segmentation_cache_keys.pop(0)\n                del self.low_res_logits_cache[cache_key]\n        masks = masks[0]\n        low_res_masks = low_res_logits[0]\n\n        return masks, low_res_masks\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.__init__","title":"<code>__init__(*args, model_id=f'sam/{SAM_VERSION_ID}', **kwargs)</code>","text":"<p>Initializes the SegmentAnything.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def __init__(self, *args, model_id: str = f\"sam/{SAM_VERSION_ID}\", **kwargs):\n\"\"\"Initializes the SegmentAnything.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, model_id=model_id, **kwargs)\n    self.sam = sam_model_registry[self.version_id](\n        checkpoint=self.cache_file(\"encoder.pth\")\n    )\n    self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.predictor = SamPredictor(self.sam)\n    self.ort_session = onnxruntime.InferenceSession(\n        self.cache_file(\"decoder.onnx\"),\n        providers=[\n            \"CUDAExecutionProvider\",\n            \"OpenVINOExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n    self.embedding_cache = {}\n    self.image_size_cache = {}\n    self.embedding_cache_keys = []\n\n    self.low_res_logits_cache = {}\n    self.segmentation_cache_keys = []\n    self.task_type = \"unsupervised-segmentation\"\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.embed_image","title":"<code>embed_image(image, image_id=None, **kwargs)</code>","text":"<p>Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached, the cached result will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be embedded. The format should be compatible with the preproc_image method.</p> required <code>image_id</code> <code>Optional[str]</code> <p>An identifier for the image. If provided, the embedding result will be cached                       with this ID. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image                                and the second element is the shape (height, width) of the processed image.</p> Notes <ul> <li>Embeddings and image sizes are cached to improve performance on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Example <p>img_array = ... # some image array embed_image(img_array, image_id=\"sample123\") (array([...]), (224, 224))</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def embed_image(self, image: Any, image_id: Optional[str] = None, **kwargs):\n\"\"\"\n    Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n    the cached result will be returned.\n\n    Args:\n        image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n        image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                  with this ID. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                           and the second element is the shape (height, width) of the processed image.\n\n    Notes:\n        - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n\n    Example:\n        &gt;&gt;&gt; img_array = ... # some image array\n        &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n        (array([...]), (224, 224))\n    \"\"\"\n    if image_id and image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n        )\n    img_in = self.preproc_image(image)\n    self.predictor.set_image(img_in)\n    embedding = self.predictor.get_image_embedding().cpu().numpy()\n    if image_id:\n        self.embedding_cache[image_id] = embedding\n        self.image_size_cache[image_id] = img_in.shape[:2]\n        self.embedding_cache_keys.append(image_id)\n        if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.embedding_cache_keys.pop(0)\n            del self.embedding_cache[cache_key]\n            del self.image_size_cache[cache_key]\n    return (embedding, img_in.shape[:2])\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of file names.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n\"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: List of file names.\n    \"\"\"\n    return [\"encoder.pth\", \"decoder.onnx\"]\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Performs inference based on the request type.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Type Description <p>Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def infer_from_request(self, request: SamInferenceRequest):\n\"\"\"Performs inference based on the request type.\n\n    Args:\n        request (SamInferenceRequest): The inference request.\n\n    Returns:\n        Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n    \"\"\"\n    t1 = perf_counter()\n    if isinstance(request, SamEmbeddingRequest):\n        embedding, _ = self.embed_image(**request.dict())\n        inference_time = perf_counter() - t1\n        if request.format == \"json\":\n            return SamEmbeddingResponse(\n                embeddings=embedding.tolist(), time=inference_time\n            )\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.save(binary_vector, embedding)\n            binary_vector.seek(0)\n            return SamEmbeddingResponse(\n                embeddings=binary_vector.getvalue(), time=inference_time\n            )\n    elif isinstance(request, SamSegmentationRequest):\n        masks, low_res_masks = self.segment_image(**request.dict())\n        if request.format == \"json\":\n            masks = masks &gt; self.predictor.model.mask_threshold\n            masks = masks2poly(masks)\n            low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n            low_res_masks = masks2poly(low_res_masks)\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.savez_compressed(\n                binary_vector, masks=masks, low_res_masks=low_res_masks\n            )\n            binary_vector.seek(0)\n            binary_data = binary_vector.getvalue()\n            return binary_data\n        else:\n            raise ValueError(f\"Invalid format {request.format}\")\n\n        response = SamSegmentationResponse(\n            masks=[m.tolist() for m in masks],\n            low_res_masks=[m.tolist() for m in low_res_masks],\n            time=perf_counter() - t1,\n        )\n        return response\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage):\n\"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.segment_image","title":"<code>segment_image(image, embeddings=None, embeddings_format='json', has_mask_input=False, image_id=None, mask_input=None, mask_input_format='json', orig_im_size=None, point_coords=[], point_labels=[], use_mask_input_cache=True, **kwargs)</code>","text":"<p>Segments an image based on provided embeddings, points, masks, or cached results. If embeddings are not directly provided, the function can derive them from the input image or cache.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be segmented.</p> required <code>embeddings</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>The embeddings of the image. Defaults to None, in which case the image is used to compute embeddings.</p> <code>None</code> <code>embeddings_format</code> <code>Optional[str]</code> <p>Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.</p> <code>'json'</code> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Specifies whether mask input is provided. Defaults to False.</p> <code>False</code> <code>image_id</code> <code>Optional[str]</code> <p>A cached identifier for the image. Useful for accessing cached embeddings or masks.</p> <code>None</code> <code>mask_input</code> <code>Optional[Union[ndarray, List[List[List[float]]]]]</code> <p>Input mask for the image.</p> <code>None</code> <code>mask_input_format</code> <code>Optional[str]</code> <p>Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.</p> <code>'json'</code> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>Original size of the image when providing embeddings directly.</p> <code>None</code> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>Coordinates of points in the image. Defaults to an empty list.</p> <code>[]</code> <code>point_labels</code> <code>Optional[List[int]]</code> <p>Labels associated with the provided points. Defaults to an empty list.</p> <code>[]</code> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Flag to determine if cached mask input should be used. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image                           and the second element is the low resolution segmentation masks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary inputs are missing or inconsistent.</p> Notes <ul> <li>Embeddings, segmentations, and low-resolution logits can be cached to improve performance   on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def segment_image(\n    self,\n    image: Any,\n    embeddings: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    embeddings_format: Optional[str] = \"json\",\n    has_mask_input: Optional[bool] = False,\n    image_id: Optional[str] = None,\n    mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n    mask_input_format: Optional[str] = \"json\",\n    orig_im_size: Optional[List[int]] = None,\n    point_coords: Optional[List[List[float]]] = [],\n    point_labels: Optional[List[int]] = [],\n    use_mask_input_cache: Optional[bool] = True,\n    **kwargs,\n):\n\"\"\"\n    Segments an image based on provided embeddings, points, masks, or cached results.\n    If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n    Args:\n        image (Any): The image to be segmented.\n        embeddings (Optional[Union[np.ndarray, List[List[float]]]]): The embeddings of the image.\n            Defaults to None, in which case the image is used to compute embeddings.\n        embeddings_format (Optional[str]): Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.\n        has_mask_input (Optional[bool]): Specifies whether mask input is provided. Defaults to False.\n        image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n        mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input mask for the image.\n        mask_input_format (Optional[str]): Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.\n        orig_im_size (Optional[List[int]]): Original size of the image when providing embeddings directly.\n        point_coords (Optional[List[List[float]]]): Coordinates of points in the image. Defaults to an empty list.\n        point_labels (Optional[List[int]]): Labels associated with the provided points. Defaults to an empty list.\n        use_mask_input_cache (Optional[bool]): Flag to determine if cached mask input should be used. Defaults to True.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image\n                                      and the second element is the low resolution segmentation masks.\n\n    Raises:\n        ValueError: If necessary inputs are missing or inconsistent.\n\n    Notes:\n        - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n          on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n    \"\"\"\n    if not embeddings:\n        if not image and not image_id:\n            raise ValueError(\n                \"Must provide either image, cached image_id, or embeddings\"\n            )\n        elif image_id and not image and image_id not in self.embedding_cache:\n            raise ValueError(\n                f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n            )\n        embedding, original_image_size = self.embed_image(\n            image=image, image_id=image_id\n        )\n    else:\n        if not orig_im_size:\n            raise ValueError(\n                \"Must provide original image size if providing embeddings\"\n            )\n        original_image_size = orig_im_size\n        if embeddings_format == \"json\":\n            embedding = np.array(embeddings)\n        elif embeddings_format == \"binary\":\n            embedding = np.load(BytesIO(embeddings))\n\n    point_coords = point_coords\n    point_coords.append([0, 0])\n    point_coords = np.array(point_coords, dtype=np.float32)\n    point_coords = np.expand_dims(point_coords, axis=0)\n    point_coords = self.predictor.transform.apply_coords(\n        point_coords,\n        original_image_size,\n    )\n\n    point_labels = point_labels\n    point_labels.append(-1)\n    point_labels = np.array(point_labels, dtype=np.float32)\n    point_labels = np.expand_dims(point_labels, axis=0)\n\n    if has_mask_input:\n        if (\n            image_id\n            and image_id in self.low_res_logits_cache\n            and use_mask_input_cache\n        ):\n            mask_input = self.low_res_logits_cache[image_id]\n        elif not mask_input and (\n            not image_id or image_id not in self.low_res_logits_cache\n        ):\n            raise ValueError(\"Must provide either mask_input or cached image_id\")\n        else:\n            if mask_input_format == \"json\":\n                polys = mask_input\n                mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                for i, poly in enumerate(polys):\n                    poly = ShapelyPolygon(poly)\n                    raster = rasterio.features.rasterize(\n                        [poly], out_shape=(256, 256)\n                    )\n                    mask_input[0, i, :, :] = raster\n            elif mask_input_format == \"binary\":\n                binary_data = base64.b64decode(mask_input)\n                mask_input = np.load(BytesIO(binary_data))\n    else:\n        mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n    ort_inputs = {\n        \"image_embeddings\": embedding.astype(np.float32),\n        \"point_coords\": point_coords.astype(np.float32),\n        \"point_labels\": point_labels,\n        \"mask_input\": mask_input.astype(np.float32),\n        \"has_mask_input\": (\n            np.zeros(1, dtype=np.float32)\n            if not has_mask_input\n            else np.ones(1, dtype=np.float32)\n        ),\n        \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n    }\n    masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n    if image_id:\n        self.low_res_logits_cache[image_id] = low_res_logits\n        if image_id not in self.segmentation_cache_keys:\n            self.segmentation_cache_keys.append(image_id)\n        if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.segmentation_cache_keys.pop(0)\n            del self.low_res_logits_cache[cache_key]\n    masks = masks[0]\n    low_res_masks = low_res_logits[0]\n\n    return masks, low_res_masks\n</code></pre>"},{"location":"docs/reference/inference/models/vit/vit_classification/","title":"vit_classification","text":""},{"location":"docs/reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification","title":"<code>VitClassification</code>","text":"<p>             Bases: <code>ClassificationBaseOnnxRoboflowInferenceModel</code></p> <p>VitClassification handles classification inference for Vision Transformer (ViT) models using ONNX.</p> Inherits <p>ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference. ClassificationMixin: Mixin class providing classification-specific methods.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>A flag that specifies if the model should handle multiclass classification.</p> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>class VitClassification(ClassificationBaseOnnxRoboflowInferenceModel):\n\"\"\"VitClassification handles classification inference\n    for Vision Transformer (ViT) models using ONNX.\n\n    Inherits:\n        ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference.\n        ClassificationMixin: Mixin class providing classification-specific methods.\n\n    Attributes:\n        multiclass (bool): A flag that specifies if the model should handle multiclass classification.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\"\"\"Initializes the VitClassification instance.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Determines the weights file to be used based on the availability of AWS keys.\n\n        If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'.\n        Otherwise, it returns the path to 'best.onnx'.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and LAMBDA:\n            return \"weights.onnx\"\n        else:\n            return \"best.onnx\"\n</code></pre>"},{"location":"docs/reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Determines the weights file to be used based on the availability of AWS keys.</p> <p>If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'. Otherwise, it returns the path to 'best.onnx'.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"docs/reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the VitClassification instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Initializes the VitClassification instance.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/","title":"yolact_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT","title":"<code>YOLACT</code>","text":"<p>             Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method)</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>class YOLACT(OnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method)\"\"\"\n\n    task_type = \"instance-segmentation\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = False,\n        confidence: float = 0.5,\n        iou_threshold: float = 0.5,\n        max_candidates: int = 3000,\n        max_detections: int = 300,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[List[dict]]:\n\"\"\"\n        Performs instance segmentation inference on a given image, post-processes the results,\n        and returns the segmented instances as dictionaries containing their properties.\n\n        Args:\n            image (Any): The image or list of images to segment.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to perform class-agnostic non-max suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for filtering weak detections. Defaults to 0.5.\n            iou_threshold (float, optional): Intersection-over-union threshold for non-max suppression. Defaults to 0.5.\n            max_candidates (int, optional): Maximum number of candidate detections to consider. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections to return after non-max suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the input image(s). Defaults to False.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains:\n                - x, y: Center coordinates of the instance.\n                - width, height: Width and height of the bounding box around the instance.\n                - class: Name of the detected class.\n                - confidence: Confidence score of the detection.\n                - points: List of points describing the segmented mask's boundary.\n                - class_id: ID corresponding to the detected class.\n            If `return_image_dims` is True, the function returns a tuple where the first element is the list of detections and the\n            second element is the list of image dimensions.\n\n        Notes:\n            - The function supports processing multiple images in a batch.\n            - If an input list of images is provided, the function returns a list of lists,\n              where each inner list corresponds to the detections for a specific image.\n            - The function internally uses an ONNX model for inference.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        if isinstance(image, list):\n            imgs_with_dims = [self.preproc_image(i) for i in image]\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n            unwrap = False\n        else:\n            img_in, img_dims = self.preproc_image(image)\n            img_dims = [img_dims]\n            unwrap = True\n\n        # IN BGR order (for some reason)\n        mean = (103.94, 116.78, 123.68)\n        std = (57.38, 57.12, 58.40)\n\n        img_in = img_in.astype(np.float32)\n\n        # Our channels are RGB, so apply mean and std accordingly\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[2]) / std[2]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[0]) / std[0]\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"im_shape\": img_in.shape,\n            }\n        )\n\n    def predict(\n        self, img_in: np.ndarray, **kwargs\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        return self.onnx_session.run(None, {self.input_name: img_in})\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n        loc_data = np.float32(predictions[0])\n        conf_data = np.float32(predictions[1])\n        mask_data = np.float32(predictions[2])\n        prior_data = np.float32(predictions[3])\n        proto_data = np.float32(predictions[4])\n\n        batch_size = loc_data.shape[0]\n        num_priors = prior_data.shape[0]\n\n        boxes = np.zeros((batch_size, num_priors, 4))\n        for batch_idx in range(batch_size):\n            boxes[batch_idx, :, :] = self.decode_predicted_bboxes(\n                loc_data[batch_idx], prior_data\n            )\n\n        conf_preds = np.reshape(\n            conf_data, (batch_size, num_priors, self.num_classes + 1)\n        )\n        class_confs = conf_preds[:, :, 1:]  # remove background class\n        box_confs = np.expand_dims(\n            np.max(class_confs, axis=2), 2\n        )  # get max conf for each box\n\n        predictions = np.concatenate((boxes, box_confs, class_confs, mask_data), axis=2)\n\n        img_in_shape = preprocess_return_metadata[\"im_shape\"]\n        predictions[:, :, 0] *= img_in_shape[2]\n        predictions[:, :, 1] *= img_in_shape[3]\n        predictions[:, :, 2] *= img_in_shape[2]\n        predictions[:, :, 3] *= img_in_shape[3]\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=kwargs[\"confidence\"],\n            iou_thresh=kwargs[\"iou_threshold\"],\n            class_agnostic=kwargs[\"class_agnostic_nms\"],\n            max_detections=kwargs[\"max_detections\"],\n            max_candidate_detections=kwargs[\"max_candidates\"],\n            num_masks=32,\n            box_format=\"xyxy\",\n        )\n        predictions = np.array(predictions)\n        batch_preds = []\n        if predictions.shape != (1, 0):\n            for batch_idx, img_dim in enumerate(preprocess_return_metadata[\"img_dims\"]):\n                boxes = predictions[batch_idx, :, :4]\n                scores = predictions[batch_idx, :, 4]\n                classes = predictions[batch_idx, :, 6]\n                masks = predictions[batch_idx, :, 7:]\n                proto = proto_data[batch_idx]\n                decoded_masks = self.decode_masks(boxes, masks, proto, img_in_shape[2:])\n                polys = masks2poly(decoded_masks)\n                infer_shape = (self.img_size_w, self.img_size_h)\n                boxes = post_process_bboxes(\n                    [boxes], infer_shape, [img_dim], self.preproc, self.resize_method\n                )[0]\n                polys = post_process_polygons(\n                    img_in_shape[2:],\n                    polys,\n                    img_dim,\n                    self.preproc,\n                    resize_method=self.resize_method,\n                )\n                preds = []\n                for box, poly, score, cls in zip(boxes, polys, scores, classes):\n                    confidence = float(score)\n                    class_name = self.class_names[int(cls)]\n                    points = [{\"x\": round(x, 1), \"y\": round(y, 1)} for (x, y) in poly]\n                    pred = {\n                        \"x\": round((box[2] + box[0]) / 2, 1),\n                        \"y\": round((box[3] + box[1]) / 2, 1),\n                        \"width\": int(box[2] - box[0]),\n                        \"height\": int(box[3] - box[1]),\n                        \"class\": class_name,\n                        \"confidence\": round(confidence, 3),\n                        \"points\": points,\n                        \"class_id\": int(cls),\n                    }\n                    preds.append(pred)\n                batch_preds.append(preds)\n        else:\n            batch_preds.append([])\n        img_dims = preprocess_return_metadata[\"img_dims\"]\n        responses = self.make_response(batch_preds, img_dims, **kwargs)\n        if kwargs[\"return_image_dims\"]:\n            return responses, preprocess_return_metadata[\"img_dims\"]\n        else:\n            return responses\n\n    def make_response(\n        self,\n        predictions: List[List[dict]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: List[str] = None,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n\"\"\"\n        Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions\n        and image dimensions, optionally filtering by class name.\n\n        Args:\n            predictions (List[List[dict]]): A list containing batch predictions, where each inner list contains\n                dictionaries of segmented instances for a given image.\n            img_dims (List[Tuple[int, int]]): List of tuples specifying the dimensions of each image in the format\n                (height, width).\n            class_filter (List[str], optional): A list of class names to filter the predictions by. If not provided,\n                all predictions are included.\n\n        Returns:\n            List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered\n            predictions and corresponding image dimensions for a given image.\n\n        Examples:\n            &gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n            &gt;&gt;&gt; img_dims = [(300, 400), ...]\n            &gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n            &gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n            1\n        \"\"\"\n        responses = [\n            InstanceSegmentationInferenceResponse(\n                predictions=[\n                    InstanceSegmentationPrediction(**p)\n                    for p in batch_pred\n                    if not class_filter or p[\"class_name\"] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[i][1], height=img_dims[i][0]\n                ),\n            )\n            for i, batch_pred in enumerate(predictions)\n        ]\n        return responses\n\n    def decode_masks(self, boxes, masks, proto, img_dim):\n\"\"\"Decodes the masks from the given parameters.\n\n        Args:\n            boxes (np.array): Bounding boxes.\n            masks (np.array): Masks.\n            proto (np.array): Proto data.\n            img_dim (tuple): Image dimensions.\n\n        Returns:\n            np.array: Decoded masks.\n        \"\"\"\n        ret_mask = np.matmul(proto, np.transpose(masks))\n        ret_mask = 1 / (1 + np.exp(-ret_mask))\n        w, h, _ = ret_mask.shape\n        gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n        pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n        top, left = int(pad[1]), int(pad[0])  # y, x\n        bottom, right = int(h - pad[1]), int(w - pad[0])\n        ret_mask = np.transpose(ret_mask, (2, 0, 1))\n        ret_mask = ret_mask[:, top:bottom, left:right]\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=0)\n        ret_mask = ret_mask.transpose((1, 2, 0))\n        ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=2)\n        ret_mask = ret_mask.transpose((2, 0, 1))\n        ret_mask = crop_mask(ret_mask, boxes)  # CHW\n        ret_mask[ret_mask &lt; 0.5] = 0\n\n        return ret_mask\n\n    def decode_predicted_bboxes(self, loc, priors):\n\"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n        Args:\n            loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n            priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n        Returns:\n            np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n        \"\"\"\n\n        variances = [0.1, 0.2]\n\n        boxes = np.concatenate(\n            [\n                priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n                priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n            ],\n            1,\n        )\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n\n        return boxes\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.decode_masks","title":"<code>decode_masks(boxes, masks, proto, img_dim)</code>","text":"<p>Decodes the masks from the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>array</code> <p>Bounding boxes.</p> required <code>masks</code> <code>array</code> <p>Masks.</p> required <code>proto</code> <code>array</code> <p>Proto data.</p> required <code>img_dim</code> <code>tuple</code> <p>Image dimensions.</p> required <p>Returns:</p> Type Description <p>np.array: Decoded masks.</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_masks(self, boxes, masks, proto, img_dim):\n\"\"\"Decodes the masks from the given parameters.\n\n    Args:\n        boxes (np.array): Bounding boxes.\n        masks (np.array): Masks.\n        proto (np.array): Proto data.\n        img_dim (tuple): Image dimensions.\n\n    Returns:\n        np.array: Decoded masks.\n    \"\"\"\n    ret_mask = np.matmul(proto, np.transpose(masks))\n    ret_mask = 1 / (1 + np.exp(-ret_mask))\n    w, h, _ = ret_mask.shape\n    gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n    pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n    top, left = int(pad[1]), int(pad[0])  # y, x\n    bottom, right = int(h - pad[1]), int(w - pad[0])\n    ret_mask = np.transpose(ret_mask, (2, 0, 1))\n    ret_mask = ret_mask[:, top:bottom, left:right]\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=0)\n    ret_mask = ret_mask.transpose((1, 2, 0))\n    ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=2)\n    ret_mask = ret_mask.transpose((2, 0, 1))\n    ret_mask = crop_mask(ret_mask, boxes)  # CHW\n    ret_mask[ret_mask &lt; 0.5] = 0\n\n    return ret_mask\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.decode_predicted_bboxes","title":"<code>decode_predicted_bboxes(loc, priors)</code>","text":"<p>Decode predicted bounding box coordinates using the scheme employed by Yolov2.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>array</code> <p>The predicted bounding boxes of size [num_priors, 4].</p> required <code>priors</code> <code>array</code> <p>The prior box coordinates with size [num_priors, 4].</p> required <p>Returns:</p> Type Description <p>np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_predicted_bboxes(self, loc, priors):\n\"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n    Args:\n        loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n        priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n    Returns:\n        np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n    \"\"\"\n\n    variances = [0.1, 0.2]\n\n    boxes = np.concatenate(\n        [\n            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n        ],\n        1,\n    )\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n\n    return boxes\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.infer","title":"<code>infer(image, class_agnostic_nms=False, confidence=0.5, iou_threshold=0.5, max_candidates=3000, max_detections=300, return_image_dims=False, **kwargs)</code>","text":"<p>Performs instance segmentation inference on a given image, post-processes the results, and returns the segmented instances as dictionaries containing their properties.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to segment. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to perform class-agnostic non-max suppression. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering weak detections. Defaults to 0.5.</p> <code>0.5</code> <code>iou_threshold</code> <code>float</code> <p>Intersection-over-union threshold for non-max suppression. Defaults to 0.5.</p> <code>0.5</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections to consider. Defaults to 3000.</p> <code>3000</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections to return after non-max suppression. Defaults to 300.</p> <code>300</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the input image(s). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains: - x, y: Center coordinates of the instance. - width, height: Width and height of the bounding box around the instance. - class: Name of the detected class. - confidence: Confidence score of the detection. - points: List of points describing the segmented mask's boundary. - class_id: ID corresponding to the detected class.</p> <code>List[List[dict]]</code> <p>If <code>return_image_dims</code> is True, the function returns a tuple where the first element is the list of detections and the</p> <code>List[List[dict]]</code> <p>second element is the list of image dimensions.</p> Notes <ul> <li>The function supports processing multiple images in a batch.</li> <li>If an input list of images is provided, the function returns a list of lists,   where each inner list corresponds to the detections for a specific image.</li> <li>The function internally uses an ONNX model for inference.</li> </ul> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = False,\n    confidence: float = 0.5,\n    iou_threshold: float = 0.5,\n    max_candidates: int = 3000,\n    max_detections: int = 300,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[List[dict]]:\n\"\"\"\n    Performs instance segmentation inference on a given image, post-processes the results,\n    and returns the segmented instances as dictionaries containing their properties.\n\n    Args:\n        image (Any): The image or list of images to segment.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to perform class-agnostic non-max suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for filtering weak detections. Defaults to 0.5.\n        iou_threshold (float, optional): Intersection-over-union threshold for non-max suppression. Defaults to 0.5.\n        max_candidates (int, optional): Maximum number of candidate detections to consider. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections to return after non-max suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the input image(s). Defaults to False.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains:\n            - x, y: Center coordinates of the instance.\n            - width, height: Width and height of the bounding box around the instance.\n            - class: Name of the detected class.\n            - confidence: Confidence score of the detection.\n            - points: List of points describing the segmented mask's boundary.\n            - class_id: ID corresponding to the detected class.\n        If `return_image_dims` is True, the function returns a tuple where the first element is the list of detections and the\n        second element is the list of image dimensions.\n\n    Notes:\n        - The function supports processing multiple images in a batch.\n        - If an input list of images is provided, the function returns a list of lists,\n          where each inner list corresponds to the detections for a specific image.\n        - The function internally uses an ONNX model for inference.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, **kwargs)</code>","text":"<p>Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions and image dimensions, optionally filtering by class name.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[dict]]</code> <p>A list containing batch predictions, where each inner list contains dictionaries of segmented instances for a given image.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>List of tuples specifying the dimensions of each image in the format (height, width).</p> required <code>class_filter</code> <code>List[str]</code> <p>A list of class names to filter the predictions by. If not provided, all predictions are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[InstanceSegmentationInferenceResponse]</code> <p>List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered</p> <code>List[InstanceSegmentationInferenceResponse]</code> <p>predictions and corresponding image dimensions for a given image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n&gt;&gt;&gt; img_dims = [(300, 400), ...]\n&gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n&gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n1\n</code></pre> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[dict]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: List[str] = None,\n    **kwargs,\n) -&gt; List[InstanceSegmentationInferenceResponse]:\n\"\"\"\n    Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions\n    and image dimensions, optionally filtering by class name.\n\n    Args:\n        predictions (List[List[dict]]): A list containing batch predictions, where each inner list contains\n            dictionaries of segmented instances for a given image.\n        img_dims (List[Tuple[int, int]]): List of tuples specifying the dimensions of each image in the format\n            (height, width).\n        class_filter (List[str], optional): A list of class names to filter the predictions by. If not provided,\n            all predictions are included.\n\n    Returns:\n        List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered\n        predictions and corresponding image dimensions for a given image.\n\n    Examples:\n        &gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n        &gt;&gt;&gt; img_dims = [(300, 400), ...]\n        &gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n        &gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n        1\n    \"\"\"\n    responses = [\n        InstanceSegmentationInferenceResponse(\n            predictions=[\n                InstanceSegmentationPrediction(**p)\n                for p in batch_pred\n                if not class_filter or p[\"class_name\"] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[i][1], height=img_dims[i][0]\n            ),\n        )\n        for i, batch_pred in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/","title":"yolo_world","text":""},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld","title":"<code>YOLOWorld</code>","text":"<p>             Bases: <code>RoboflowCoreModel</code></p> <p>YOLO-World class for zero-shot object detection.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The YOLO-World model.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>class YOLOWorld(RoboflowCoreModel):\n\"\"\"YOLO-World class for zero-shot object detection.\n\n    Attributes:\n        model: The YOLO-World model.\n    \"\"\"\n\n    task_type = \"object-detection\"\n\n    def __init__(self, *args, model_id=\"yolo_world/l\", **kwargs):\n\"\"\"Initializes the YOLO-World model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n        self.model = YOLO(self.cache_file(\"yolo-world.pt\"))\n        logger.debug(\"Loading CLIP ViT-B/32\")\n        clip_model = Clip(model_id=\"clip/ViT-B-32\")\n        logger.debug(\"CLIP loaded\")\n        self.clip_model = clip_model\n        self.class_names = None\n\n    def preproc_image(self, image: Any):\n\"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image[:, :, ::-1]\n\n    def infer_from_request(\n        self,\n        request: YOLOWorldInferenceRequest,\n    ) -&gt; ObjectDetectionInferenceResponse:\n\"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        \"\"\"\n        result = self.infer(**request.dict())\n        return result\n\n    def infer(\n        self,\n        image: Any = None,\n        text: list = None,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: Optional[int] = DEFAUlT_MAX_DETECTIONS,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        **kwargs,\n    ):\n\"\"\"\n        Run inference on a provided image.\n\n        Args:\n            image - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            GroundingDINOInferenceRequest: The inference response.\n        \"\"\"\n        logger.debug(\"YOLOWorld infer() - image preprocessing.\")\n        t1 = perf_counter()\n        image = self.preproc_image(image)\n        logger.debug(\"YOLOWorld infer() - image ready.\")\n        img_dims = image.shape\n\n        if text is not None and text != self.class_names:\n            logger.debug(\"YOLOWorld infer() - classes embeddings are calculated.\")\n            self.set_classes(text)\n            logger.debug(\"YOLOWorld infer() - classes embeddings are ready.\")\n        if self.class_names is None:\n            raise ValueError(\n                \"Class names not set and not provided in the request. Must set class names before inference or provide them via the argument `text`.\"\n            )\n        logger.debug(\"YOLOWorld infer() - prediction starts.\")\n        results = self.model.predict(\n            image,\n            conf=confidence,\n            verbose=False,\n        )[0]\n        logger.debug(\"YOLOWorld infer() - predictions ready.\")\n        t2 = perf_counter() - t1\n\n        logger.debug(\"YOLOWorld infer() - post-processing starting\")\n        if len(results) &gt; 0:\n            bbox_array = np.array([box.xywh.tolist()[0] for box in results.boxes])\n            conf_array = np.array([[float(box.conf)] for box in results.boxes])\n            cls_array = np.array(\n                [self.get_cls_conf_array(int(box.cls)) for box in results.boxes]\n            )\n\n            pred_array = np.concatenate([bbox_array, conf_array, cls_array], axis=1)\n            pred_array = np.expand_dims(pred_array, axis=0)\n            pred_array = w_np_non_max_suppression(\n                pred_array,\n                conf_thresh=confidence,\n                iou_thresh=iou_threshold,\n                class_agnostic=class_agnostic_nms,\n                max_detections=max_detections,\n                max_candidate_detections=max_candidates,\n                box_format=\"xywh\",\n            )[0]\n        else:\n            pred_array = []\n        predictions = []\n        logger.debug(\"YOLOWorld infer() - post-processing done\")\n        for i, pred in enumerate(pred_array):\n            predictions.append(\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n            )\n\n        responses = ObjectDetectionInferenceResponse(\n            predictions=predictions,\n            image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n            time=t2,\n        )\n        return responses\n\n    def set_classes(self, text: list):\n\"\"\"Set the class names for the model.\n\n        Args:\n            text (list): The class names.\n        \"\"\"\n        class_names_to_calculate_embeddings = []\n        classes_embeddings = {}\n        for class_name in text:\n            class_name_hash = f\"clip-embedding:{get_text_hash(text=class_name)}\"\n            embedding_for_class = cache.get_numpy(class_name_hash)\n            if embedding_for_class is not None:\n                logger.debug(f\"Cache hit for class: {class_name}\")\n                classes_embeddings[class_name] = embedding_for_class\n            else:\n                logger.debug(f\"Cache miss for class: {class_name}\")\n                class_names_to_calculate_embeddings.append(class_name)\n        if len(class_names_to_calculate_embeddings) &gt; 0:\n            logger.debug(\n                f\"Calculating CLIP embeddings for {len(class_names_to_calculate_embeddings)} class names\"\n            )\n            cache_miss_embeddings = self.clip_model.embed_text(\n                text=class_names_to_calculate_embeddings\n            )\n        else:\n            cache_miss_embeddings = []\n        for missing_class_name, calculated_embedding in zip(\n            class_names_to_calculate_embeddings, cache_miss_embeddings\n        ):\n            classes_embeddings[missing_class_name] = calculated_embedding\n            missing_class_name_hash = (\n                f\"clip-embedding:{get_text_hash(text=missing_class_name)}\"\n            )\n            cache.set_numpy(  # caching vectors of shape (512,)\n                missing_class_name_hash,\n                calculated_embedding,\n                expire=EMBEDDINGS_EXPIRE_TIMEOUT,\n            )\n        embeddings_in_order = np.stack(\n            [classes_embeddings[class_name] for class_name in text], axis=0\n        )\n        txt_feats = torch.from_numpy(embeddings_in_order)\n        txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n        self.model.model.txt_feats = txt_feats.reshape(\n            -1, len(text), txt_feats.shape[-1]\n        ).detach()\n        self.model.model.model[-1].nc = len(text)\n        self.class_names = text\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"yolo-world.pt\"]\n\n    def get_cls_conf_array(self, class_id) -&gt; list:\n        arr = [0] * len(self.class_names)\n        arr[class_id] = 1\n        return arr\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.__init__","title":"<code>__init__(*args, model_id='yolo_world/l', **kwargs)</code>","text":"<p>Initializes the YOLO-World model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def __init__(self, *args, model_id=\"yolo_world/l\", **kwargs):\n\"\"\"Initializes the YOLO-World model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n\n    self.model = YOLO(self.cache_file(\"yolo-world.pt\"))\n    logger.debug(\"Loading CLIP ViT-B/32\")\n    clip_model = Clip(model_id=\"clip/ViT-B-32\")\n    logger.debug(\"CLIP loaded\")\n    self.clip_model = clip_model\n    self.class_names = None\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n\"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"yolo-world.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.infer","title":"<code>infer(image=None, text=None, confidence=DEFAULT_CONFIDENCE, max_detections=DEFAUlT_MAX_DETECTIONS, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, **kwargs)</code>","text":"<p>Run inference on a provided image.</p> <p>Parameters:</p> Name Type Description Default <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> required <p>Returns:</p> Name Type Description <code>GroundingDINOInferenceRequest</code> <p>The inference response.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def infer(\n    self,\n    image: Any = None,\n    text: list = None,\n    confidence: float = DEFAULT_CONFIDENCE,\n    max_detections: Optional[int] = DEFAUlT_MAX_DETECTIONS,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    **kwargs,\n):\n\"\"\"\n    Run inference on a provided image.\n\n    Args:\n        image - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        GroundingDINOInferenceRequest: The inference response.\n    \"\"\"\n    logger.debug(\"YOLOWorld infer() - image preprocessing.\")\n    t1 = perf_counter()\n    image = self.preproc_image(image)\n    logger.debug(\"YOLOWorld infer() - image ready.\")\n    img_dims = image.shape\n\n    if text is not None and text != self.class_names:\n        logger.debug(\"YOLOWorld infer() - classes embeddings are calculated.\")\n        self.set_classes(text)\n        logger.debug(\"YOLOWorld infer() - classes embeddings are ready.\")\n    if self.class_names is None:\n        raise ValueError(\n            \"Class names not set and not provided in the request. Must set class names before inference or provide them via the argument `text`.\"\n        )\n    logger.debug(\"YOLOWorld infer() - prediction starts.\")\n    results = self.model.predict(\n        image,\n        conf=confidence,\n        verbose=False,\n    )[0]\n    logger.debug(\"YOLOWorld infer() - predictions ready.\")\n    t2 = perf_counter() - t1\n\n    logger.debug(\"YOLOWorld infer() - post-processing starting\")\n    if len(results) &gt; 0:\n        bbox_array = np.array([box.xywh.tolist()[0] for box in results.boxes])\n        conf_array = np.array([[float(box.conf)] for box in results.boxes])\n        cls_array = np.array(\n            [self.get_cls_conf_array(int(box.cls)) for box in results.boxes]\n        )\n\n        pred_array = np.concatenate([bbox_array, conf_array, cls_array], axis=1)\n        pred_array = np.expand_dims(pred_array, axis=0)\n        pred_array = w_np_non_max_suppression(\n            pred_array,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            box_format=\"xywh\",\n        )[0]\n    else:\n        pred_array = []\n    predictions = []\n    logger.debug(\"YOLOWorld infer() - post-processing done\")\n    for i, pred in enumerate(pred_array):\n        predictions.append(\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": (pred[0] + pred[2]) / 2,\n                    \"y\": (pred[1] + pred[3]) / 2,\n                    \"width\": pred[2] - pred[0],\n                    \"height\": pred[3] - pred[1],\n                    \"confidence\": pred[4],\n                    \"class\": self.class_names[int(pred[6])],\n                    \"class_id\": int(pred[6]),\n                }\n            )\n        )\n\n    responses = ObjectDetectionInferenceResponse(\n        predictions=predictions,\n        image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n        time=t2,\n    )\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def infer_from_request(\n    self,\n    request: YOLOWorldInferenceRequest,\n) -&gt; ObjectDetectionInferenceResponse:\n\"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    \"\"\"\n    result = self.infer(**request.dict())\n    return result\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def preproc_image(self, image: Any):\n\"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image[:, :, ::-1]\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.set_classes","title":"<code>set_classes(text)</code>","text":"<p>Set the class names for the model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>list</code> <p>The class names.</p> required Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def set_classes(self, text: list):\n\"\"\"Set the class names for the model.\n\n    Args:\n        text (list): The class names.\n    \"\"\"\n    class_names_to_calculate_embeddings = []\n    classes_embeddings = {}\n    for class_name in text:\n        class_name_hash = f\"clip-embedding:{get_text_hash(text=class_name)}\"\n        embedding_for_class = cache.get_numpy(class_name_hash)\n        if embedding_for_class is not None:\n            logger.debug(f\"Cache hit for class: {class_name}\")\n            classes_embeddings[class_name] = embedding_for_class\n        else:\n            logger.debug(f\"Cache miss for class: {class_name}\")\n            class_names_to_calculate_embeddings.append(class_name)\n    if len(class_names_to_calculate_embeddings) &gt; 0:\n        logger.debug(\n            f\"Calculating CLIP embeddings for {len(class_names_to_calculate_embeddings)} class names\"\n        )\n        cache_miss_embeddings = self.clip_model.embed_text(\n            text=class_names_to_calculate_embeddings\n        )\n    else:\n        cache_miss_embeddings = []\n    for missing_class_name, calculated_embedding in zip(\n        class_names_to_calculate_embeddings, cache_miss_embeddings\n    ):\n        classes_embeddings[missing_class_name] = calculated_embedding\n        missing_class_name_hash = (\n            f\"clip-embedding:{get_text_hash(text=missing_class_name)}\"\n        )\n        cache.set_numpy(  # caching vectors of shape (512,)\n            missing_class_name_hash,\n            calculated_embedding,\n            expire=EMBEDDINGS_EXPIRE_TIMEOUT,\n        )\n    embeddings_in_order = np.stack(\n        [classes_embeddings[class_name] for class_name in text], axis=0\n    )\n    txt_feats = torch.from_numpy(embeddings_in_order)\n    txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n    self.model.model.txt_feats = txt_feats.reshape(\n        -1, len(text), txt_feats.shape[-1]\n    ).detach()\n    self.model.model.model[-1].nc = len(text)\n    self.class_names = text\n</code></pre>"},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/","title":"yolonas_object_detection","text":""},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/#inference.models.yolonas.yolonas_object_detection.YOLONASObjectDetection","title":"<code>YOLONASObjectDetection</code>","text":"<p>             Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> Source code in <code>inference/models/yolonas/yolonas_object_detection.py</code> <pre><code>class YOLONASObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    box_format = \"xyxy\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLO-NAS model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        boxes = predictions[0]\n        class_confs = predictions[1]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/#inference.models.yolonas.yolonas_object_detection.YOLONASObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLO-NAS model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/#inference.models.yolonas.yolonas_object_detection.YOLONASObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolonas/yolonas_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    boxes = predictions[0]\n    class_confs = predictions[1]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/","title":"yolov5_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation","title":"<code>YOLOv5InstanceSegmentation</code>","text":"<p>             Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv5 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>class YOLOv5InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n\"\"\"YOLOv5 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        return predictions[0], predictions[1]\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    return predictions[0], predictions[1]\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/","title":"yolov5_object_detection","text":""},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection","title":"<code>YOLOv5ObjectDetection</code>","text":"<p>             Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>class YOLOv5ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov7/yolov7_instance_segmentation/","title":"yolov7_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov7/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentation","title":"<code>YOLOv7InstanceSegmentation</code>","text":"<p>             Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv7 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv7 model with ONNX runtime.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>class YOLOv7InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n\"\"\"YOLOv7 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv7 model\n    with ONNX runtime.\n\n    Methods:\n        predict: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        protos = predictions[4]\n        predictions = predictions[0]\n        return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov7/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    protos = predictions[4]\n    predictions = predictions[0]\n    return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_classification/","title":"yolov8_classification","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/","title":"yolov8_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation","title":"<code>YOLOv8InstanceSegmentation</code>","text":"<p>             Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv8 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>class YOLOv8InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n\"\"\"YOLOv8 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        protos = predictions[1]\n        predictions = predictions[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:-32]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        masks = predictions[:, :, -32:]\n        predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n        return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    protos = predictions[1]\n    predictions = predictions[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:-32]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    masks = predictions[:, :, -32:]\n    predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n    return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/","title":"yolov8_keypoints_detection","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection","title":"<code>YOLOv8KeypointsDetection</code>","text":"<p>             Bases: <code>KeypointsDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX keypoints detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing keypoints detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>class YOLOv8KeypointsDetection(KeypointsDetectionBaseOnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX keypoints detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing keypoints detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        number_of_classes = len(self.get_class_names)\n        class_confs = predictions[:, :, 4 : 4 + number_of_classes]\n        keypoints_detections = predictions[:, :, 4 + number_of_classes :]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        bboxes_predictions = np.concatenate(\n            [boxes, confs, class_confs, keypoints_detections], axis=2\n        )\n        return (bboxes_predictions,)\n\n    def keypoints_count(self) -&gt; int:\n\"\"\"Returns the number of keypoints in the model.\"\"\"\n        if self.keypoints_metadata is None:\n            raise ModelArtefactError(\"Keypoints metadata not available.\")\n        return superset_keypoints_count(self.keypoints_metadata)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.keypoints_count","title":"<code>keypoints_count()</code>","text":"<p>Returns the number of keypoints in the model.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>def keypoints_count(self) -&gt; int:\n\"\"\"Returns the number of keypoints in the model.\"\"\"\n    if self.keypoints_metadata is None:\n        raise ModelArtefactError(\"Keypoints metadata not available.\")\n    return superset_keypoints_count(self.keypoints_metadata)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    number_of_classes = len(self.get_class_names)\n    class_confs = predictions[:, :, 4 : 4 + number_of_classes]\n    keypoints_detections = predictions[:, :, 4 + number_of_classes :]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    bboxes_predictions = np.concatenate(\n        [boxes, confs, class_confs, keypoints_detections], axis=2\n    )\n    return (bboxes_predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/","title":"yolov8_object_detection","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection","title":"<code>YOLOv8ObjectDetection</code>","text":"<p>             Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>class YOLOv8ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/","title":"yolov9_object_detection","text":""},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection","title":"<code>YOLOv9ObjectDetection</code>","text":"<p>             Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv9 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov9/yolov9_object_detection.py</code> <pre><code>class YOLOv9ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n\"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv9 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n\"\"\"Gets the weights file for the YOLOv9 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions.\n        \"\"\"\n        # (b x 8 x 8000)\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv9 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov9/yolov9_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n\"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions.\n    \"\"\"\n    # (b x 8 x 8000)\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"enterprise/enterprise/","title":"Enterprise Features","text":"<p>Roboflow Enterprise customers have access to advanced Inference features. These include:</p> <ol> <li>Active learning: Actively collect data from your production line for use in training new, more accurate models over time.</li> <li>Parallel processing server: Run requests in parallel to achieve increased throughput and lower latency when running inference on models.</li> <li>A license to run Inference on more than one device.</li> </ol> <p>To learn more about Roboflow's enterprise offerings, contact the sales team.</p>"},{"location":"enterprise/manage_devices/","title":"Manage devices","text":"<p>The Roboflow team is working on support for remote device management with Inference. </p> <p>To learn more about this feature, contact the Roboflow sales team.</p>"},{"location":"enterprise/parallel_processing/","title":"Parallel Inference","text":"<p>Note</p> <p>This feature is only available for Roboflow Enterprise users. Contact our sales team to learn more about Roboflow Enterprise.</p> <p>You can run multiple models in parallel with Inference with parallel processing, a version of Roboflow Inference that processes inference requests asynchronously.</p> <p>Inference Parallel supports all the same features as Roboflow Inference, with the exception that it does not support Core models (i.e. CLIP and SAM).</p> <p>With Inference Parallel, preprocessing, auto batching, inference, and post processing all run in separate threads to increase server FPS throughput.</p> <p>Separate requests to the same model will be batched on the fly as allowed by <code>$MAX_BATCH_SIZE</code>, and then response handling will occurr independently. Images are passed via Python's SharedMemory module to maximize throughput.</p> <p>These changes result in as much as a 76% speedup on one measured workload.</p>"},{"location":"enterprise/parallel_processing/#how-to-use-inference-with-parallel-processing","title":"How To Use Inference with Parallel Processing","text":"<p>You can run Inference with Parallel Processing in two ways: via the CLI or via Docker.</p> BashDocker <p>First, build the parallel server</p> <pre><code>./inference/enterprise/parallel/build.sh\n</code></pre> <p>Then, run the server:</p> <pre><code>./inference/enterprise/parallel/run.sh\n</code></pre> <p>A message will appear in the terminal indicating that the server is running and ready for use.</p> <p>We provide a container at Docker Hub that you can pull using <code>docker pull roboflow/roboflow-inference-server-gpu-parallel:latest</code>. If you are pulling a pinned tag, be sure to change the <code>$TAG</code> variable in <code>run.sh</code>.</p>"},{"location":"enterprise/parallel_processing/#benchmarking","title":"Benchmarking","text":"<p>We evaluated the performance of Inference Parallel on a variety of models from Roboflow Universe. We compared the performance of Inference Parallel to the latest version of Inference Server (0.9.5.rc) on the same hardware.</p> <p>We ran our tests on a computer with eight cores and one GPU. Instance segmentation metrics are calculated using <code>\"mask_decode_mode\": \"fast\"</code> in the request body. Requests are posted concurrently with a parallelism of 1000.</p> <p>Here are the results of our tests:</p> Workspace Model Model Type split 0.9.5.rc FPS 0.9.5.parallel FPS senior-design-project-j9gpp nbafootage/3 object-detection train 30.2 fps 44.03 fps niklas-bommersbach-jyjff dart-scorer/8 object-detection train 26.6 fps 47.0 fps geonu water-08xpr/1 instance-segmentation valid 4.7 fps 6.1 fps university-of-bradford detecting-drusen_1/2 instance-segmentation train 6.2 fps 7.2 fps fy-project-y9ecd cataract-detection-viwsu/2 classification train 48.5 fps 65.4 fps hesunyu playing-cards-ir0wr/1 classification train 44.6 fps 57.7 fps <p>Inference with parallel processing enabled achieved higher FPS on every test. On eome models, the FPS increase by using Inference with parallel processing was greater than 10 FPS.</p>"},{"location":"enterprise/stream_management_api/","title":"Stream Management","text":"<p>[!IMPORTANT]  We require a Roboflow Enterprise License to use this in production. See inference/enterpise/LICENSE.txt for details.</p>"},{"location":"enterprise/stream_management_api/#overview","title":"Overview","text":"<p>This feature is designed to cater to users requiring the execution of inference to generate predictions using Roboflow object-detection models, particularly when dealing with online video streams. It enhances the functionalities of the familiar <code>inference.Stream()</code> and <code>InferencePipeline()</code> interfaces, as found in the open-source version of the library, by introducing a sophisticated management layer. The inclusion of additional capabilities empowers users to remotely manage the state of inference pipelines through the HTTP management interface integrated into this package.</p> <p>This functionality proves beneficial in various scenarios, including but not limited to:</p> <ul> <li>Performing inference across multiple online video streams simultaneously.</li> <li>Executing inference on multiple devices that necessitate coordination.</li> <li>Establishing a monitoring layer to oversee video processing based on the <code>inference</code> package.</li> </ul>"},{"location":"enterprise/stream_management_api/#design","title":"Design","text":""},{"location":"enterprise/stream_management_api/#example-use-case","title":"Example use-case","text":"<p>Joe aims to monitor objects within the footage captured by a fleet of IP cameras installed in his factory. After successfully training an object-detection model on the Roboflow platform, he is now prepared for deployment. With four cameras in his factory, Joe opts for a model that is sufficiently compact, allowing for over 30 inferences per second on his Jetson devices. Considering this computational budget per device, Joe determines that he requires two Jetson devices to efficiently process footage from all cameras, anticipating an inference throughput of approximately 15 frames per second for each video source.</p> <p>To streamline the deployment, Joe chooses to deploy Stream Management containers to all available Jetson devices within his local network. This setup enables him to communicate with each Jetson device via HTTP, facilitating the orchestration of processing tasks. Joe develops a web app through which he can send commands to the devices and retrieve metrics regarding the statuses of the video streams.</p> <p>Finally, Joe implements a UDP server capable of receiving predictions, leveraging the <code>supervision</code> package to effectively track objects in the footage. This comprehensive approach allows Joe to manage and monitor the object-detection process seamlessly across his fleet of Jetson devices.</p>"},{"location":"enterprise/stream_management_api/#how-to-run","title":"How to run?","text":""},{"location":"enterprise/stream_management_api/#in-docker-using-docker-compose","title":"In docker - using <code>docker compose</code>","text":"<p>The most prevalent use-cases are conveniently encapsulated with Docker Compose configurations, ensuring readiness for immediate use. Nevertheless, in specific instances where custom configuration adjustments are required within Docker containers, such as passing camera devices, alternative options may prove more suitable.</p>"},{"location":"enterprise/stream_management_api/#cpu-based-devices","title":"CPU-based devices","text":"<pre><code>repository_root$ docker compose -f ./docker/dockerfiles/stream-management-api.compose-cpu.yaml up\n</code></pre>"},{"location":"enterprise/stream_management_api/#gpu-based-devices","title":"GPU-based devices","text":"<pre><code>repository_root$ docker compose -f ./docker/dockerfiles/stream-management-api.compose-gpu.yaml up\n</code></pre>"},{"location":"enterprise/stream_management_api/#jetson-devices-jetpack-511","title":"Jetson devices (<code>JetPack 5.1.1</code>)","text":"<pre><code>repository_root$ docker-compose -f ./docker/dockerfiles/stream-management-api.compose-jetson.5.1.1.yaml up\n</code></pre> <p>Disclaimer: At Jetson devices, some operations (like container bootstrap or initialisation of model) takes more time than for other ones. In particular - docker compose definition in current form do not define active awaiting TCP socket port to be opened by Stream Manager - which means that initial requests to HTTP API may be responded with HTTP 503.</p>"},{"location":"enterprise/stream_management_api/#in-docker-running-api-and-stream-manager-containers-separately","title":"In docker - running API and stream manager containers separately","text":""},{"location":"enterprise/stream_management_api/#run","title":"Run","text":""},{"location":"enterprise/stream_management_api/#cpu-based-devices_1","title":"CPU-based devices","text":"<pre><code>docker run -d --name stream_manager --network host roboflow/roboflow-inference-stream-manager-cpu:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#gpu-based-devices_1","title":"GPU-based devices","text":"<pre><code>docker run -d --name stream_manager --network host --runtime nvidia roboflow/roboflow-inference-stream-manager-gpu:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#jetson-devices-jetpack-511_1","title":"Jetson devices (<code>JetPack 5.1.1</code>)","text":"<pre><code>docker run -d --name stream_manager --network host --runtime nvidia roboflow/roboflow-inference-stream-manager-jetson-5.1.1:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#configuration-parameters","title":"Configuration parameters","text":""},{"location":"enterprise/stream_management_api/#stream-management-api","title":"Stream Management API","text":"<ul> <li><code>STREAM_MANAGER_HOST</code> - hostname for stream manager container (alter with container name if <code>--network host</code> not used   or used against remote machine)</li> <li><code>STREAM_MANAGER_PORT</code> - port to communicate with stream manager (must match with stream manager container)</li> </ul>"},{"location":"enterprise/stream_management_api/#stream-manager","title":"Stream Manager","text":"<ul> <li><code>PORT</code> - port at which server will be running</li> <li>one can mount volume under container's <code>/tmp/cache</code> to enable permanent storage of models - for faster inference   pipelines initialisation</li> <li>at the level of this container the connectivity to camera must be enabled - so if device passing to docker must   happen - it should happen at this stage</li> </ul>"},{"location":"enterprise/stream_management_api/#build-optional","title":"Build (Optional)","text":""},{"location":"enterprise/stream_management_api/#stream-management-api_1","title":"Stream Management API","text":"<pre><code>docker build -t roboflow/roboflow-inference-stream-management-api:dev -f docker/dockerfiles/Dockerfile.stream_management_api .\n</code></pre>"},{"location":"enterprise/stream_management_api/#stream-manager_1","title":"Stream Manager","text":"<pre><code>docker build -t roboflow/roboflow-inference-stream-manager-{device}:dev -f docker/dockerfiles/Dockerfile.onnx.{device}.stream_manager .\n</code></pre>"},{"location":"enterprise/stream_management_api/#bare-metal-deployment","title":"Bare-metal deployment","text":"<p>In some cases, it would be required to deploy the application at host level. This is possible, although client must resolve the environment in a way that is presented in Stream Manager and Stream Management API dockerfiles appropriate for specific platform. Once this is done the following command should be run:</p> <pre><code>repository_root$ python -m inference.enterprise.stream_management.manager.app  # runs manager\n</code></pre> <pre><code>repository_root$ python -m inference.enterprise.stream_management.api.app  # runs management API\n</code></pre>"},{"location":"enterprise/stream_management_api/#how-to-integrate","title":"How to integrate?","text":"<p>After running <code>roboflow-inference-stream-management-api</code> container, HTTP API will be available under <code>http://127.0.0.1:8080</code> (given that default configuration is used).</p> <p>One can call <code>wget http://127.0.0.1:8080/openapi.json</code> to get OpenApi specification of API that can be rendered here</p> <p>Example Python client is provided below:</p> <pre><code>import requests\nfrom typing import Optional\n\nURL = \"http://127.0.0.1:8080\"\n\ndef list_pipelines() -&gt; dict:\n    response = requests.get(f\"{URL}/list_pipelines\")\n    return response.json()\n\n\ndef get_pipeline_status(pipeline_id: str) -&gt; dict:\n    response = requests.get(f\"{URL}/status/{pipeline_id}\")\n    return response.json()\n\n\ndef pause_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/pause/{pipeline_id}\")\n    return response.json()\n\n\ndef resume_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/resume/{pipeline_id}\")\n    return response.json()\n\ndef terminate_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/terminate/{pipeline_id}\")\n    return response.json()\n\ndef initialise_pipeline(\n    video_reference: str,\n    model_id: str,\n    api_key: str,\n    sink_host: str,\n    sink_port: int,\n    max_fps: Optional[int] = None,\n) -&gt; dict:\n    response = requests.post(\n        f\"{URL}/initialise\",\n        json={\n            \"type\": \"init\",\n            \"sink_configuration\": {\n                \"type\": \"udp_sink\",\n                \"host\": sink_host,\n                \"port\": sink_port,\n            },\n            \"video_reference\": video_reference,\n            \"model_id\": model_id,\n            \"api_key\": api_key,\n            \"max_fps\": max_fps,\n\n        },\n    )\n    return response.json()\n</code></pre>"},{"location":"enterprise/stream_management_api/#important-notes","title":"Important notes","text":"<ul> <li>Please remember that <code>initialise_pipeline()</code> must be filled with <code>video_reference</code> and <code>sink_configuration</code>   in such a way, that any resource (video file / camera device) or URI (stream reference, sink reference) must be   reachable from Stream Manager environment! For instance - in some cases inside docker containers <code>localhost</code> will   be bound into container localhost not the localhost of the machine hosting container.</li> </ul>"},{"location":"enterprise/stream_management_api/#developer-notes","title":"Developer notes","text":"<p>The pivotal element of the implementation is the Stream Manager component, operating as an application in single-threaded, TCP-server mode. It systematically processes requests received from a TCP socket, taking on the responsibility of spawning and overseeing processes that run the <code>InferencePipelineManager</code>. Communication between the <code>InferencePipelineManager</code> processes and the main process of the Stream Manager occurs through multiprocessing queues. These queues facilitate the exchange of input commands and the retrieval of results.</p> <p>Requests directed to the Stream Manager are sequentially handled in blocking mode, ensuring that each request must conclude before the initiation of the next one.</p>"},{"location":"enterprise/stream_management_api/#communication-protocol-requests","title":"Communication protocol - requests","text":"<p>Stream Manager accepts the following binary protocol in communication. Each communication payload contains:</p> <pre><code>[HEADER: 4B, big-endian, not signed - int value with message size][MESSAGE: utf-8 serialised json of size dictated by header]\n</code></pre> <p>Message must be a valid JSON after decoding and represent valid command.</p>"},{"location":"enterprise/stream_management_api/#list_pipelines-command","title":"<code>list_pipelines</code> command","text":"<pre><code>{\n\"type\": \"list_pipelines\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#init-command","title":"<code>init</code> command","text":"<pre><code>{\n\"type\": \"init\",\n\"model_id\": \"some/1\",\n\"video_reference\": \"rtsp://192.168.0.1:554\",\n\"sink_configuration\": {\n\"type\": \"udp_sink\",\n\"host\": \"192.168.0.3\",\n\"port\": 9999\n},\n\"api_key\": \"YOUR-API-KEY\",\n\"max_fps\": 16,\n\"model_configuration\": {\n\"type\": \"object-detection\",\n\"class_agnostic_nms\": true,\n\"confidence\": 0.5,\n\"iou_threshold\": 0.4,\n\"max_candidates\": 300,\n\"max_detections\": 3000\n},\n\"video_source_properties\": {\n\"frame_width\": 1920,\n\"frame_height\": 1080,\n\"fps\": 30\n}\n}\n</code></pre> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"enterprise/stream_management_api/#terminate-command","title":"<code>terminate</code> command","text":"<pre><code>{\n\"type\": \"terminate\",\n\"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#pause-command","title":"<code>pause</code> command","text":"<pre><code>{\n\"type\": \"mute\",\n\"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#resume-command","title":"<code>resume</code> command","text":"<pre><code>{\n\"type\": \"resume\",\n\"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#status-command","title":"<code>status</code> command","text":"<pre><code>{\n\"type\": \"status\",\n\"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#communication-protocol-responses","title":"Communication protocol - responses","text":"<p>Stream Manager, for each request that can be processed (without timeout or source disconnection), will return the result in a format:</p> <pre><code>[HEADER: 4B, big-endian, not signed - int value with result size][RESULT: utf-8 serialised json of size dictated by header]\n</code></pre> <p>Structure of result:</p> <ul> <li><code>request_id</code> - field with random string representing request id assigned by Stream Manager - to ease debugging</li> <li><code>pipeline_id</code> - if command from request can be associated to specific pipeline - its ID will be denoted in response</li> <li><code>response</code> - payload of operation response</li> </ul> <p>Each <code>response</code> has the <code>status</code> key with two values possible: <code>success</code> or <code>failure</code> to denote operation status. Each failed response contain <code>error_type</code> key to dispatch error handling and optional fields <code>error_class</code> and <code>error_message</code> representing inner details of error.</p> <p>Content of successful responses depends on type of operation.</p>"},{"location":"enterprise/stream_management_api/#future-work","title":"Future work","text":"<ul> <li>securing API connection layer (to enable safe remote control)</li> <li>securing TCP socket of Stream Manager</li> </ul>"},{"location":"enterprise/active-learning/active_learning/","title":"Active Learning","text":"<p>Active Learning is a process of iterative improvement of model by retraining models on dataset that grows over time. This process includes data collection (usually with smart selection of datapoints that model would most benefit from), labeling, model re-training, evaluation and deployment - to close the circle and start new iteration.</p> <p>Elements of that process can be partially or fully automated - providing an elegant way of improving dataset over time,  which is important to ensure good quality of model predictions over time (as the data distribution may change and a model trained on old data may not be performant facing the new one). At Roboflow, we've brought automated data collection  mechanism - which is the foundational building block for Active Learning  -- to the platform.</p>"},{"location":"enterprise/active-learning/active_learning/#where-to-start","title":"Where to start?","text":"<p>We suggest clients apply the following strategy to train their models. If it's applicable - start from a small, good  quality dataset labeled manually (making sure that the test set is representative of the problem to be solved) and train  an initial model. Once that is done - deploy your model, enabling Active Learning data collection, and gradually increase  the size of your dataset with data collected in production environment.</p> <p>Alternatively, it is also possible to start the project with a Universe model. Then, for each request you can specify <code>active_learning_target_dataset</code> - pointing to the project where the data should be  saved. This way, if you find a model that meets your minimal quality criteria, you may start generating valuable  predictions from day zero, while collecting good quality data to train even better models in the future.</p>"},{"location":"enterprise/active-learning/active_learning/#how-active-learning-data-collection-works","title":"How Active Learning data collection works?","text":"<p>Here is the standard workflow for Active Learning data collection:</p> <ul> <li>The user initiates the creation of an Active Learning configuration within the Roboflow app.</li> <li>This configuration is then distributed across all active inference instances, which may include those running against video streams and the HTTP API, both on-premises and within the Roboflow platform.</li> <li>During runtime, as predictions are generated, images and model predictions (treated as initial annotations) are dynamically collected and submitted in batches into user project. These batches are then ready for labeling within the Roboflow platform.</li> </ul> <p>How active learning works with Inference is configured in your server active learning configuration. Learn how to configure active learning.</p> <p>Active learning can be disabled by setting <code>ACTIVE_LEARNING_ENABLED=false</code> in the environment where you run <code>inference</code>.</p>"},{"location":"enterprise/active-learning/active_learning/#usage-patterns","title":"Usage patterns","text":"<p>Active Learning data collection may be combined with different components of the Roboflow ecosystem. In particular:</p> <ul> <li>the <code>inference</code> Python package can be used to get predictions from the model and register them at Roboflow platform</li> <li>one may want to use <code>InferencePipeline</code> to get predictions from video and register its video frames using Active Learning</li> <li>self-hosted <code>inference</code> server - where data is collected while processing requests</li> <li>Roboflow hosted <code>inference</code> - where you let us make sure you get your predictions and data registered. No  infrastructure needs to run on your end, we take care of everything</li> <li>Roboflow <code>workflows</code> - our newest feature - supports <code>ActiveLearningDataCollectionBlock</code></li> </ul>"},{"location":"enterprise/active-learning/active_learning/#sampling-strategies","title":"Sampling Strategies","text":"<p><code>inference</code> makes it possible to configure the way data is selected for registration. One may configure one or more sampling strategies during Active Learning configuration. We support five strategies for sampling image data for use in training new model versions.  These strategies are:</p> <ul> <li>Random sampling: Images are collected at random.</li> <li>Close-to-threshold: Collect data close to a given threshold.</li> <li>Detection count-based (Detection models only): Collect data with a specific number of detections returned by a detection model.</li> <li>Class-based (Classification models only): Collect data with a specific class returned by a classification model.</li> </ul>"},{"location":"enterprise/active-learning/active_learning/#how-data-is-sampled","title":"How Data is Sampled","text":"<p>When you run Inference with an active learning configuration, the following steps are run:</p> <ol> <li>Sampling methods are evaluated to decide which ones are applicable to the image and prediction (evaluation happens in the order of definition in your configuration).</li> <li>A global limit for batch (defined in <code>batching_strategy</code>) is checked. Its violation terminates Active Learning attempt.</li> <li>Matching methods are checked against limits defined within their configurations. The first method with matching limit is selected.</li> </ol> <p>Once a datapoint is selected and there is no limit violation, it will be saved into Roboflow platform with tags relevant for specific strategy (and global tags defined at the level of Active Learning configuration).</p>"},{"location":"enterprise/active-learning/active_learning/#active-learning-configuration","title":"Active Learning Configuration","text":"<p>One may choose to configure their Active Learning with the Roboflow app UI by navigating to the <code>Active Learning</code> panel. Alternatively, requests to Roboflow API may be sent with custom configuration. Here is how to configure Active Learning directly through the API.</p>"},{"location":"enterprise/active-learning/active_learning/#configuration-options","title":"Configuration options","text":"<ul> <li><code>enabled</code>: boolean flag to enable / disable the configuration (required) - <code>{\"enabled\": false}</code> is minimal valid config</li> <li><code>max_image_size</code>: two element list with positive integers (height, width) enforcing down-sizing (with aspect-ratio preservation) of images before submission into Roboflow platform (optional)</li> <li><code>jpeg_compression_level</code>: integer value in range [1, 100]  representing compression level of submitted images  (optional, defaults to <code>95</code>)</li> <li><code>persist_predictions</code>: binary flag to decide if predictions should be collected along with images (required if <code>enabled</code>)</li> <li><code>sampling_strategies</code>: list of sampling strategies (non-empty list required if <code>enabled</code>)</li> <li><code>batching_strategy</code>: configuration of labeling batches creation - details below (required if <code>enabled</code>)</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/active_learning/#batching-strategy","title":"Batching strategy","text":"<p>The <code>batching_strategy</code> field holds a dictionary with the following configuration options:</p> <ul> <li><code>batches_name_prefix</code>: A string representing the prefix of batch names created by Active Learning (required)</li> <li><code>recreation_interval</code>: One of <code>[\"never\", \"daily\", \"weekly\", \"monthly\"]</code>: representing the interval which is to be used to create separate batches. This parameter allows the user to control the flow of labeling batches over time (required).</li> <li><code>max_batch_images</code>: Positive integer representing the maximum size of the batch (applied on top of any strategy limits) to prevent too much data from being collected (optional)</li> </ul>"},{"location":"enterprise/active-learning/active_learning/#strategy-limits","title":"Strategy limits","text":"<p>Each strategy can be configured with <code>limits</code>: list of values limiting how many images can be collected  each minute, hour or day. Each entry on that list can hold two values: * <code>type</code>: one of <code>[\"minutely\", \"hourly\", \"daily\"]</code>: representing the type of limit * <code>value</code>: with limit threshold</p> <p>Limits are enforced with different granularity, as they are implemented based or either Redis or memory cache (bounded into a single process). So, effectively: * if the Redis cache is used - all instances of <code>inference</code> connected to the same Redis service will share limit  enforcements * otherwise, the memory cache of a single instance is used (multiple processes will have their own limits)</p> <p>Self-hosted <code>inference</code> may be connected to your own Redis cache.</p>"},{"location":"enterprise/active-learning/active_learning/#example-configuration","title":"Example configuration","text":"<pre><code>{\n\"enabled\": true,\n\"max_image_size\": [1200, 1200],\n\"jpeg_compression_level\": 75,\n\"persist_predictions\": true,\n\"sampling_strategies\": [\n{\n\"name\": \"default_strategy\",\n\"type\": \"random\",\n\"traffic_percentage\": 0.1,\n\"limits\": [{\"type\": \"daily\", \"value\": 100}]\n}\n],\n\"batching_strategy\": {\n\"batches_name_prefix\": \"al_batch\",\n\"recreation_interval\": \"daily\"\n}\n}\n</code></pre>"},{"location":"enterprise/active-learning/active_learning/#set-configuration","title":"Set Configuration","text":"<p>To set an active learning configuration, use the following code:</p> <pre><code>import requests\n\ndef set_active_learning_configuration(\n    workspace: str,\n    project: str,\n    api_key: str,\n    config: dict,\n) -&gt; None:\n    response = requests.post(\n        f\"https://api.roboflow.com/{workspace}/{project}/active_learning?api_key={api_key}\",\n        json={\n            \"config\": config,\n        }\n    )\n    return response.json()\n\nset_active_learning_configuration(\n    workspace=\"yyy\",\n    project=\"zzz\",\n    api_key=\"XXX\",\n    config={\n        \"enabled\": True,\n        \"persist_predictions\": True,\n        \"batching_strategy\": {\n            \"batches_name_prefix\": \"my_batches\",\n            \"recreation_interval\": \"daily\",\n        },\n        \"sampling_strategies\": [\n            {\n                \"name\": \"default_strategy\",\n                \"type\": \"random\",\n                \"traffic_percentage\": 0.01, \n                \"limits\": [{\"type\": \"daily\", \"value\": 100}]\n            }\n        ]\n    }\n)\n</code></pre> <p>Where:</p> <ol> <li><code>workspace</code> is your workspace name;</li> <li><code>project</code> is your project name;</li> <li><code>api_key</code> is your API key, and;</li> <li><code>config</code> is your active learning configuration.</li> </ol>"},{"location":"enterprise/active-learning/active_learning/#retrieve-existing-configuration","title":"Retrieve Existing Configuration","text":"<p>To retrieve an existing active learning configuration, use the following code:</p> <pre><code>import requests\n\ndef get_active_learning_configuration(\n    workspace: str,\n    project: str,\n    api_key: str\n) -&gt; None:\n    response = requests.get(\n        f\"https://api.roboflow.com/{workspace}/{project}/active_learning?api_key={api_key}\",\n    )\n    return response.json()\n</code></pre> <p>Above, replace <code>workspace</code> with your workspace name, <code>project</code> with your project name, and <code>api_key</code> with your API key.</p>"},{"location":"enterprise/active-learning/active_learning/#stubs","title":"Stubs","text":"<p>One may use <code>{dataset_name}/0</code> as <code>model_id</code> while making prediction - to use null model for specific project.  It is going to provide predictions in the following format: <pre><code>{\n\"time\": 0.0002442499971948564,\n\"is_stub\": true,\n\"model_id\": \"asl-poly-instance-seg/0\",\n\"task_type\": \"instance-segmentation\"\n}\n</code></pre></p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>This option, combined with Active Learning (namely <code>random</code> sampling strategy), provides a way to start data collection even prior any model is trained. There are several benefits of such strategy. The most important is building  the dataset representing the true production distribution, before any model is trained.</p> <p>Example client usage: <pre><code>import cv2\nfrom inference_sdk import InferenceHTTPClient\n\nimage = cv2.imread(\"&lt;path_to_your_image&gt;\")\nLOCALHOST_CLIENT = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",\n    api_key=\"XXX\"\n)\nLOCALHOST_CLIENT.infer(image, model_id=\"asl-poly-instance-seg/0\")\n</code></pre></p>"},{"location":"enterprise/active-learning/active_learning/#parameters-of-requests-to-inference-server-influencing-the-active-learning-data-collection","title":"Parameters of requests to <code>inference</code> server influencing the Active Learning data collection","text":"<p>There are a few parameters that can be added to request to influence how data collection works, in particular:</p> <ul> <li><code>disable_active_learning</code> - to disable functionality at the level of a single request (if for some reason you do not  want input data to be collected - useful for testing purposes)</li> <li><code>active_learning_target_dataset</code> - making inference from a specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. Please remember that you cannot use incompatible types of models in <code>project_a</code> and <code>project_b</code>; if that is the case, data will not be  registered. For instance, classification predictions cannot be registered in detection-based projects. You are free to mix  tasks like object-detection, instance-segmentation, or keypoints detection, but naturally not every detail of the required label may be available from prediction.</li> </ul> <p>Visit Inference SDK docs to learn more.</p>"},{"location":"enterprise/active-learning/classes_based/","title":"Classes Based","text":"<p>Collect and save images that match a class from a classifier prediction for use in improving your model.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>classification</code></li> </ul>"},{"location":"enterprise/active-learning/classes_based/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>classes_based</code> is used to identify close to threshold sampling strategy (required)</li> <li><code>selected_class_names</code>: list of class names to consider during sampling - (required)</li> <li><code>probability</code>: fraction of datapoints that matches sampling criteria that will be persisted. It is meant to be float   value in range [0.0, 1.0] (required)</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/classes_based/#example","title":"Example","text":"<p>Here is an example of a configuration manifest for the close to threshold sampling strategy:</p> <pre><code>{\n\"name\": \"underrepresented_classes\",\n\"type\": \"classes_based\",\n\"selected_class_names\": [\"cat\"],\n\"probability\": 1.0,\n\"tags\": [\"hard-classes\"],\n\"limits\": [\n{ \"type\": \"minutely\", \"value\": 10 },\n{ \"type\": \"hourly\", \"value\": 100 },\n{ \"type\": \"daily\", \"value\": 1000 }\n]\n}\n</code></pre> <p>Learn how to configure active learning for your model.</p>"},{"location":"enterprise/active-learning/close_to_threshold_sampling/","title":"Close to Threshold Sampling","text":"<p>Select data points that lead to specific prediction confidences for particular classes.</p> <p>This method is applicable to both detection and classification models, although the behavior may vary slightly between the two.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>classification</code></li> <li><code>object-detection</code></li> <li><code>instance-segmentation</code></li> <li><code>keypoints-detection</code></li> </ul>"},{"location":"enterprise/active-learning/close_to_threshold_sampling/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>close_to_threshold</code> is used to identify close to threshold sampling strategy (required)</li> <li><code>selected_class_names</code>: list of class names to consider during sampling; if not provided, all classes can be sampled. (Optional)</li> <li><code>threshold</code> and <code>epsilon</code>: Represent the center and radius for the confidence range that triggers sampling. Both are   to be float values in range [0.0, 1.0]. For example, if one aims to obtain datapoints where the classifier is highly   confident (0.8, 1.0), set threshold=0.9 and epsilon=0.1. Note that this is limited to outcomes from model   post-processing and threshold filtering - hence not all model predictions may be visible at the level of Active Learning   logic. (required)</li> <li><code>probability</code>: Fraction of datapoints matching sampling criteria that will be persisted. It is meant to be float   value in range [0.0, 1.0] (required)</li> <li><code>minimum_objects_close_to_threshold</code>: (used for detection predictions only) Specify how many detected objects from   selected classes must be close to the threshold to accept the datapoint. If given - must be integer value &gt;= 1.   (Optional - with default to <code>1</code>)</li> <li><code>only_top_classes</code>: (used for classification predictions only) Flag to decide whether only the <code>top</code> or   <code>predicted_classes</code> (for multi-class/multi-label cases, respectively) should be considered. This helps avoid sampling   based on non-leading classes in predictions. Default: <code>True</code>.</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/close_to_threshold_sampling/#example","title":"Example","text":"<p>Here is an example of a configuration manifest for the close to threshold sampling strategy:</p> <pre><code>{\n\"name\": \"hard_examples\",\n\"type\": \"close_to_threshold\",\n\"selected_class_names\": [\"a\", \"b\"],\n\"threshold\": 0.25,\n\"epsilon\": 0.1,\n\"probability\": 0.5,\n\"tags\": [\"my_tag_1\", \"my_tag_2\"],\n\"limits\": [\n{ \"type\": \"minutely\", \"value\": 10 },\n{ \"type\": \"hourly\", \"value\": 100 },\n{ \"type\": \"daily\", \"value\": 1000 }\n]\n}\n</code></pre> <p>Learn how to configure active learning for your model.</p>"},{"location":"enterprise/active-learning/detection_number/","title":"Detection Number","text":"<p>Choose specific detections based on count and detection classes. Collect and save images for use in improving your model.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>object-detection</code></li> <li><code>instance-segmentation</code></li> <li><code>keypoints-detection</code></li> </ul>"},{"location":"enterprise/active-learning/detection_number/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>detections_number_based</code> is used to identify close to threshold sampling strategy (required)</li> <li><code>selected_class_names</code>: list of class names to consider during sampling; if not provided, all classes can be sampled. (Optional)</li> <li><code>probability</code>: fraction of datapoints that matches sampling criteria that will be persisted. It is meant to be float   value in range [0.0, 1.0] (required)</li> <li><code>more_than</code>: minimal number of detected objects - if given it is meant to be integer &gt;= 0   (optional - if not given - lower limit is not applied)</li> <li><code>less_than</code>: maximum number of detected objects - if given it is meant to be integer &gt;= 0   (optional - if not given - upper limit is not applied)</li> <li>NOTE: if both <code>more_than</code> and <code>less_than</code> is not given - any number of matching detections will match the   sampling condition</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/detection_number/#example","title":"Example","text":"<pre><code>{\n\"name\": \"multiple_detections\",\n\"type\": \"detections_number_based\",\n\"probability\": 0.2,\n\"more_than\": 3,\n\"tags\": [\"crowded\"],\n\"limits\": [\n{ \"type\": \"minutely\", \"value\": 10 },\n{ \"type\": \"hourly\", \"value\": 100 },\n{ \"type\": \"daily\", \"value\": 1000 }\n]\n}\n</code></pre> <p>Learn how to configure active learning for your model.</p>"},{"location":"enterprise/active-learning/random_sampling/","title":"Random Sampling","text":"<p>Randomly select data to be saved for future labeling.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>stub</code></li> <li><code>classification</code></li> <li><code>object-detection</code></li> <li><code>instance-segmentation</code></li> <li><code>keypoints-detection</code></li> </ul>"},{"location":"enterprise/active-learning/random_sampling/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>random</code> is used to identify random sampling strategy (required)</li> <li><code>traffic_percentage</code>: float value in range [0.0, 1.0] defining the percentage of traffic to be persisted (required)</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> <li><code>limits</code>: definition of limits for data collection within a specific strategy</li> </ul>"},{"location":"enterprise/active-learning/random_sampling/#example","title":"Example","text":"<p>Here is an example of a configuration manifest for random sampling strategy:</p> <pre><code>{\n\"name\": \"my_random_sampling\",\n\"type\": \"random\",\n\"traffic_percentage\": 0.01,\n\"tags\": [\"my_tag_1\", \"my_tag_2\"],\n\"limits\": [\n{ \"type\": \"minutely\", \"value\": 10 },\n{ \"type\": \"hourly\", \"value\": 100 },\n{ \"type\": \"daily\", \"value\": 1000 }\n]\n}\n</code></pre> <p>Learn how to configure active learning for your model.</p>"},{"location":"fine-tuned/yolonas/","title":"YOLO-NAS","text":"<p>YOLO-NAS is a computer vision model architecture developed by Deci AI.</p>"},{"location":"fine-tuned/yolonas/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv5 model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/yolonas/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLO-NAS model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolonas/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLO-NAS Object Detection Model</li> <li>Configure a YOLO-NAS Classification Model</li> <li>Configure a YOLO-NAS Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolonas/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv5 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov5/","title":"YOLOv5","text":"<p>YOLOv5 is a computer vision model architecture implemented in the <code>ultralytics</code> Python package.</p>"},{"location":"fine-tuned/yolov5/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv5 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Classification</li> <li>Image Segmentation</li> </ul>"},{"location":"fine-tuned/yolov5/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv5 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov5/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv5 Object Detection Model</li> <li>Configure a YOLOv5 Classification Model</li> <li>Configure a YOLOv5 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov5/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv5 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov7/","title":"YOLOv7","text":"<p>YOLOv7 is a computer vision model architecture introduced in the paper \"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\".</p>"},{"location":"fine-tuned/yolov7/#supported-model-types","title":"Supported Model Types","text":"<ul> <li>Classification</li> </ul>"},{"location":"fine-tuned/yolov7/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov7/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Deploy a YOLOv7 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov7/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv7 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov8/","title":"YOLOv8","text":"<p>YOLOv8 is a computer vision model architecture implemented in the <code>ultralytics</code> Python package.</p>"},{"location":"fine-tuned/yolov8/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv8 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Classification</li> <li>Image Segmentation</li> </ul>"},{"location":"fine-tuned/yolov8/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov8/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv8 Object Detection Model</li> <li>Configure a YOLOv8 Classification Model</li> <li>Configure a YOLOv8 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov8/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv7 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov9/","title":"YOLOv9","text":"<p>YOLOv9 is a computer vision model architecture introduced in the paper \"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information\".</p>"},{"location":"fine-tuned/yolov9/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv9 model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/yolov9/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov9/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv9 Object Detection Model</li> </ul>"},{"location":"fine-tuned/yolov9/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv7 is licensed when using Inference to deploy your model.</p>"},{"location":"foundation/about/","title":"What is a Foundation Model?","text":"<p>Foundation models are machine learning models that have been trained on vast amounts of data to accomplish a specific task.</p> <p>For example, OpenAI trained CLIP, a foundation model. CLIP enables you to classify images. You can also compare the similarity of images and text with CLIP.</p> <p>The CLIP training process, which was run using over 400 million pairs of images and text, allowed the model to build an extensive range of knowledge, which can be applied to a range of domains.</p> <p>Foundation models are being built for a range of vision tasks, from image segmentation to classification to zero-shot object detection.</p> <p>Inference supports the following foundation models:</p> <ul> <li>Gaze (LC2S-Net): Detect the direction in which someone is looking.</li> <li>CLIP: Classify images and compare the similarity of images and text.</li> <li>DocTR: Read characters in images.</li> <li>Grounding DINO: Detect objects in images using text prompts.</li> <li>Segment Anything (SAM): Segment objects in images.</li> <li>CogVLM: A large multimodal model (LMM).</li> </ul> <p>All of these models can be used over a HTTP request with Inference. This means you don't need to spend time setting up and configuring each model.</p>"},{"location":"foundation/about/#how-are-foundation-models-used","title":"How Are Foundation Models Used?","text":"<p>Use cases vary depending on the foundation model with which you are working. For example, CLIP has been used extensively in the field of computer vision for tasks such as:</p> <ol> <li>Clustering images to identify groups of similar images and outliers;</li> <li>Classifying images;</li> <li>Moderating image content;</li> <li>Identifying if two images are too similar or too different, ideal for dataset management and cleaning;</li> <li>Building dataset search experiences,</li> <li>And more.</li> </ol> <p>Grounding DINO, on the other hand, can be used out-of-the-box to detect a range of objects. Or you can use Grounding DINO to automatically label data for use in training a smaller, faster object detection model that is fine-tuned to your use case.</p>"},{"location":"foundation/about/#how-to-use-foundation-models","title":"How to Use Foundation Models","text":"<p>The guides in this section walk through how to use each of the foundation models listed above with Inference. No machine learning experience is required to use each model. Our code snippets and accompanying reference material provide the knowledge you need to get started working with foundation models.</p>"},{"location":"foundation/clip/","title":"CLIP (Classification, Embeddings)","text":"<p>CLIP is a computer vision model that can measure the similarity between text and images.</p> <p>CLIP can be used for, among other things:</p> <ul> <li>Image classification</li> <li>Automated labeling for classification models</li> <li>Image clustering</li> <li>Gathering images for model training that are sufficiently dissimilar from existing samples</li> <li>Content moderation</li> </ul> <p>With Inference, you can calculate CLIP embeddings for images and text in real-time.</p> <p>In this guide, we will show:</p> <ol> <li>How to classify video frames with CLIP in real time, and;</li> <li>How to calculate CLIP image and text embeddings for use in clustering and comparison.</li> </ol>"},{"location":"foundation/clip/#how-can-i-use-clip-model-in-inference","title":"How can I use CLIP model in <code>inference</code>?","text":"<ul> <li>directly from <code>inference[clip]</code> package, integrating the model directly into your code</li> <li>using <code>inference</code> HTTP API (hosted locally, or at Roboflow platform), integrating via HTTP protocol</li> <li>using <code>inference-sdk</code> package (<code>pip install inference-sdk</code>) and <code>InferenceHTTPClient</code></li> <li>creating custom code to make HTTP requests (see API Reference)</li> </ul>"},{"location":"foundation/clip/#supported-clip-versions","title":"Supported CLIP versions","text":"<ul> <li><code>clip/RN101</code></li> <li><code>clip/RN50</code></li> <li><code>clip/RN50x16</code></li> <li><code>clip/RN50x4</code></li> <li><code>clip/RN50x64</code></li> <li><code>clip/ViT-B-16</code></li> <li><code>clip/ViT-B-32</code></li> <li><code>clip/ViT-L-14-336px</code></li> <li><code>clip/ViT-L-14</code></li> </ul>"},{"location":"foundation/clip/#classify-video-frames","title":"Classify Video Frames","text":"<p>With CLIP, you can classify images and video frames without training a model. This is because CLIP has been pre-trained to recognize many different objects.</p> <p>To use CLIP to classify video frames, you need a prompt. In the example below, we will use the prompt \"cell phone\".</p> <p>We can compare the similarity of \"cell phone\" to each video frame and use that to classify the video frame.</p> <p>Below is a demo of CLIP classifying video frames in real time. The code for the example is below the video.</p> <p>First, install the Inference CLIP extension:</p> <pre><code>pip install inference[clip]\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import cv2\nimport inference\nfrom inference.core.utils.postprocess import cosine_similarity\n\nfrom inference.models import Clip\nclip = Clip(model_id=\"clip/ViT-B-16\")  # `model_id` has default, but here is how to test other versions\n\nprompt = \"an ace of spades playing card\"\ntext_embedding = clip.embed_text(prompt)\n\ndef render(result, image):\n    # get the cosine similarity between the prompt &amp; the image\n    similarity = cosine_similarity(result[\"embeddings\"][0], text_embedding[0])\n\n    # scale the result to 0-100 based on heuristic (~the best &amp; worst values I've observed)\n    range = (0.15, 0.40)\n    similarity = (similarity-range[0])/(range[1]-range[0])\n    similarity = max(min(similarity, 1), 0)*100\n\n    # print the similarity\n    text = f\"{similarity:.1f}%\"\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (255, 255, 255), 30)\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (206, 6, 103), 16)\n\n    # print the prompt\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 10)\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (206, 6, 103), 5)\n\n    # display the image\n    cv2.imshow(\"CLIP\", image)\n    cv2.waitKey(1)\n\n# start the stream\ninference.Stream(\n    source=\"webcam\",\n    model=clip,\n\n    output_channel_order=\"BGR\",\n    use_main_thread=True,\n\n    on_prediction=render\n)\n</code></pre> <p>Run the code to use CLIP on your webcam.</p> <p>Note: The model will take a minute or two to load. You will not see output while the model is loading.</p>"},{"location":"foundation/clip/#calculate-a-clip-embedding","title":"Calculate a CLIP Embedding","text":"<p>CLIP enables you to calculate embeddings. Embeddings are numeric, semantic representations of images and text. They are useful for clustering and comparison.</p> <p>You can use CLIP embeddings to compare the similarity of text and images.</p> <p>There are two types of CLIP embeddings: image and text.</p> <p>Below we show how to calculate, then compare, both types of embeddings.</p>"},{"location":"foundation/clip/#image-embedding","title":"Image Embedding","text":"<p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <p>In the code below, we calculate an image embedding.</p> <p>Create a new Python file and add this code:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\nembeddings = CLIENT.get_clip_image_embeddings(inference_input=\"https://i.imgur.com/Q6lDy8B.jpg\")\nprint(embeddings)\n\n# since release `0.9.17`, you may pass extra argument `clip_version` to get_clip_image_embeddings(...) to select\n# model version\n</code></pre>"},{"location":"foundation/clip/#text-embedding","title":"Text Embedding","text":"<p>In the code below, we calculate a text embedding.</p> <p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nembeddings = CLIENT.get_clip_text_embeddings(text=\"the quick brown fox jumped over the lazy dog\")\nprint(embeddings)\n\n# since release `0.9.17`, you may pass extra argument `clip_version` to get_clip_text_embeddings(...) to select\n# model version\n</code></pre>"},{"location":"foundation/clip/#compare-embeddings","title":"Compare Embeddings","text":"<p>To compare embeddings for similarity, you can use cosine similarity.</p> <p>The code you need to compare image and text embeddings is the same.</p> <p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nresult = CLIENT.clip_compare(\n  subject=\"./image.jpg\",\n  prompt=[\"dog\", \"cat\"]\n)\nprint(result)\n# since release `0.9.17`, you may pass extra argument `clip_version` to clip_compare(...) to select\n# model version\n</code></pre> <p>The resulting number will be between 0 and 1. The higher the number, the more similar the image and text are.</p>"},{"location":"foundation/clip/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of CLIP.</p> <ul> <li>CLIP Embed Images: 0.5 seconds per inference (59.55 seconds for 100 inferences).</li> <li>CLIP Embed Text: 0.5 seconds per inference (51.52 seconds for 100 inferences).</li> <li>CLIP Compare Image and Text: 0.58 seconds per inference (58.03 seconds for 100 inferences).</li> </ul>"},{"location":"foundation/clip/#see-also","title":"See Also","text":"<ul> <li>What is CLIP?</li> <li>Build an Image Search Engine with CLIP and Faiss</li> <li>Build a Photo Memories App with CLIP</li> <li>Analyze and Classify Video with CLIP</li> </ul>"},{"location":"foundation/cogvlm/","title":"CogVLM (Multimodal Language Model)","text":"<p>CogVLM is a Large Multimodal Model (LMM). CogVLM is available for use in Inference.</p> <p>You can ask CogVLM questions about the contents of an image and retrieve a text response.</p>"},{"location":"foundation/cogvlm/#model-quantization","title":"Model Quantization","text":"<p>You can run CogVLM through Roboflow Inference with three degrees of quantization. Quantization allows you to make a model smaller, but there is an accuracy trade-off. The three degrees of quantization are:</p> <ul> <li>No quantization: Run the full model. For this, you will need 80 GB of RAM. You could run the model on an 80 GB NVIDIA A100.</li> <li>8-bit quantization: Run the model with less accuracy than no quantization. You will need 32 GB of RAM.You could run this model on an A100 with sufficient virtual RAM.</li> <li>4-bit quantization: Run the model with less accuracy than 8-bit quantization. You will need 16 GB of RAM. You could run this model on an NVIDIA T4.</li> </ul>"},{"location":"foundation/cogvlm/#use-cogvlm-with-inference","title":"Use CogVLM with Inference","text":"<p>To use CogVLM with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. </p> <p>Then, retrieve your API key from the Roboflow dashboard. Learn how to retrieve your API key.</p> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>We recommend using CogVLM paired with inference HTTP API adjusted to run in GPU environment. It's easy to set up  with our <code>inference-cli</code> tool. Run the following command to set up environment and run the API under  <code>http://localhost:9001</code></p> <pre><code>pip install inference inference-cli inference-sdk\ninference server start  # make sure that you are running this at machine with GPU! Otherwise CogVLM will not be available\n</code></pre> <p>Let's ask a question about the following image:</p> <p></p> <p>Use <code>inference-sdk</code> to prompt the model:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = CLIENT.prompt_cogvlm(\n    visual_prompt=\"./forklift.jpg\",\n    text_prompt=\"Is there a forklift close to a conveyor belt?\",\n)\nprint(result)\n</code></pre> <p>Above, replace <code>forklift.jpeg</code> with the path to the image in which you want to detect objects.</p> <p>Let's use the prompt \"Is there a forklift close to a conveyor belt?\u201d\"</p> <p>The results of CogVLM will appear in your terminal:</p> <pre><code>{\n    'response': 'yes, there is a forklift close to a conveyor belt, and it appears to be transporting a stack of items onto it.',\n    'time': 12.89864671198302\n}\n</code></pre> <p>CogVLM successfully answered our question, noting there is a forklift close to the conveyor belt in the image.</p>"},{"location":"foundation/cogvlm/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of CogVLM.</p> <p>CogVLM ran 100 inferences in 365.22 seconds (11.69 seconds per inference, on average).</p>"},{"location":"foundation/cogvlm/#see-also","title":"See Also","text":"<ul> <li>How to deploy CogVLM</li> </ul>"},{"location":"foundation/doctr/","title":"DocTR (OCR)","text":"<p>DocTR is an Optical Character Recognition (OCR) model.</p> <p>You can use DocTR to read the text in an image.</p> <p>To use DocTR with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. </p> <p>Then, retrieve your API key from the Roboflow dashboard. Learn how to retrieve your API key.</p> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Let's retrieve the text in the following image:</p> <p></p> <p>Create a new Python file and add the following code:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = CLIENT.ocr_image(inference_input=\"./container.jpg\")  # single image request\nprint(result)\n</code></pre> <p>Above, replace <code>container.jpeg</code> with the path to the image in which you want to detect objects.</p> <p>The results of DocTR will appear in your terminal:</p> <pre><code>{'result': '', 'time': 3.98263641900121, 'result': 'MSKU 0439215', 'time': 3.870879542999319}\n</code></pre>"},{"location":"foundation/doctr/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of DocTR.</p> <p>DocTR ran 100 inferences in 365.22 seconds (3.65 seconds per inference, on average).</p>"},{"location":"foundation/doctr/#see-also","title":"See Also","text":"<ul> <li>How to detect text in images with OCR</li> </ul>"},{"location":"foundation/gaze/","title":"L2CS-Net (Gaze Detection)","text":"<p>L2CS-Net is a gaze estimation model.</p> <p>You can detect the direction in which someone is looking using the L2CS-Net model.</p>"},{"location":"foundation/gaze/#how-to-use-l2cs-net","title":"How to Use L2CS-Net","text":"<p>To use L2CS-Net with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>L2CS-Net accepts an image and returns pitch and yaw values that you can use to:</p> <ol> <li>Figure out the direction in which someone is looking, and;</li> <li>Estimate, roughly, where someone is looking.</li> </ol> <p>We recommend using L2CS-Net paired with inference HTTP API. It's easy to set up with our <code>inference-cli</code> tool. Run the  following command to set up environment and run the API under <code>http://localhost:9001</code></p> <pre><code>pip install inference inference-cli inference-sdk\ninference server start  # this starts server under http://localhost:9001\n</code></pre> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nCLIENT.detect_gazes(inference_input=\"./image.jpg\")  # single image request\n</code></pre> <p>Above, replace <code>image.jpg</code> with the image in which you want to detect gazes.</p> <p>The code above makes two assumptions:</p> <ol> <li>Faces are roughly one meter away from the camera.</li> <li>Faces are roughly 250mm tall.</li> </ol> <p>These assumptions are a good starting point if you are using a computer webcam with L2CS-Net, where people in the frame are likely to be sitting at a desk.</p> <p>On the first run, the model will be downloaded. On subsequent runs, the model will be cached locally and loaded from the cache. It will take a few moments for the model to download.</p> <p>The results of L2CS-Net will appear in your terminal:</p> <pre><code>[{'face': {'x': 1107.0, 'y': 1695.5, 'width': 1056.0, 'height': 1055.0, 'confidence': 0.9355756640434265, 'class': 'face', 'class_confidence': None, 'class_id': 0, 'tracker_id': None, 'landmarks': [{'x': 902.0, 'y': 1441.0}, {'x': 1350.0, 'y': 1449.0}, {'x': 1137.0, 'y': 1692.0}, {'x': 1124.0, 'y': 1915.0}, {'x': 625.0, 'y': 1551.0}, {'x': 1565.0, 'y': 1571.0}]}, 'yaw': -0.04104889929294586, 'pitch': 0.029525401070713997}]\n</code></pre> <p>We have created a full gaze detection example that shows how to:</p> <ol> <li>Use L2CS-Net with a webcam;</li> <li>Calculate the direction in which and point in space at which someone is looking;</li> <li>Calculate what quadrant of the screen someone is looking at, and;</li> <li>Annotate the image with the direction someone is looking.</li> </ol> <p>This example will let you run L2CS-Net and see the results of the model in real time. Here is an recording of the example working:</p> <p>Learn how to set up the example.</p>"},{"location":"foundation/gaze/#l2cs-net-inference-response","title":"L2CS-Net Inference Response","text":"<p>Here is the structure of the data returned by a gaze request:</p> <pre><code>[{'face': {'class': 'face',\n           'class_confidence': None,\n           'class_id': 0,\n           'confidence': 0.9355756640434265,\n           'height': 1055.0,\n           'landmarks': [{'x': 902.0, 'y': 1441.0},\n                         {'x': 1350.0, 'y': 1449.0},\n                         {'x': 1137.0, 'y': 1692.0},\n                         {'x': 1124.0, 'y': 1915.0},\n                         {'x': 625.0, 'y': 1551.0},\n                         {'x': 1565.0, 'y': 1571.0}],\n           'tracker_id': None,\n           'width': 1056.0,\n           'x': 1107.0,\n           'y': 1695.5},\n  'pitch': 0.029525401070713997,\n  'yaw': -0.04104889929294586}]\n</code></pre>"},{"location":"foundation/gaze/#see-also","title":"See Also","text":"<ul> <li>Gaze Detection and Eye Tracking: A How-To Guide</li> </ul>"},{"location":"foundation/grounding_dino/","title":"Grounding DINO (Object Detection)","text":"<p>Grounding DINO is a zero-shot object detection model.</p> <p>You can use Grounding DINO to identify objects in images and videos using arbitrary text prompts.</p> <p>To use Grounding DINO effectively, we recommend experimenting with the model to understand which text prompts help achieve the desired results.</p> <p>Note</p> <p>Grounding DINO is most effective at identifying common objects (i.e. cars, people, dogs, etc.). It is less effective at identifying uncommon objects (i.e. a specific type of car, a specific person, a specific dog, etc.).</p>"},{"location":"foundation/grounding_dino/#how-to-use-grounding-dino","title":"How to Use Grounding DINO","text":"<p>First, install the Inference Grounding DINO extension:</p> <pre><code>pip install \"inference[grounding-dino]\"\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from inference.models.grounding_dino import GroundingDINO\n\nmodel = GroundingDINO(api_key=\"\")\n\nresults = model.infer(\n    {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/fruit.png\",\n        },\n        \"text\": [\"apple\"],\n\n        # Optional params\n        \"box_threshold\": 0.5\n        \"text_threshold\": 0.5\n    }\n)\n\nprint(results.json())\n</code></pre> <p>In this code, we load Grounding DINO, run Grounding DINO on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>apple</code> with the object you want to detect.</li> <li><code>fruit.png</code> with the path to the image in which you want to detect objects.</li> </ol> <p>Additionally, you can tweak the optional <code>box_threshold</code> and <code>class_threshold</code> params for your specific use case. Both values default to 0.5 if not set. See the Grounding DINO README for an explanation of the model's thresholds.</p> <p>To use Grounding DINO with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The predictions from your model will be printed to the console.</p>"},{"location":"foundation/sam/","title":"Segment Anything (Segmentation)","text":"<p>Segment Anything is an open source image segmentation model.</p> <p>You can use Segment Anything to identify the precise location of objects in an image.</p> <p>To use Segment Anything, you need to:</p> <ol> <li>Create an embedding for an image, and;</li> <li>Specify the coordinates of the object you want to segment.</li> </ol>"},{"location":"foundation/sam/#how-to-use-segment-anything","title":"How to Use Segment Anything","text":"<p>To use Segment Anything with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre>"},{"location":"foundation/sam/#embed-an-image","title":"Embed an Image","text":"<p>An embedding is a numeric representation of an image. SAM uses embeddings as input to calcualte the location of objects in an image.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"image_id\": \"example_image_id\",\n}\n\nbase_url = \"http://localhost:9001\"\n\n# Define your Roboflow API Key\napi_key = \"YOUR ROBOFLOW API KEY\"\n\nres = requests.post(\n    f\"{base_url}/sam/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n\nembeddings = res.json()['embeddings']\n</code></pre> <p>This code makes a request to Inference to embed an image using SAM.</p> <p>The <code>example_image_id</code> is used to cache the embeddings for later use so you don't have to send them back in future segmentation requests.</p>"},{"location":"foundation/sam/#segment-an-object","title":"Segment an Object","text":"<p>To segment an object, you need to know at least one point in the image that represents the object that you want to use.</p> <p>For testing with a single image, you can upload an image to the Polygon Zone web interface and hover over a point in the image to see the coordinates of that point.</p> <p>You may also opt to use an object detection model to identify an object, then use the center point of the bounding box as a prompt for segmentation.</p> <p>Create a new Python file and add the following code:</p> <pre><code>#Define request payload\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"point_coords\": [[380, 350]],\n    \"point_labels\": [1],\n    \"image_id\": \"example_image_id\",\n}\n\nres = requests.post(\n    f\"{base_url}/sam/embed_image?api_key={api_key}\",\n    json=infer_clip_payload,\n)\n\nmasks = request.json()['masks']\n</code></pre> <p>This request returns segmentation masks that represent the object of interest.</p>"},{"location":"foundation/sam/#see-also","title":"See Also","text":"<ul> <li>What is Segment Anything Model (SAM)?</li> </ul>"},{"location":"foundation/yolo_world/","title":"YOLO-World (Object Detection)","text":"<p>YOLO-World is a zero-shot object detection model.</p> <p>You can use YOLO-World to identify objects in images and videos using arbitrary text prompts.</p> <p>To use YOLO-World effectively, we recommend experimenting with the model to understand which text prompts help achieve the desired results.</p> <p>YOLO World is faster than many other zero-shot object detection models like YOLO-World. On powerful hardware like a V100 GPU, YOLO World can run in real-time.</p> <p>Note</p> <p>YOLO-World, like most state-of-the-art zero-shot detection models, is most effective at identifying common objects (i.e. cars, people, dogs, etc.). It is less effective at identifying uncommon objects (i.e. a specific type of car, a specific person, a specific dog, etc.).</p> <p>Note</p> <p>In <code>inference</code> package YOLO-World models are identified by <code>yolo_world/&lt;version&gt;</code>, where <code>&lt;version&gt;</code> can be one of the following: <code>s</code>, <code>m</code>, <code>l</code>, <code>x</code>, <code>v2-s</code>, <code>v2-m</code>, <code>v2-l</code>, <code>v2-x</code>. Versions <code>v2-...</code> denote newer models, with improved evaluation metrics.</p>"},{"location":"foundation/yolo_world/#how-to-use-yolo-world","title":"How to Use YOLO-World","text":"Inference Python LibraryInference Server HTTP APIInference Pipeline (Video) <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import cv2\nimport supervision as sv\n\nfrom inference.models.yolo_world.yolo_world import YOLOWorld\n\nimage = cv2.imread(\"image.jpeg\")\n\nmodel = YOLOWorld(model_id=\"yolo_world/l\")\nclasses = [\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]\nresults = model.infer(\"image.jpeg\", text=classes, confidence=0.03)\n\ndetections = sv.Detections.from_inference(results[0])\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [classes[class_id] for class_id in detections.class_id]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections\n)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, you will need to set up an Inference server to use the YOLO World HTTP API.</p> <p>To do this, run:</p> <pre><code>pip install inference inference-sdk\ninference server start\n</code></pre> <p>Then, create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import os\nimport cv2\nimport supervision as sv\n\nfrom inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = client.infer_from_yolo_world(\n    inference_input=[\"https://media.roboflow.com/dog.jpeg\"],\n    class_names=[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"],\n    model_version=\"l\",\n    confidence=0.1,\n)\n\ndetections = sv.Detections.from_inference(results[0])\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [classes[class_id] for class_id in detections.class_id]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections\n)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Info</p> <pre><code>**Breaking change!** There were versions: `0.9.14` and `0.9.15` where Yolo World was exposed\nbehind `InferencePipeline.init(...)` initializer that you needed to run with specific combination \nof parameters to alter default behavior of pipeline such that it runs against YoloWorld model. \nWe decided to provide an explicit way of running this foundation model in `InferencePipeline` providing\na dedicated init function starting from version `0.9.16`\n</code></pre> <p>You can easily run predictions against <code>YoloWorld</code> model using <code>InferencePipeline</code>. There is a custom init method to ease handling that use-case:</p> <pre><code># import the InferencePipeline interface\nfrom inference import InferencePipeline\n# import a built-in sink called render_boxes (sinks are the logic that happens after inference)\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init_with_yolo_world(\n    video_reference=\"./your_video.mp4\",\n    classes=[\"person\", \"dog\", \"car\", \"truck\"],\n    model_size=\"s\",\n    on_prediction=render_boxes,\n)\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>In this code, we load YOLO-World, run YOLO-World on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]</code> with the objects you want to detect.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from YOLO-World will be displayed in a new window.</p> <p></p>"},{"location":"foundation/yolo_world/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of YOLO-World.</p> <p>YOLO-World ran 100 inferences in 9.18 seconds (0.09 seconds per inference, on average).</p>"},{"location":"include/install/","title":"Install","text":"<p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"include/model_id/","title":"Model id","text":"<p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"inference_helpers/inference_cli/","title":"Inference CLI","text":""},{"location":"inference_helpers/inference_cli/#roboflow-inference-cli","title":"Roboflow Inference CLI","text":"<p>Roboflow Inference CLI offers a lightweight interface for running the Roboflow inference server locally or the Roboflow Hosted API.</p> <p>To create custom Inference server Docker images, go to the parent package, Roboflow Inference.</p> <p>Roboflow has everything you need to deploy a computer vision model to a range of devices and environments. Inference supports object detection, classification, and instance segmentation models, and running foundation models (CLIP and SAM).</p>"},{"location":"inference_helpers/inference_cli/#installation","title":"Installation","text":"<pre><code>pip install roboflow-cli\n</code></pre>"},{"location":"inference_helpers/inference_cli/#examples","title":"Examples","text":""},{"location":"inference_helpers/inference_cli/#inference-server-start","title":"inference server start","text":"<p>Starts a local Inference server. It optionally takes a port number (default is 9001) and will only start the docker container if there is not already a container running on that port.</p> <p>If you would rather run your server on a virtual machine in Google cloud or Amazon cloud, skip to the section titled \"Deploy Inference on Cloud\" below.</p> <p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If you haven't installed Docker yet, you can get it from Docker's official website.</p> <p>The CLI will automatically detect the device you are running on and pull the appropriate Docker image.</p> <pre><code>inference server start --port 9001 [-e {optional_path_to_file_with_env_variables}]\n</code></pre> <p>Parameter <code>--env-file</code> (or <code>-e</code>) is the optional path for .env file that will be loaded into your Inference server in case that values of internal parameters needs to be adjusted. Any value passed explicitly as command parameter is considered as more important and will shadow the value defined in <code>.env</code> file under the same target variable name.</p>"},{"location":"inference_helpers/inference_cli/#development-mode","title":"Development Mode","text":"<p>Use the <code>--dev</code> flag to start the Inference Server in development mode. Development mode enables the Inference Server's built in notebook environment for easy testing and development.</p>"},{"location":"inference_helpers/inference_cli/#inference-server-status","title":"inference server status","text":"<p>Checks the status of the local inference server.</p> <pre><code>inference server status\n</code></pre>"},{"location":"inference_helpers/inference_cli/#inference-server-stop","title":"inference server stop","text":"<p>Stops the inference server.</p> <pre><code>inference server stop\n</code></pre>"},{"location":"inference_helpers/inference_cli/#deploy-inference-on-a-cloud-vm","title":"Deploy Inference on a Cloud VM","text":"<p>You can deploy Roboflow Inference containers to virtual machines in the cloud. These VMs are configured to run CPU or GPU-based Inference servers under the hood, so you don't have to deal with OS/GPU drivers/docker installations, etc! The Inference cli currently supports deploying the Roboflow Inference container images into a virtual machine running on Google (GCP) or Amazon cloud (AWS).</p> <p>The Roboflow Inference CLI assumes the corresponding cloud CLI is configured for the project you want to deploy the virtual machine into. Read instructions for setting up Google/GCP - gcloud cli or the Amazon/AWS aws cli.</p> <p>Roboflow Inference cloud deploy is powered by the popular Skypilot project.</p>"},{"location":"inference_helpers/inference_cli/#cloud-deploy-examples","title":"Cloud Deploy Examples","text":"<p>We illustrate Inference cloud deploy with some examples, below.</p> <p>Deploy GPU or CPU inference to AWS or GCP</p> <pre><code># Deploy the roboflow Inference GPU container into a GPU-enabled VM in AWS\n\ninference cloud deploy --provider aws --compute-type gpu\n</code></pre> <pre><code># Deploy the roboflow Inference CPU container into a CPU-only VM in GCP\n\ninference cloud deploy --provider gcp --compute-type cpu\n</code></pre> <p>Note the \"cluster name\" printed after the deployment completes. This handle is used in many subsequent commands. The deploy command also prints helpful debug and cost information about your VM.</p> <p>Deploying Inference into a cloud VM will also print out an endpoint of the form \"http://1.2.3.4:9001\"; you can now run inferences against this endpoint.</p> <p>Note that the port 9001 is automatically opened - check with your security admin if this is acceptable for your cloud/project.</p>"},{"location":"inference_helpers/inference_cli/#view-status-of-deployments","title":"View status of deployments","text":"<pre><code>inference cloud status\n</code></pre>"},{"location":"inference_helpers/inference_cli/#stop-and-start-deployments","title":"Stop and start deployments","text":"<pre><code># Stop the VM, you only pay for disk storage while the VM is stopped\ninference cloud stop &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/inference_cli/#restart-deployments","title":"Restart deployments","text":"<pre><code>inference cloud start &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/inference_cli/#undeploy-delete-the-cloud-deployment","title":"Undeploy (delete) the cloud deployment","text":"<pre><code>inference cloud undeploy &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/inference_cli/#ssh-into-the-cloud-deployment","title":"SSH into the cloud deployment","text":"<p>You can SSH into your cloud deployment with the following command: <pre><code>ssh &lt;deployment_handle&gt;\n</code></pre></p> <p>The required SSH key is automatically added to your .ssh/config, you don't need to configure this manually.</p>"},{"location":"inference_helpers/inference_cli/#cloud-deploy-customization","title":"Cloud Deploy Customization","text":"<p>Roboflow Inference cloud deploy will create VMs based on internally tested templates.</p> <p>For advanced usecases and to customize the template, you can use your sky yaml template on the command-line, like so:</p> <pre><code>inference cloud deploy --custom /path/to/sky-template.yaml\n</code></pre> <p>If you want you can download the standard template stored in the roboflow cli and the modify it for your needs, this command will do that.</p> <pre><code># This command will print out the standard gcp/cpu sky template.\ninference cloud deploy --dry-run --provider gcp --compute-type cpu\n</code></pre> <p>Then you can deploy a custom template based off your changes.</p> <p>As an aside, you can also use the sky cli to control your deployment(s) and access some more advanced functionality.</p> <p>Roboflow Inference deploy currently supports AWS and GCP, please open an issue on the Inference GitHub repository if you would like to see other cloud providers supported.</p>"},{"location":"inference_helpers/inference_cli/#inference-infer","title":"inference infer","text":"<p>It takes input path / url and model version to produce predictions (and optionally make visualisation using  <code>supervision</code>). You can also specify a host to run inference on our hosted inference server.</p> <p>Note</p> <p>If you decided to use hosted inference server - make sure command <code>inference server start</code> was used first </p> <p>Tip</p> <p>Use <code>inference infer --help</code> to display description of parameters</p> <p>Tip</p> <p>Roboflow API key can be provided via <code>ROBOFLOW_API_KEY</code> environment variable</p>"},{"location":"inference_helpers/inference_cli/#local-image","title":"Local image","text":"<p>This command is going to make a prediction from local image using selected model and print the prediction on  the console.</p> <pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY}\n</code></pre> <p>To display visualised prediction use <code>-D</code> option. To save prediction and visualisation in a local directory, use <code>-o {path_to_your_directory}</code> option. Those options work also in other modes.</p> <pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY} -D -o {path_to_your_output_directory}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#hosted-image","title":"Hosted image","text":"<pre><code>inference infer -i https://[YOUR_HOSTED_IMAGE_URL] -m {your_project}/{version} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#hosted-api-inference","title":"Hosted API inference","text":"<pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY} -h https://detect.roboflow.com\n</code></pre>"},{"location":"inference_helpers/inference_cli/#local-directory","title":"Local directory","text":"<pre><code>inference infer -i {your_directory_with_images} -m {your_project}/{version} -o {path_to_your_output_directory} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#video-file","title":"Video file","text":"<pre><code>inference infer -i {path_to_your_video_file} -m {your_project}/{version} -o {path_to_your_output_directory} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#configuration-of-visualisation","title":"Configuration of visualisation","text":"<p>Option <code>-c</code> can be provided with a path to <code>*.yml</code> file configuring <code>supervision</code> visualisation. There are few pre-defined configs: - <code>bounding_boxes</code> - with <code>BoundingBoxAnnotator</code> and <code>LabelAnnotator</code> annotators - <code>bounding_boxes_tracing</code> - with <code>ByteTracker</code> and annotators (<code>BoundingBoxAnnotator</code>, <code>LabelAnnotator</code>) - <code>masks</code> - with <code>MaskAnnotator</code> and <code>LabelAnnotator</code> annotators - <code>polygons</code> - with <code>PolygonAnnotator</code> and <code>LabelAnnotator</code> annotators</p> <p>Custom configuration can be created following the schema: <pre><code>annotators:\n- type: \"bounding_box\"\nparams:\nthickness: 2\n- type: \"label\"\nparams:\ntext_scale: 0.5\ntext_thickness: 2\ntext_padding: 5\n- type: \"trace\"\nparams:\ntrace_length: 60\nthickness: 2\ntracking:\ntrack_thresh: 0.25\ntrack_buffer: 30\nmatch_thresh: 0.8\nframe_rate: 30\n</code></pre> <code>annotators</code> field is a list of dictionaries with two keys: <code>type</code> and <code>param</code>. <code>type</code> points to  name of annotator class: <pre><code>from supervision import *\nANNOTATOR_TYPE2CLASS = {\n    \"bounding_box\": BoundingBoxAnnotator,\n    \"mask\": MaskAnnotator,\n    \"polygon\": PolygonAnnotator,\n    \"color\": ColorAnnotator,\n    \"halo\": HaloAnnotator,\n    \"ellipse\": EllipseAnnotator,\n    \"box_corner\": BoxCornerAnnotator,\n    \"circle\": CircleAnnotator,\n    \"dot\": DotAnnotator,\n    \"label\": LabelAnnotator,\n    \"blur\": BlurAnnotator,\n    \"trace\": TraceAnnotator,\n    \"heat_map\": HeatMapAnnotator,\n    \"pixelate\": PixelateAnnotator,\n    \"triangle\": TriangleAnnotator,\n}\n</code></pre> <code>param</code> is a dictionary of annotator constructor parameters (check them in  <code>supervision</code> docs - you would only be able to use primitive values, classes and enums that are defined in constructors may not be possible to resolve from yaml config).</p> <p><code>tracking</code> is an optional key that holds a dictionary with constructor parameters for <code>ByteTrack</code>.</p>"},{"location":"inference_helpers/inference_cli/#configuration-of-model","title":"Configuration of model","text":"<p><code>-mc</code> parameter can be provided with path to <code>*.yml</code> file that specifies  model configuration (like confidence threshold or IoU threshold). If given, configuration will be used to initialise <code>InferenceConfiguration</code> object from <code>inference_sdk</code> library. See sdk docs to discover which options can be configured via <code>*.yml</code> file - configuration keys must match with names of fields in <code>InferenceConfiguration</code> object.</p>"},{"location":"inference_helpers/inference_cli/#inference-benchmark","title":"inference benchmark","text":"<p>Note</p> <p>The command is introduced in <code>inference_cli&gt;=0.9.10</code></p> <p><code>inference benchmark</code> is a set of command suited to run benchmarks of <code>inference</code>. There are two types of benchmark  available <code>inference benchmark api-speed</code> - to test <code>inference</code> HTTP server and <code>inference benchmark python-package-speed</code> to verify the performance of <code>inference</code> Python package.</p> <p>Tip</p> <p>Use <code>inference benchmark api-speed --help</code> / <code>inference benchmark python-package-speed --help</code> to display all options of benchmark commands.</p> <p>Tip</p> <p>Roboflow API key can be provided via <code>ROBOFLOW_API_KEY</code> environment variable</p>"},{"location":"inference_helpers/inference_cli/#running-benchmark-of-python-package","title":"Running benchmark of Python package","text":"<p>Basic benchmark can be run using the following command: </p> <p><pre><code>inference benchmark python-package-speed \\\n-m {your_model_id} \\\n-d {pre-configured dataset name or path to directory with images} \\\n-o {output_directory}  </code></pre> Command runs specified number of inferences using pointed model and saves statistics (including benchmark  parameter, throughput, latency, errors and platform details) in pointed directory.</p>"},{"location":"inference_helpers/inference_cli/#running-benchmark-of-inference-server","title":"Running benchmark of <code>inference server</code>","text":"<p>Note</p> <p>Before running API benchmark - make sure the server is up and running: <pre><code>inference server start\n</code></pre></p> <p>Basic benchmark can be run using the following command: </p> <p><pre><code>inference benchmark api-speed \\\n-m {your_model_id} \\\n-d {pre-configured dataset name or path to directory with images} \\\n-o {output_directory}  </code></pre> Command runs specified number of inferences using pointed model and saves statistics (including benchmark  parameter, throughput, latency, errors and platform details) in pointed directory.</p> <p>This benchmark has more configuration options to support different ways HTTP API profiling. In default mode, single client will be spawned, and it will send one request after another sequentially. This may be suboptimal in specific cases, so one may specify number of concurrent clients using <code>-c {number_of_clients}</code> option. Each client will send next request once previous is handled. This option will also not cover all scenarios of tests. For instance one may want to send <code>x</code> requests each second (which is closer to the scenario of production environment where multiple clients are sending requests concurrently). In this scenario, <code>--rps {value}</code>  option can be used (and <code>-c</code> will be ignored). Value provided in <code>--rps</code> option specifies how many requests  are to be spawned each second without waiting for previous requests to be handled. In I/O intensive benchmark  scenarios - we suggest running command from multiple separate processes and possibly multiple hosts.</p>"},{"location":"inference_helpers/inference_cli/#supported-devices","title":"Supported Devices","text":"<p>Roboflow Inference CLI currently supports the following device targets:</p> <ul> <li>x86 CPU</li> <li>ARM64 CPU</li> <li>NVIDIA GPU</li> </ul> <p>For Jetson specific inference server images, check out the Roboflow Inference package, or pull the images directly following instructions in the official Roboflow Inference documentation.</p>"},{"location":"inference_helpers/inference_landing_page/","title":"Inference Landing Page","text":"<p>The Roboflow Inference server hosts a landing page. This page contains links to helpful resources including documentation and examples.</p>"},{"location":"inference_helpers/inference_landing_page/#visit-the-inference-landing-page","title":"Visit the Inference Landing Page","text":"<p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>The easiest way to start an inference server is with the inference CLI. Install it via pip:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Now run the <code>inference sever start</code> command.</p> <pre><code>inference server start\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. This page contains links to resources and examples related to <code>inference</code>.</p>"},{"location":"inference_helpers/inference_landing_page/#inference-notebook","title":"Inference Notebook","text":"<p>Roboflow Inference Servers come equipped with a built in Jupyterlab environment. This environment is the fastest way to get up and running with inference for development and testing. To use it, first start an inference server. Be sure to specify the <code>--dev</code> flag so that the notebook environment is enabled (it is disabled by default).</p> <pre><code>inference server start --dev\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. From the landing page, select the button labeled \"Jump Into an Inference Enabled Notebook\" to open a new tab for the Jupyterlab environment.</p> <p>This Jupyterlab environment comes preloaded with several example notebooks and all of the dependencies needed to run <code>inference</code>.</p>"},{"location":"inference_helpers/inference_sdk/","title":"Inference Client","text":"<p>The <code>InferenceHTTPClient</code> enables you to interact with Inference over HTTP.</p> <p>You can use this client to run models hosted:</p> <ol> <li>On the Roboflow platform (use client version <code>v0</code>), and;</li> <li>On device with Inference.</li> </ol> <p>For models trained at Roboflow platform, client accepts the following inputs:</p> <ul> <li>A single image (Given as a local path, URL, <code>np.ndarray</code> or <code>PIL.Image</code>);</li> <li>Multiple images;</li> <li>A directory of images, or;</li> <li>A video file.</li> <li>Single image encoded as <code>base64</code></li> </ul> <p>For core model - client exposes dedicated methods to be used, but standard image loader used accepts file paths, URLs, <code>np.ndarray</code> and <code>PIL.Image</code> formats. Apart from client version (<code>v0</code> or <code>v1</code>) - options provided via configuration are used against models trained at the platform, not the core models.</p> <p>The client returns a dictionary of predictions for each image or frame.</p> <p>Starting from <code>0.9.10</code> - <code>InferenceHTTPClient</code> provides async equivalents for the majority of methods and support for requests parallelism and batching implemented (yet in limited scope, not for all methods).  Further details to be found in specific sections of this document. </p> <p>Tip</p> <p>Read our Run Model on an Image guide to learn how to run a model with the Inference Client.</p>"},{"location":"inference_helpers/inference_sdk/#quickstart","title":"Quickstart","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\nresult = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#asyncio-client","title":"AsyncIO client","text":"<pre><code>import asyncio\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\nloop = asyncio.get_event_loop()\nresult = loop.run_until_complete(\n  CLIENT.infer_async(image_url, model_id=\"soccer-players-5fuqs/1\")\n)\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#configuration-options-used-for-models-trained-at-roboflow-platform","title":"Configuration options (used for models trained at Roboflow platform)","text":""},{"location":"inference_helpers/inference_sdk/#configuring-with-context-managers","title":"configuring with context managers","text":"<p>Methods <code>use_configuration(...)</code>, <code>use_api_v0(...)</code>, <code>use_api_v1(...)</code>, <code>use_model(...)</code> are designed to work in context managers. Once context manager is left - old config values are restored.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.8)\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nwith CLIENT.use_api_v0():\n    _ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\nwith CLIENT.use_configuration(custom_configuration):\n    _ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\nwith CLIENT.use_model(\"soccer-players-5fuqs/1\"):\n    _ = CLIENT.infer(image_url)\n\n# after leaving context manager - changes are reverted and `model_id` is still required\n_ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n</code></pre> <p>As you can see - <code>model_id</code> is required to be given for prediction method only when default model is not configured.</p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"inference_helpers/inference_sdk/#setting-the-configuration-once-and-using-till-next-change","title":"Setting the configuration once and using till next change","text":"<p>Methods <code>configure(...)</code>, <code>select_api_v0(...)</code>, <code>select_api_v1(...)</code>, <code>select_model(...)</code> are designed alter the client state and will be preserved until next change.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.8)\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.select_api_v0()\n_ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\n# API v0 still holds\nCLIENT.configure(custom_configuration)\nCLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\n# API v0 and custom configuration still holds\nCLIENT.select_model(model_id=\"soccer-players-5fuqs/1\")\n_ = CLIENT.infer(image_url)\n\n# API v0, custom configuration and selected model - still holds\n_ = CLIENT.infer(image_url)\n</code></pre> <p>One may also initialise in <code>chain</code> mode:</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(api_url=\"http://localhost:9001\", api_key=\"ROBOFLOW_API_KEY\") \\\n    .select_api_v0() \\\n    .select_model(\"soccer-players-5fuqs/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#overriding-model_id-for-specific-call","title":"Overriding <code>model_id</code> for specific call","text":"<p><code>model_id</code> can be overriden for specific call</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(api_url=\"http://localhost:9001\", api_key=\"ROBOFLOW_API_KEY\") \\\n    .select_model(\"soccer-players-5fuqs/1\")\n\n_ = CLIENT.infer(image_url, model_id=\"another-model/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#parallel-batch-inference","title":"Parallel / Batch inference","text":"<p>You may want to predict against multiple images at single call. There are two parameters of <code>InferenceConfiguration</code> that specifies batching and parallelism options: - <code>max_concurrent_requests</code> - max number of concurrent requests that can be started  - <code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only  support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code> to prevent errors)</p> <p>Thanks to that the following improvements can be achieved: - if you run inference container with API on prem on powerful GPU machine - setting <code>max_batch_size</code> properly may bring performance / throughput benefits - if you run inference against hosted Roboflow API - setting <code>max_concurrent_requests</code> will cause multiple images being served at once bringing performance / throughput benefits - combination of both options can be beneficial for clients running inference container with API on cluster of machines, then the load of single node can be optimised and parallel requests to different nodes can be made at a time  ``</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\npredictions = CLIENT.infer([image_url] * 5, model_id=\"soccer-players-5fuqs/1\")\n\nprint(predictions)\n</code></pre> <p>Methods that support batching / parallelism: -<code>infer(...)</code> and <code>infer_async(...)</code> - <code>infer_from_api_v0(...)</code> and <code>infer_from_api_v0_async(...)</code> (enforcing <code>max_batch_size=1</code>) - <code>ocr_image(...)</code> and <code>ocr_image_async(...)</code> (enforcing <code>max_batch_size=1</code>) - <code>detect_gazes(...)</code> and <code>detect_gazes_async(...)</code> - <code>get_clip_image_embeddings(...)</code> and <code>get_clip_image_embeddings_async(...)</code></p>"},{"location":"inference_helpers/inference_sdk/#client-for-core-models","title":"Client for core models","text":"<p><code>InferenceHTTPClient</code> now supports core models hosted via <code>inference</code>. Part of the models can be used at Roboflow hosted inference platform (use <code>https://infer.roboflow.com</code> as url), other are possible to be deployed locally (usually local server will be available under <code>http://localhost:9001</code>).</p> <p>Tip</p> <p>Install <code>inference-cli</code> package to easily run <code>inference</code> API locally <pre><code>pip install inference-cli\ninference server start\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#clip","title":"Clip","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.get_clip_image_embeddings(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.get_clip_image_embeddings(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\nCLIENT.get_clip_text_embeddings(text=\"some\")  # single text request\nCLIENT.get_clip_text_embeddings(text=[\"some\", \"other\"])  # other text request\nCLIENT.clip_compare(\n    subject=\"./my_image.jpg\",\n    prompt=[\"fox\", \"dog\"],\n)\n</code></pre> <p><code>CLIENT.clip_compare(...)</code> method allows to compare different combination of <code>subject_type</code> and <code>prompt_type</code>:</p> <ul> <li><code>(image, image)</code></li> <li><code>(image, text)</code></li> <li><code>(text, image)</code></li> <li><code>(text, text)</code>   Default mode is <code>(image, text)</code>.</li> </ul> <p>Tip</p> <p>Check out async methods for Clip model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.get_clip_image_embeddings_async(inference_input=\"./my_image.jpg\")  # single image request\n  await CLIENT.get_clip_image_embeddings_async(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n  await CLIENT.get_clip_text_embeddings_async(text=\"some\")  # single text request\n  await CLIENT.get_clip_text_embeddings_async(text=[\"some\", \"other\"])  # other text request\n  await CLIENT.clip_compare_async(\n      subject=\"./my_image.jpg\",\n      prompt=[\"fox\", \"dog\"],\n  )\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#cogvlm","title":"CogVLM","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.prompt_cogvlm(\n    visual_prompt=\"./my_image.jpg\",\n    text_prompt=\"So - what is your final judgement about the content of the picture?\",\n    chat_history=[(\"I think the image shows XXX\", \"You are wrong - the image shows YYY\")], # optional parameter\n)\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#doctr","title":"DocTR","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.ocr_image(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.ocr_image(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n</code></pre> <p>Tip</p> <p>Check out async methods for DocTR model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.ocr_image(inference_input=\"./my_image.jpg\")  # single image request\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#gaze","title":"Gaze","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.detect_gazes(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.detect_gazes(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n</code></pre> <p>Tip</p> <p>Check out async methods for Gaze model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.detect_gazes(inference_input=\"./my_image.jpg\")  # single image request\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#inference-against-stream","title":"Inference against stream","text":"<p>One may want to infer against video or directory of images - and that modes are supported in <code>inference-client</code></p> <pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nfor frame_id, frame, prediction in CLIENT.infer_on_stream(\"video.mp4\", model_id=\"soccer-players-5fuqs/1\"):\n    # frame_id is the number of frame\n    # frame - np.ndarray with video frame\n    # prediction - prediction from the model\n    pass\n\nfor file_path, image, prediction in CLIENT.infer_on_stream(\"local/dir/\", model_id=\"soccer-players-5fuqs/1\"):\n    # file_path - path to the image\n    # frame - np.ndarray with video frame\n    # prediction - prediction from the model\n    pass\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#what-is-actually-returned-as-prediction","title":"What is actually returned as prediction?","text":"<p><code>inference_client</code> returns plain Python dictionaries that are responses from model serving API. Modification is done only in context of <code>visualization</code> key that keep server-generated prediction visualisation (it can be transcoded to the format of choice) and in terms of client-side re-scaling.</p>"},{"location":"inference_helpers/inference_sdk/#methods-to-control-inference-server-in-v1-mode-only","title":"Methods to control <code>inference</code> server (in <code>v1</code> mode only)","text":""},{"location":"inference_helpers/inference_sdk/#getting-server-info","title":"Getting server info","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.get_server_info()\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#listing-loaded-models","title":"Listing loaded models","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.list_loaded_models()\n</code></pre> <p>Tip</p> <p>This method has async equivaluent: <code>list_loaded_models_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#getting-specific-model-description","title":"Getting specific model description","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.get_model_description(model_id=\"some/1\", allow_loading=True)\n</code></pre> <p>If <code>allow_loading</code> is set to <code>True</code>: model will be loaded as side-effect if it is not already loaded. Default: <code>True</code>.</p> <p>Tip</p> <p>This method has async equivaluent: <code>get_model_description_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#loading-model","title":"Loading model","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.load_model(model_id=\"some/1\", set_as_default=True)\n</code></pre> <p>The pointed model will be loaded. If <code>set_as_default</code> is set to <code>True</code>: after successful load, model will be used as default model for the client. Default value: <code>False</code>.</p> <p>Tip</p> <p>This method has async equivaluent: <code>load_model_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#unloading-model","title":"Unloading model","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.unload_model(model_id=\"some/1\")\n</code></pre> <p>Sometimes (to avoid OOM at server side) - unloading model will be required. test_postprocessing.py</p> <p>Tip</p> <p>This method has async equivaluent: <code>unload_model_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#unloading-all-models","title":"Unloading all models","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.unload_all_models()\n</code></pre> <p>Tip</p> <p>This method has async equivaluent: <code>unload_all_models_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#inference-workflows","title":"Inference <code>workflows</code>","text":"<p>Tip</p> <p>This feature is in <code>alpha</code> preview. We encourage you to experiment and reach out to us with issues spotted. Check out documentation of deployment specs, create one and run</p> <p>Tip</p> <p>This feature only works with locally hosted inference container and hosted platform (access may be limited).  Use inefernce-cli to run local container with HTTP API: <pre><code>inference server start\n</code></pre></p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    \"http://127.0.0.1:9001\",\n    \"XXX\",\n)\n\nCLIENT.infer_from_workflow(\n    specification={\n        \"version\": \"1.0\",\n        \"inputs\": [\n            {\"type\": \"InferenceImage\", \"name\": \"image\"},\n            {\"type\": \"InferenceParameter\", \"name\": \"my_param\"},\n        ],\n        # ...\n    },\n    images={\n        \"image\": \"url or your np.array\",\n    },\n    parameters={\n        \"my_param\": 37,\n    },\n)\n</code></pre> <p>Please note that either <code>specification</code> is provided with specification of workflow as described here or  both <code>workspace_name</code> and <code>workflow_name</code> are given to use workflow predefined in Roboflow app. <code>workspace_name</code> can be found in Roboflow APP URL once browser shows the main panel of workspace. </p>"},{"location":"inference_helpers/inference_sdk/#details-about-client-configuration","title":"Details about client configuration","text":"<p><code>inference-client</code> provides <code>InferenceConfiguration</code> dataclass to hold whole configuration.</p> <pre><code>from inference_sdk import InferenceConfiguration\n</code></pre> <p>Overriding fields in this config changes the behaviour of client (and API serving model). Specific fields are used in specific contexts. In particular:</p>"},{"location":"inference_helpers/inference_sdk/#inference-in-v0-mode","title":"Inference in <code>v0</code> mode","text":"<p>The following fields are passed to API</p> <ul> <li><code>confidence_threshold</code> (as <code>confidence</code>) - to alter model thresholding</li> <li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints   based on model confidence</li> <li><code>format</code>: to visualise on server side - use <code>image</code> (but then you loose prediction details from response)</li> <li><code>visualize_labels</code> (as <code>labels</code>) - used in visualisation to show / hide labels for classes</li> <li><code>mask_decode_mode</code></li> <li><code>tradeoff_factor</code></li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>iou_threshold</code> (as <code>overlap</code>) - to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>count_inference</code> as <code>countinference</code></li> <li><code>service_secret</code></li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#classification-model-in-v1-mode","title":"Classification model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li> <p><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></p> </li> <li> <p><code>visualize_predictions</code>: flag to enable / disable visualisation</p> </li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li> <p><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful, for instance, while testing the model)</p> </li> <li> <p><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</p> </li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> </ul>"},{"location":"inference_helpers/inference_sdk/#object-detection-model-in-v1-mode","title":"Object detection model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>class_filter</code> to filter out list of classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#keypoints-detection-model-in-v1-mode","title":"Keypoints detection model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints   based on model confidence</li> <li><code>class_filter</code> to filter out list of object classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#instance-segmentation-model-in-v1-mode","title":"Instance segmentation model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>class_filter</code> to filter out list of classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>mask_decode_mode</code></li> <li><code>tradeoff_factor</code></li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#configuration-of-client","title":"Configuration of client","text":"<ul> <li><code>output_visualisation_format</code>: one of (<code>VisualisationResponseFormat.BASE64</code>, <code>VisualisationResponseFormat.NUMPY</code>,   <code>VisualisationResponseFormat.PILLOW</code>) - given that server-side visualisation is enabled - one may choose what   format should be used in output</li> <li><code>image_extensions_for_directory_scan</code>: while using <code>CLIENT.infer_on_stream(...)</code> with local directory   this parameter controls type of files (extensions) allowed to be processed -   default: <code>[\"jpg\", \"jpeg\", \"JPG\", \"JPEG\", \"png\", \"PNG\"]</code></li> <li><code>client_downsizing_disabled</code>: set to <code>True</code> if you want to avoid client-side downsizing - default <code>False</code>.   Client-side scaling is only supposed to down-scale (keeping aspect-ratio) the input for inference -   to utilise internet connection more efficiently (but for the price of images manipulation / transcoding).   If model registry endpoint is available (mode <code>v1</code>) - model input size information will be used, if not:   <code>default_max_input_size</code> will be in use.</li> <li><code>max_concurrent_requests</code> - max number of concurrent requests that can be started </li> <li><code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only  support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code> to prevent errors)</li> </ul>"},{"location":"inference_helpers/inference_sdk/#faqs","title":"FAQs","text":""},{"location":"inference_helpers/inference_sdk/#why-does-the-inference-client-have-two-modes-v0-and-v1","title":"Why does the Inference client have two modes (<code>v0</code> and <code>v1</code>)?","text":"<p>We are constantly improving our <code>infrence</code> package - initial version (<code>v0</code>) is compatible with models deployed at Roboflow platform (task types: <code>classification</code>, <code>object-detection</code>, <code>instance-segmentation</code> and <code>keypoints-detection</code>) are supported. Version <code>v1</code> is available in locally hosted Docker images with HTTP API.</p> <p>Locally hosted <code>inference</code> server exposes endpoints for model manipulations, but those endpoints are not available at the moment for models deployed at Roboflow platform.</p> <p><code>api_url</code> parameter passed to <code>InferenceHTTPClient</code> will decide on default client mode - URLs with <code>*.roboflow.com</code> will be defaulted to version <code>v0</code>.</p> <p>Usage of model registry control methods with <code>v0</code> clients will raise <code>WrongClientModeError</code>.</p>"},{"location":"models/from_local_weights/","title":"From Local Weights","text":"<p>You can upload supported weights to Roboflow and deploy them to your device.</p> <p>This is ideal if you have already trained a model outside of Roboflow that you want to deploy with Inference.</p> <p>To upload weights to Roboflow, you will need:</p> <ol> <li>A Roboflow account</li> <li>A project with your dataset (that does not have a trained model)</li> </ol> <p>To learn how to create a project and a dataset, refer to these guides:</p> <ul> <li>Create a project</li> <li>Create a dataset</li> </ul> <p>Once you have a project with a dataset, you can upload your weights.</p> <p>Install the Roboflow Python package:</p> <pre><code>pip install roboflow\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.project(\"your-project-id\")\nversion = project.version(1)\nversion.deploy(\"model-type\", \"path/to/training/results/\")\n</code></pre> <p>The following model types are supported:</p> <ul> <li>yolov5, yolov5-seg</li> <li>yolov7-seg</li> <li>yolov8, yolov8-seg, yolov8-cls</li> </ul> <p>In the code above, replace:</p> <ol> <li><code>your-project-id</code> with the ID of your project. Learn how to retrieve your Roboflow project ID.</li> <li><code>1</code> with the version number of your project.</li> <li><code>model-type</code> with the model type you want to deploy.</li> <li><code>path/to/training/results/</code> with the path to the weights you want to upload. This path will vary depending on what model architecture you are using.</li> </ol> <p>Your model weights will be uploaded to Roboflow. It may take a few minutes for your weights to be processed. Once your weights have been processed, your dataset version page will be updated to say that a model is available with your weights.</p> <p>You can then use the model with Inference following our Run a Private, Fine-Tuned Model model.</p>"},{"location":"models/supported_models/","title":"Supported models","text":"<p>There are three ways you can deploy models with Inference:</p> <ol> <li>Deploy a model hosted on Roboflow onto your device.</li> <li>Upload suported model weights to Roboflow, then deploy to your device.</li> <li>Deploy a foundation model out-of-the-box.</li> </ol> <p>The pages linked above will walk you through each of these options.</p>"},{"location":"models/supported_models/#supported-custom-models","title":"Supported Custom Models","text":"<p>The following custom models are supported:</p> <ul> <li>YOLOv5 (Segmentation, Object Detection)</li> <li>YOLOv7 (Segmentation)</li> <li>YOLOv8 (Classification, Segmentation, Object Detection)</li> <li>YOLO-NAS (Coming soon)</li> </ul>"},{"location":"models/supported_models/#supported-foundation-models","title":"Supported Foundation Models","text":"<ul> <li>CLIP</li> <li>CogVLM</li> <li>DocTR</li> <li>Grounding DINO</li> <li>L2CS-Net</li> <li>Segment Anything</li> <li>YOLOWorld</li> </ul>"},{"location":"notebooks/clip_classification/","title":"CLIP Classify Content of Video","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install supervision opencv-python\n</pre> !pip install supervision opencv-python In\u00a0[25]: Copied! <pre>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport supervision as sv\nfrom tqdm import tqdm\nfrom supervision import get_video_frames_generator\nimport time\n\nINFERENCE_ENDPOINT = \"https://infer.roboflow.com\"\nAPI_KEY = \"YOUR_API_KEY\"\nVIDEO = \"VIDEO_PATH\"\n</pre> import requests import base64 from PIL import Image from io import BytesIO import os import supervision as sv from tqdm import tqdm from supervision import get_video_frames_generator import time  INFERENCE_ENDPOINT = \"https://infer.roboflow.com\" API_KEY = \"YOUR_API_KEY\" VIDEO = \"VIDEO_PATH\"   In\u00a0[\u00a0]: Copied! <pre>#Prompt list to evaluate similarity between each image and each prompt. If something else is selected, then we ignore the caption\n#change this to your desired prompt list\nprompt_list = [['action video game shooting xbox','Drake rapper music','soccer game ball',\n                'marvel combic book','beyonce','Church pope praying',\n                'Mcdonalds French Fries',\"something else\"]]\n</pre> #Prompt list to evaluate similarity between each image and each prompt. If something else is selected, then we ignore the caption #change this to your desired prompt list prompt_list = [['action video game shooting xbox','Drake rapper music','soccer game ball',                 'marvel combic book','beyonce','Church pope praying',                 'Mcdonalds French Fries',\"something else\"]] In\u00a0[26]: Copied! <pre>def classify_image(image: str, prompt: str) -&gt; dict:\n    \n    image_data = Image.fromarray(image)\n\n    buffer = BytesIO()\n    image_data.save(buffer, format=\"JPEG\")\n    image_data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n    payload = {\n        \"api_key\": API_KEY,\n        \"subject\": {\n            \"type\": \"base64\",\n            \"value\": image_data\n        },\n        \"prompt\": prompt,\n    }\n\n    data = requests.post(INFERENCE_ENDPOINT + \"/clip/compare?api_key=\" + API_KEY, json=payload)\n\n    response = data.json()\n    #print(response[\"similarity\"])\n    sim = response[\"similarity\"]\n\n    highest_prediction = 0\n    highest_prediction_index = 0\n\n    for i, prediction in enumerate(response[\"similarity\"]):\n        if prediction &gt; highest_prediction:\n            highest_prediction = prediction\n            highest_prediction_index = i\n\n    return prompt[highest_prediction_index], sim[highest_prediction_index]\n</pre> def classify_image(image: str, prompt: str) -&gt; dict:          image_data = Image.fromarray(image)      buffer = BytesIO()     image_data.save(buffer, format=\"JPEG\")     image_data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")      payload = {         \"api_key\": API_KEY,         \"subject\": {             \"type\": \"base64\",             \"value\": image_data         },         \"prompt\": prompt,     }      data = requests.post(INFERENCE_ENDPOINT + \"/clip/compare?api_key=\" + API_KEY, json=payload)      response = data.json()     #print(response[\"similarity\"])     sim = response[\"similarity\"]      highest_prediction = 0     highest_prediction_index = 0      for i, prediction in enumerate(response[\"similarity\"]):         if prediction &gt; highest_prediction:             highest_prediction = prediction             highest_prediction_index = i      return prompt[highest_prediction_index], sim[highest_prediction_index] In\u00a0[\u00a0]: Copied! <pre>def process_video_frames(video_path, prompt_list, total_frames=160, total_seconds=80, stride_length=30,max_retries):\n    if not os.path.exists(video_path):\n        print(f\"The specified video file '{video_path}' does not exist.\")\n        return\n\n    frames_per_second = total_frames / total_seconds\n    frame_dict = {}\n\n    for frame_index, frame in enumerate(sv.get_video_frames_generator(source_path=video_path, stride=stride_length, start=0)):\n        frame_second = frame_index * (1 / frames_per_second)\n        frame_key = f\"Frame {frame_index}: {frame_second:.2f} seconds\"\n        frame_dict[frame_key] = []\n\n        print(frame_key)\n        retries = 0\n\n        for prompt in prompt_list:\n            try: \n                label, similarity = classify_image(frame)\n                if label != \"something else\":\n                    print('label found')\n                    frame_dict[frame_key].append({label: similarity})\n                    print('\\n')\n\n            except Exception as e:\n                retries += 1\n                print(f\"Error: {e}\")\n                print(f\"Retrying... (Attempt {retries}/{max_retries})\")\n\n                if retries &gt;= max_retries:\n                    print(\"Max retries exceeded. Skipping frame.\")\n                    break\n\n    return frame_dict\n\n# Example usage:\nmax_retries = 4\nprompt_list = prompt_list\nclip_results = process_video_frames(VIDEO, prompt_list,max_retries)\n</pre>  def process_video_frames(video_path, prompt_list, total_frames=160, total_seconds=80, stride_length=30,max_retries):     if not os.path.exists(video_path):         print(f\"The specified video file '{video_path}' does not exist.\")         return      frames_per_second = total_frames / total_seconds     frame_dict = {}      for frame_index, frame in enumerate(sv.get_video_frames_generator(source_path=video_path, stride=stride_length, start=0)):         frame_second = frame_index * (1 / frames_per_second)         frame_key = f\"Frame {frame_index}: {frame_second:.2f} seconds\"         frame_dict[frame_key] = []          print(frame_key)         retries = 0          for prompt in prompt_list:             try:                  label, similarity = classify_image(frame)                 if label != \"something else\":                     print('label found')                     frame_dict[frame_key].append({label: similarity})                     print('\\n')              except Exception as e:                 retries += 1                 print(f\"Error: {e}\")                 print(f\"Retrying... (Attempt {retries}/{max_retries})\")                  if retries &gt;= max_retries:                     print(\"Max retries exceeded. Skipping frame.\")                     break      return frame_dict  # Example usage: max_retries = 4 prompt_list = prompt_list clip_results = process_video_frames(VIDEO, prompt_list,max_retries)  In\u00a0[\u00a0]: Copied! <pre># Flatten the nested dictionary\ndata = clip_results\n# Define the threshold based on the similarity score returned for the most similar prompt\nthreshold = 0.22\n\n# Filter out key-value pairs below the threshold for each frame\nfiltered_data = [\n    {\n        frame: [\n            {key: value}\n            for item in items\n            for key, value in item.items()\n            if value &gt; threshold\n        ]\n    }\n    for frame, items in data.items()\n]\nprint(filtered_data)\n</pre> # Flatten the nested dictionary data = clip_results # Define the threshold based on the similarity score returned for the most similar prompt threshold = 0.22  # Filter out key-value pairs below the threshold for each frame filtered_data = [     {         frame: [             {key: value}             for item in items             for key, value in item.items()             if value &gt; threshold         ]     }     for frame, items in data.items() ] print(filtered_data) In\u00a0[44]: Copied! <pre># Specify the filename for the JSON file\nimport json\nfilename = f\"{str(threshold)}.json\"\n\n# Write the dictionary to the JSON file\nwith open(filename, 'w') as json_file:\n    json.dump(filtered_data, json_file, indent=4)  # The indent parameter is optional for pretty-printing\n\n#print(f'Data has been written to {filename})\n</pre> # Specify the filename for the JSON file import json filename = f\"{str(threshold)}.json\"  # Write the dictionary to the JSON file with open(filename, 'w') as json_file:     json.dump(filtered_data, json_file, indent=4)  # The indent parameter is optional for pretty-printing  #print(f'Data has been written to {filename})"},{"location":"notebooks/clip_classification/#clip-classify-content-of-video","title":"CLIP Classify Content of Video\u00b6","text":"<p>CLIP is a powerful foundation model for zero-shot classification. In this scenario, we are using CLIP to classify the topics in a Youtube video. Plug in your own video and set of prompts!</p> <p>Click the Open in Colab button to run the cookbook on Google Colab.</p> <p>Let's begin!</p>"},{"location":"notebooks/clip_classification/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>opencv</code> and <code>supervision</code></p>"},{"location":"notebooks/clip_classification/#imports-configure-roboflow-inference-server","title":"Imports &amp; Configure Roboflow Inference Server\u00b6","text":""},{"location":"notebooks/clip_classification/#prompt-list-for-clip-similarity-function","title":"Prompt List for CLIP similarity function\u00b6","text":""},{"location":"notebooks/clip_classification/#clip-endpoint-compare-frame-prompt-list-similarity","title":"CLIP Endpoint Compare Frame &amp; Prompt List Similarity\u00b6","text":""},{"location":"notebooks/clip_classification/#process-video-return-most-similar-prompt-to-frame","title":"Process Video &amp; Return Most Similar Prompt to Frame\u00b6","text":""},{"location":"notebooks/clip_classification/#create-json-file-and-filter-out-low-similarity-classes","title":"Create JSON file and filter out low similarity classes\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/","title":"InferencePipeline on RTSP Stream","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install inference supervision==0.18.0\n</pre> !pip install inference supervision==0.18.0 In\u00a0[\u00a0]: Copied! <pre>from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\nimport supervision as sv\nimport pandas as pd\nfrom collections import defaultdict\nimport cv2\nimport numpy as np\nimport time\n</pre> from inference.core.interfaces.stream.inference_pipeline import InferencePipeline from inference.core.interfaces.stream.sinks import render_boxes import supervision as sv import pandas as pd from collections import defaultdict import cv2 import numpy as np import time  In\u00a0[\u00a0]: Copied! <pre># Create an instance of FPSMonitor\nfps_monitor = sv.FPSMonitor()\n\nREGISTERED_ALIASES = {\n    \"yolov8n-640\": \"coco/3\",\n    \"yolov8n-1280\": \"coco/9\",\n    \"yolov8m-640\": \"coco/8\"\n}\n\nAPI_KEY = \"API_KEY\"\nRTSP_STREAM = \"RTSP_URL\"\n\n# Example alias\nalias = \"yolov8n-640\"\n\n# Function to resolve an alias to the actual model ID\ndef resolve_roboflow_model_alias(model_id: str) -&gt; str:\n    return REGISTERED_ALIASES.get(model_id, model_id)\n\n# Resolve the alias to get the actual model ID\nmodel_name = resolve_roboflow_model_alias(alias)\n\n# Modify the render_boxes function to enable displaying statistics\ndef on_prediction(predictions, video_frame):\n    render_boxes(\n        predictions=predictions,\n        video_frame=video_frame,\n        fps_monitor=fps_monitor,  # Pass the FPS monitor object\n        display_statistics=True,   # Enable displaying statistics\n    )\n    \npipeline = InferencePipeline.init(\n    model_id= model_name,\n    video_reference=RTSP_STREAM,\n    on_prediction=on_prediction,\n    api_key=API_KEY,\n    confidence=0.5,\n)\n\npipeline.start()\npipeline.join()\n</pre> # Create an instance of FPSMonitor fps_monitor = sv.FPSMonitor()  REGISTERED_ALIASES = {     \"yolov8n-640\": \"coco/3\",     \"yolov8n-1280\": \"coco/9\",     \"yolov8m-640\": \"coco/8\" }  API_KEY = \"API_KEY\" RTSP_STREAM = \"RTSP_URL\"  # Example alias alias = \"yolov8n-640\"  # Function to resolve an alias to the actual model ID def resolve_roboflow_model_alias(model_id: str) -&gt; str:     return REGISTERED_ALIASES.get(model_id, model_id)  # Resolve the alias to get the actual model ID model_name = resolve_roboflow_model_alias(alias)  # Modify the render_boxes function to enable displaying statistics def on_prediction(predictions, video_frame):     render_boxes(         predictions=predictions,         video_frame=video_frame,         fps_monitor=fps_monitor,  # Pass the FPS monitor object         display_statistics=True,   # Enable displaying statistics     )      pipeline = InferencePipeline.init(     model_id= model_name,     video_reference=RTSP_STREAM,     on_prediction=on_prediction,     api_key=API_KEY,     confidence=0.5, )  pipeline.start() pipeline.join() In\u00a0[\u00a0]: Copied! <pre>#ByteTrack &amp; Supervision\ntracker = sv.ByteTrack()\nannotator = sv.BoxAnnotator()\nframe_count = defaultdict(int)\ncolors = sv.ColorPalette.default()\n\n#define polygon zone of interest\npolygons = [\nnp.array([\n[390, 543],[1162, 503],[1510, 711],[410, 819],[298, 551],[394, 543]\n])\n]\n\n#create zones, zone_annotator, and box_annotator based on polygon zone of interest\nzones = [\n    sv.PolygonZone(\n        polygon=polygon,\n        frame_resolution_wh=[1440,2560],\n    )\n    for polygon\n    in polygons\n]\nzone_annotators = [\n    sv.PolygonZoneAnnotator(\n        zone=zone,\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=8,\n        text_scale=4\n    )\n    for index, zone\n    in enumerate(zones)\n]\nbox_annotators = [\n    sv.BoxAnnotator(\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=4,\n        text_scale=2\n        )\n    for index\n    in range(len(polygons))\n]\n\n\n#columns for csv output\ncolumns = ['trackerID', 'class_id', 'frame_count','entry_timestamp','exit_timestamp','time_in_zone']\nframe_count_df = pd.DataFrame(columns=columns)\n\n# Define a dictionary to store the first detection timestamp for each tracker_id\nfirst_detection_timestamps = {}\nlast_detection_timestamps = {}\n\ndef render(predictions: dict, video_frame) -&gt; None:\n    detections = sv.Detections.from_roboflow(predictions)\n    detections = tracker.update_with_detections(detections)\n    \n    for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n        mask = zone.trigger(detections=detections)\n        detections_filtered = detections[mask]\n        \n        image = box_annotator.annotate(scene=video_frame.image, detections=detections, skip_label=False)\n        image = zone_annotator.annotate(scene=image)\n        \n        for tracker_id, class_id in zip(detections_filtered.tracker_id, detections_filtered.class_id):\n            frame_count[tracker_id] += 1\n            \n            # Check if tracker_id is not in first_detection_timestamps, if not, add the timestamp\n            if tracker_id not in first_detection_timestamps:\n                first_detection_timestamps[tracker_id] = time.time()\n            \n            last_detection_timestamps[tracker_id] = time.time()\n            \n            time_difference = last_detection_timestamps[tracker_id] - first_detection_timestamps[tracker_id]\n            \n            # Add data to the DataFrame\n            frame_count_df.loc[tracker_id] = [tracker_id, class_id, frame_count[tracker_id], first_detection_timestamps[tracker_id],last_detection_timestamps[tracker_id], time_difference]\n    \n    frame_count_df.to_csv('demo.csv', index=False)\n    \n    cv2.imshow(\"Prediction\", image)\n    cv2.waitKey(1)\n    \n\n#Initialize &amp; Deploy InferencePipeline\npipeline = InferencePipeline.init(\n    model_id=\"coco/8\",\n    video_reference=\"RTSP_URL\",\n    on_prediction=render,\n    api_key = 'API_KEY',\n    confidence=0.5,\n)\npipeline.start()\npipeline.join()\n</pre> #ByteTrack &amp; Supervision tracker = sv.ByteTrack() annotator = sv.BoxAnnotator() frame_count = defaultdict(int) colors = sv.ColorPalette.default()  #define polygon zone of interest polygons = [ np.array([ [390, 543],[1162, 503],[1510, 711],[410, 819],[298, 551],[394, 543] ]) ]  #create zones, zone_annotator, and box_annotator based on polygon zone of interest zones = [     sv.PolygonZone(         polygon=polygon,         frame_resolution_wh=[1440,2560],     )     for polygon     in polygons ] zone_annotators = [     sv.PolygonZoneAnnotator(         zone=zone,         color=colors.by_idx(index),         thickness=4,         text_thickness=8,         text_scale=4     )     for index, zone     in enumerate(zones) ] box_annotators = [     sv.BoxAnnotator(         color=colors.by_idx(index),         thickness=4,         text_thickness=4,         text_scale=2         )     for index     in range(len(polygons)) ]   #columns for csv output columns = ['trackerID', 'class_id', 'frame_count','entry_timestamp','exit_timestamp','time_in_zone'] frame_count_df = pd.DataFrame(columns=columns)  # Define a dictionary to store the first detection timestamp for each tracker_id first_detection_timestamps = {} last_detection_timestamps = {}  def render(predictions: dict, video_frame) -&gt; None:     detections = sv.Detections.from_roboflow(predictions)     detections = tracker.update_with_detections(detections)          for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):         mask = zone.trigger(detections=detections)         detections_filtered = detections[mask]                  image = box_annotator.annotate(scene=video_frame.image, detections=detections, skip_label=False)         image = zone_annotator.annotate(scene=image)                  for tracker_id, class_id in zip(detections_filtered.tracker_id, detections_filtered.class_id):             frame_count[tracker_id] += 1                          # Check if tracker_id is not in first_detection_timestamps, if not, add the timestamp             if tracker_id not in first_detection_timestamps:                 first_detection_timestamps[tracker_id] = time.time()                          last_detection_timestamps[tracker_id] = time.time()                          time_difference = last_detection_timestamps[tracker_id] - first_detection_timestamps[tracker_id]                          # Add data to the DataFrame             frame_count_df.loc[tracker_id] = [tracker_id, class_id, frame_count[tracker_id], first_detection_timestamps[tracker_id],last_detection_timestamps[tracker_id], time_difference]          frame_count_df.to_csv('demo.csv', index=False)          cv2.imshow(\"Prediction\", image)     cv2.waitKey(1)       #Initialize &amp; Deploy InferencePipeline pipeline = InferencePipeline.init(     model_id=\"coco/8\",     video_reference=\"RTSP_URL\",     on_prediction=render,     api_key = 'API_KEY',     confidence=0.5, ) pipeline.start() pipeline.join()"},{"location":"notebooks/inference_pipeline_rtsp/#inferencepipeline-on-rtsp-stream","title":"InferencePipeline on RTSP Stream\u00b6","text":"<p>The Roboflow Inference Pipeline is a drop-in replacement for the Hosted Inference API that can be deployed on your own hardware. The Inference Pipeline interface is made for streaming and is likely the best route to go for real time use cases. It is an asynchronous interface that can consume many different video sources including local devices (like webcams), RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>We have optimized Inference Pipeline to get maximum performance from the NVIDIA Jetson line of edge-AI devices. We have done this by specifically tailoring the drivers, libraries, and binaries specifically to its CPU and GPU architectures.</p> <p>Let's begin!</p>"},{"location":"notebooks/inference_pipeline_rtsp/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>inference</code> and <code>supervision</code></p>"},{"location":"notebooks/inference_pipeline_rtsp/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/#run-inference-pipeline-with-coco-model-aliases-native-fps-monitor","title":"Run Inference Pipeline with COCO Model Aliases &amp; Native FPS Monitor\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/#time-in-zone-with-bytetrack-using-supervision-save-data-to-csv","title":"Time in Zone with Bytetrack using Supervision, save data to CSV\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/","title":"RGB Anomaly Detection","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install sklearn\n</pre> !pip install sklearn In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport numpy as np\nimport time\nimport base64\nimport requests\nimport os, glob\nfrom sklearn.cluster import KMeans\n</pre> import cv2 import numpy as np import time import base64 import requests import os, glob from sklearn.cluster import KMeans In\u00a0[\u00a0]: Copied! <pre>def parse_polygon_annotation(annotation_data, image_shape):\n    width, height = image_shape[1], image_shape[0]\n    return [(int(data['x']), int(data['y'])) for data in annotation_data]\n\ndef extract_polygon_area(image_path, polygon_points):\n    image = cv2.imread(image_path)\n    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n    cv2.drawContours(mask, [np.array(polygon_points)], -1, (255, 255, 255), -1)\n    return cv2.bitwise_and(image, image, mask=mask)\n\ndef compute_average_color(image):\n    mask = np.all(image != [0, 0, 0], axis=2)\n    avg_color = np.mean(image[mask], axis=0)\n    return avg_color\n\ndef color_difference(color1, color2):\n    return np.linalg.norm(np.array(color1) - np.array(color2))\n\ndef count_color_matches(dominant_colors, target_colors, threshold):\n    matches_count = {tuple(target): 0 for target in target_colors}\n    matched_colors = {tuple(target): [] for target in target_colors}\n    \n    for color in dominant_colors:\n        for target in target_colors:\n            difference = color_difference(color, target)\n            \n            if difference &lt; threshold:\n                matches_count[tuple(target)] += 1\n                matched_colors[tuple(target)].append(color)\n                \n    return matches_count, matched_colors\n\ndef get_dominant_colors(image, k=5):\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    image = image[np.any(image != [0, 0, 0], axis=1)]\n    kmeans = KMeans(n_clusters=k, n_init='auto')\n    kmeans.fit(image)\n    dominant_colors = kmeans.cluster_centers_\n    return dominant_colors\n\ndef extract_target_colors(target_image_path,inference_server_address, project_id, version_number):\n    target_image = cv2.imread(target_image_path)\n    with open(target_image_path, \"rb\") as f:\n        im_bytes = f.read()        \n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json'\n    }\n\n    params = {\n        'api_key': 'FFgkmScNUBERP9t3PJvV',\n    }\n\n    response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)\n    data = response.json()\n\n    for predictions in data['predictions']:\n        Pred_points = predictions['points']\n        target_image = cv2.imread(target_image_path)\n        polygon_points = parse_polygon_annotation(Pred_points, target_image.shape)\n        polygon_image = extract_polygon_area(target_image_path, polygon_points)\n        target_dominant_colors = get_dominant_colors(polygon_image)\n    \n    return target_dominant_colors\n\ndef match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold):\n    global prediction_counter, image_counter\n    total_matches = 0\n    matched_filepaths = []\n\n    extention_images = \".jpg\"\n    get_images = sorted(glob.glob(images_folder + '*' + extention_images))\n\n    for images in get_images:\n        t0 = time.time()\n        print(\"File path: \" + images)\n        img = cv2.imread(images)\n        with open(images, \"rb\") as f:\n            im_bytes = f.read()        \n        im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n        headers = {\n            'Content-Type': 'application/json',\n            'Accept': 'application/json'\n        }\n\n        params = {\n            'api_key': '',\n        }\n        \n        response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)\n        data = response.json()\n\n        for predictions in data['predictions']:\n            prediction_counter += 1\n            image_counter += 1\n            Pred_points = predictions['points']\n            image = cv2.imread(images)\n            polygon_points = parse_polygon_annotation(Pred_points, image.shape)\n            polygon_image = extract_polygon_area(images, polygon_points)\n            dominant_colors = get_dominant_colors(polygon_image)\n            matches, matched_colors_list = count_color_matches(dominant_colors, target_dominant_colors, color_threshold)\n        \n        all_matched = all(value &gt; 0 for value in matches.values())\n        \n        if all_matched:\n            matched_filepaths.append(images)\n            total_matches += 1\n\n    print(f\"\\nTotal images where all target colors matched: {total_matches}\")\n    print(f\"\\nMatched images where all target colors matched: {matched_filepaths}\")\n</pre> def parse_polygon_annotation(annotation_data, image_shape):     width, height = image_shape[1], image_shape[0]     return [(int(data['x']), int(data['y'])) for data in annotation_data]  def extract_polygon_area(image_path, polygon_points):     image = cv2.imread(image_path)     mask = np.zeros(image.shape[:2], dtype=np.uint8)     cv2.drawContours(mask, [np.array(polygon_points)], -1, (255, 255, 255), -1)     return cv2.bitwise_and(image, image, mask=mask)  def compute_average_color(image):     mask = np.all(image != [0, 0, 0], axis=2)     avg_color = np.mean(image[mask], axis=0)     return avg_color  def color_difference(color1, color2):     return np.linalg.norm(np.array(color1) - np.array(color2))  def count_color_matches(dominant_colors, target_colors, threshold):     matches_count = {tuple(target): 0 for target in target_colors}     matched_colors = {tuple(target): [] for target in target_colors}          for color in dominant_colors:         for target in target_colors:             difference = color_difference(color, target)                          if difference &lt; threshold:                 matches_count[tuple(target)] += 1                 matched_colors[tuple(target)].append(color)                      return matches_count, matched_colors  def get_dominant_colors(image, k=5):     image = image.reshape((image.shape[0] * image.shape[1], 3))     image = image[np.any(image != [0, 0, 0], axis=1)]     kmeans = KMeans(n_clusters=k, n_init='auto')     kmeans.fit(image)     dominant_colors = kmeans.cluster_centers_     return dominant_colors  def extract_target_colors(target_image_path,inference_server_address, project_id, version_number):     target_image = cv2.imread(target_image_path)     with open(target_image_path, \"rb\") as f:         im_bytes = f.read()             im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")      headers = {         'Content-Type': 'application/json',         'Accept': 'application/json'     }      params = {         'api_key': 'FFgkmScNUBERP9t3PJvV',     }      response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)     data = response.json()      for predictions in data['predictions']:         Pred_points = predictions['points']         target_image = cv2.imread(target_image_path)         polygon_points = parse_polygon_annotation(Pred_points, target_image.shape)         polygon_image = extract_polygon_area(target_image_path, polygon_points)         target_dominant_colors = get_dominant_colors(polygon_image)          return target_dominant_colors  def match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold):     global prediction_counter, image_counter     total_matches = 0     matched_filepaths = []      extention_images = \".jpg\"     get_images = sorted(glob.glob(images_folder + '*' + extention_images))      for images in get_images:         t0 = time.time()         print(\"File path: \" + images)         img = cv2.imread(images)         with open(images, \"rb\") as f:             im_bytes = f.read()                 im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")         headers = {             'Content-Type': 'application/json',             'Accept': 'application/json'         }          params = {             'api_key': '',         }                  response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)         data = response.json()          for predictions in data['predictions']:             prediction_counter += 1             image_counter += 1             Pred_points = predictions['points']             image = cv2.imread(images)             polygon_points = parse_polygon_annotation(Pred_points, image.shape)             polygon_image = extract_polygon_area(images, polygon_points)             dominant_colors = get_dominant_colors(polygon_image)             matches, matched_colors_list = count_color_matches(dominant_colors, target_dominant_colors, color_threshold)                  all_matched = all(value &gt; 0 for value in matches.values())                  if all_matched:             matched_filepaths.append(images)             total_matches += 1      print(f\"\\nTotal images where all target colors matched: {total_matches}\")     print(f\"\\nMatched images where all target colors matched: {matched_filepaths}\") In\u00a0[\u00a0]: Copied! <pre>def main():\n    target_image_path = \"TARGET_IMAGE_PATH\"\n    inference_server_address = \"http://detect.roboflow.com/\"\n    version_number = 1\n    project_id = \"PROJECT_ID\"\n    images_folder = \"IMAGE_FOLDER_PATH\"\n    # grab all the .jpg files\n    extention_images = \".jpg\"\n    get_images = sorted(glob.glob(images_folder + '*' + extention_images))\n    MAX_COLOR_DIFFERENCE = 3 * 256 # DO NOT EDIT\n    TARGET_COLOR_PERCENT_THRESHOLD= 0.08 # Value must be between 0 - 1 - DO EDIT\n    color_threshold = int(MAX_COLOR_DIFFERENCE * TARGET_COLOR_PERCENT_THRESHOLD)\n\n\n    target_dominant_colors = extract_target_colors(target_image_path,inference_server_address, project_id, version_number)\n    match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold)\n\nif __name__ == \"__main__\":\n    main()\n</pre> def main():     target_image_path = \"TARGET_IMAGE_PATH\"     inference_server_address = \"http://detect.roboflow.com/\"     version_number = 1     project_id = \"PROJECT_ID\"     images_folder = \"IMAGE_FOLDER_PATH\"     # grab all the .jpg files     extention_images = \".jpg\"     get_images = sorted(glob.glob(images_folder + '*' + extention_images))     MAX_COLOR_DIFFERENCE = 3 * 256 # DO NOT EDIT     TARGET_COLOR_PERCENT_THRESHOLD= 0.08 # Value must be between 0 - 1 - DO EDIT     color_threshold = int(MAX_COLOR_DIFFERENCE * TARGET_COLOR_PERCENT_THRESHOLD)       target_dominant_colors = extract_target_colors(target_image_path,inference_server_address, project_id, version_number)     match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold)  if __name__ == \"__main__\":     main()"},{"location":"notebooks/rgb_anomaly_detection/#rgb-anomaly-detection","title":"RGB Anomaly Detection\u00b6","text":"<p>In this cookbook, we identify color / RGB anomalies for segmented items. Capture a base image to extract your ground truth RGB with Roboflow and compare to neew data collected. In this scenario, we are assessing variations in logo color.</p> <p>Click the Open in Colab button to run the cookbook on Google Colab.</p> <p>Let's begin!</p>"},{"location":"notebooks/rgb_anomaly_detection/#install-required-packages","title":"Install required packages\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#extract-target-rgb-color-from-polygon-and-run-kmeans","title":"Extract target RGB color from polygon and run Kmeans\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#run-main-function-to-compare-base-color-logo-with-target-colors-and-run-anomaly-detection","title":"Run main function to compare base color logo with target colors and run anomaly detection\u00b6","text":""},{"location":"quickstart/aliases/","title":"Aliases","text":"<p>Inference supports running any of the 50,000+ pre-trained public models hosted on Roboflow Universe, as well as fine-tuned models.</p> <p>We have defined IDs for common models for ease of use. These models do not require an API key for use unlike other public or private models.</p> <p>Tip</p> <p>See the Use a fine-tuned model guide for an example on how to deploy a model.</p> <p>You can click the link associated with a model below to test the model in your browser, and use the ID with Inference to deploy the model to the edge.</p>"},{"location":"quickstart/aliases/#supported-pre-trained-models","title":"Supported Pre-Trained Models","text":"Model Size Task Model ID Test Model in Browser YOLOv8n 640 Object Detection yolov8n-640 Test in Browser YOLOv8n 1280 Object Detection yolov8n-1280 Test in Browser YOLOv8s 640 Object Detection yolov8s-640 Test in Browser YOLOv8s 1280 Object Detection yolov8s-1280 Test in Browser YOLOv8m 640 640 Object Detection yolov8m-640 Test in Browser YOLOv8m 1280 Object Detection yolov8m-1280 Test in Browser YOLOv8l 640 Object Detection yolov8l-640 Test in Browser YOLOv8l 1280 Object Detection yolov8l-1280 Test in Browser YOLOv8x 640 Object Detection yolov8x-640 Test in Browser YOLOv8x 1280 Object Detection yolov8x-1280 Test in Browser YOLO-NAS (small) 640 Object Detection yolo-nas-s-640 Test in Browser YOLO-NAS (medium) 640 Object Detection yolo-nas-m-640 Test in Browser YOLO-NAS (large) 640 Object Detection yolo-nas-l-640 Test in Browser YOLOv8n Instance Segmentation 640 Instance Segmentation yolov8n-seg-640 Test in Browser YOLOv8n Instance Segmentation 1280 Instance Segmentation yolov8n-seg-1280 Test in Browser YOLOv8s Instance Segmentation 640 Instance Segmentation yolov8s-seg-640 Test in Browser YOLOv8m Instance Segmentation 1280 Instance Segmentation yolov8s-seg-1280 Test in Browser YOLOv8m Instance Segmentation 640 Instance Segmentation yolov8m-seg-640 Test in Browser YOLOv8m Instance Segmentation 1280 Instance Segmentation yolov8m-seg-1280 Test in Browser YOLOv8l Instance Segmentation 640 Instance Segmentation yolov8l-seg-640 Test in Browser YOLOv8l Instance Segmentation 1280 Instance Segmentation yolov8l-seg-1280 Test in Browser YOLOv8x Instance Segmentation 640 Instance Segmentation yolov8x-seg-640 Test in Browser YOLOv8x Instance Segmentation 640 Instance Segmentation yolov8x-seg-1280 Test in Browser"},{"location":"quickstart/compatability_matrix/","title":"Model Compatability","text":"<p>The table below shows on what devices you can deploy models supported by Inference.</p> <p>See our Docker Getting Started guide for more information on how to deploy Inference on your device.</p> <p>Table key:</p> <ul> <li>\u2705 Fully supported</li> <li>\ud83d\udeab Not supported</li> <li>\ud83d\udea7 On roadmap, not currently supported</li> </ul> Model CPU GPU Jetson 4.5.x Jetson 4.6.x Jetson 5.x Roboflow Hosted Inference YOLOv8 Object Detection \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Classification \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Segmentation \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv5 Object Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Segmentation \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CLIP \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 DocTR \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Gaze \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \u2705 SAM \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab ViT Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLACT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"quickstart/configure_api_key/","title":"Roboflow API Key","text":"<p>Throughout these docs you will see references to your Roboflow API key. Using your Roboflow API key grants you access to the models you have trained on Roboflow, public models available on Roboflow Universe, and access to hosted inference API's.</p>"},{"location":"quickstart/configure_api_key/#access-your-roboflow-api-key","title":"Access Your Roboflow API Key","text":"<p>For some examples in the documentation you will need to provide your Roboflow API key. To access your Roboflow API key, you will need to create a free Roboflow account, then follow the docs to retrieve your key.</p>"},{"location":"quickstart/configure_api_key/#use-your-roboflow-api-key","title":"Use Your Roboflow API Key","text":"<p>There are several ways to configure your Roboflow API key when using Inference.</p>"},{"location":"quickstart/configure_api_key/#environment-variable","title":"Environment Variable","text":"<p>The recommended way is to set your Roboflow API key within your environment via the variable <code>ROBOFLOW_API_KEY</code>. In most terminals you can run:</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre> <p>Then, any command you run within that same terminal session will have access to the environment variable <code>ROBOFLOW_API_KEY</code>.</p>"},{"location":"quickstart/configure_api_key/#python","title":"Python","text":"<p>When using Inference within python, your Roboflow API key can be set via keyword arguments</p> <pre><code>from inference.models.utils import get_model\n\nmodel = get_model(model_id=\"...\", api_key=\"YOUR ROBOFLOW API KEY\")\n</code></pre> <p>Hint</p> <p>If you set your API key in your environment, you do not have to pass it as a keyword argument: <code>model = get_model(model_id=\"...\")</code></p>"},{"location":"quickstart/configure_api_key/#http-request-payload","title":"HTTP Request Payload","text":"<p>When using HTTP requests, your Roboflow API key should be passed as a url parameter, or as part of the request payload, depending on the route you are using.</p> <pre><code>import requests\n\nmy_api_key = \"YOUR ROBOFLOW API KEY\"\n\nurl = f\"http://localhost:9001/soccer-players-5fuqs/1?api_key={my_api_key}\"\nresponse = requests.post(url,...)\n\nurl = \"http://localhost:9001/infer/object_detection\"\npayload = {\n  \"api_key\": my_api_key,\n  \"model_id\": \"soccer-players-5fuqs/1\",\n  ...\n}\nresponse = requests.post(url,json=payload)\n</code></pre>"},{"location":"quickstart/configure_api_key/#docker-configuration","title":"Docker Configuration","text":"<p>If you are running the Roboflow Inference Server locally in a docker container, you can provide your Roboflow API key within the <code>docker run</code> command.</p> <pre><code>docker run -it --rm --network=host -e ROBOFLOW_API_KEY=YOUR_ROBOFLOW_API_KEY roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <p>Requests sent to this server can now omit <code>api_key</code> from the request payload.</p>"},{"location":"quickstart/devices/","title":"What Devices Can I Use?","text":"<p>You can deploy Inference on the edge, in your own cloud, or using the Roboflow hosted inference option.</p>"},{"location":"quickstart/devices/#supported-edge-devices","title":"Supported Edge Devices","text":"<p>You can set up a server to use computer vision models with Inference on the following devices:</p> <ul> <li>ARM CPU (macOS, Raspberry Pi)</li> <li>x86 CPU (macOS, Linux, Windows)</li> <li>NVIDIA GPU</li> <li>NVIDIA Jetson (JetPack 4.5.x, JetPack 4.6.x, JetPack 5.x)</li> </ul>"},{"location":"quickstart/devices/#model-compatability","title":"Model Compatability","text":"<p>The table below shows on what devices you can deploy models supported by Inference.</p> <p>See our Docker Getting Started guide for more information on how to deploy Inference on your device.</p> <p>Table key:</p> <ul> <li>\u2705 Fully supported</li> <li>\ud83d\udeab Not supported</li> <li>\ud83d\udea7 On roadmap, not currently supported</li> </ul> Model CPU GPU Jetson 4.5.x Jetson 4.6.x Jetson 5.x Roboflow Hosted Inference YOLOv8 Object Detection \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Classification \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Segmentation \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv5 Object Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Segmentation \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CLIP \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 DocTR \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Gaze \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \u2705 SAM \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab ViT Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLACT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"quickstart/devices/#cloud-platform-support","title":"Cloud Platform Support","text":"<p>You can deploy Inference on any cloud platform such as AWS, GCP, or Azure.</p> <p>The installation and setup instructions are the same as for any edge device, once you have installed the relevant drivers on your cloud platform. We recommend deploying with an official \"Deep Learning\" image from your cloud provider if you are running inference on a GPU device. \"Deep Learning\" images should have the relevant drivers pre-installed so you can set up Inference without configuring GPU drivers manually</p>"},{"location":"quickstart/devices/#use-hosted-inference-from-roboflow","title":"Use Hosted Inference from Roboflow","text":"<p>You can also run your models in the cloud with the Roboflow hosted inference offering. The Roboflow hosted inference solution enables you to deploy your models in the cloud without having to manage your own infrastructure. Roboflow's hosted solution does not support all features available in Inference that you can run on your own infrastructure.</p> <p>To learn more about device compatability with different models, refer to the model compatability matrix.</p>"},{"location":"quickstart/docker/","title":"Running With Docker","text":""},{"location":"quickstart/docker/#setup","title":"Setup","text":"<p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If you haven't installed Docker yet, you can get it from Docker's official website.</p>"},{"location":"quickstart/docker/#set-up-a-docker-inference-server-via-inference-server-start","title":"Set up a Docker Inference Server via `inference server start``","text":"<p>Another easy way to run the Roboflow Inference Server with Docker is via the command line.</p> <p>First, Install the CLI.</p> <p>Running the Inference Server is as simple as running the following command:</p> <pre><code>inference server start\n</code></pre> <p>This will pull the appropriate Docker image for your machine and start the Inference Server on port 9001. You can then send requests to the server to get predictions from your model, as described in HTTP Inference.</p> <p>Once you have your inference server running, you can check its status with the following command:</p> <pre><code>inference server status\n</code></pre> <p>Roboflow Inference CLI currently supports the following device targets:</p> <ul> <li>x86 CPU</li> <li>ARM64 CPU</li> <li>NVIDIA GPU</li> </ul> <p>For Jetson or TensorRT Runtime inference server images, pull the images directly following the instructions below.</p>"},{"location":"quickstart/docker/#manually-set-up-a-docker-container","title":"Manually Set Up a Docker Container","text":""},{"location":"quickstart/docker/#step-1-pull-from-docker-hub","title":"Step #1: Pull from Docker Hub","text":"<p>If you don't wish to build the Docker image locally or prefer to use the official releases, you can directly pull the pre-built images from the Docker Hub. These images are maintained by the Roboflow team and are optimized for various hardware configurations.</p> <p>docker pull</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <p>Official Roboflow Inference Server Docker Image for x86 CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for ARM CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-gpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.5.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-4.5.0\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.6.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-4.6.1\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 5.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-5.1.1\n</code></pre>"},{"location":"quickstart/docker/#step-2-run-the-docker-container","title":"Step #2: Run the Docker Container","text":"<p>Once you have a Docker image (either built locally or pulled from Docker Hub), you can run the Roboflow Inference Server in a container.</p> <p>docker run</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <pre><code>docker run -it --net=host \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -p 9001:9001 \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -it --network=host --gpus=all \\\nroboflow/roboflow-inference-server-gpu:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-4.5.0:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-4.6.1:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre> <p>Note: The Jetson images come with TensorRT dependencies. To use TensorRT acceleration with your model, pass an additional environment variable at runtime <code>-e ONNXRUNTIME_EXECUTION_PROVIDERS=TensorrtExecutionProvider</code>. This can improve inference speed, however, this also incurs a costly startup expense when the model is loaded. Note: On Windows and macOS, you may need to use <code>-p 9001:9001</code> instead of <code>--net=host</code> to expose the port to the host machine.</p> <p>You may add the flag <code>-e ROBOFLOW_API_KEY=&lt;YOUR API KEY&gt;</code> to your <code>docker run</code> command so that you do not need to provide a Roboflow API key in your requests. Substitute <code>&lt;YOUR API KEY&gt;</code> with your Roboflow API key. Learn how to retrieve your Roboflow API key here.</p> <p>You may add the flag <code>-v $(pwd)/cache:/tmp/cache</code> to create a cache folder on your home device so that you do not need to redownload or recompile model artifacts upon inference container reboot. You can also (preferably) store artificats in a docker volume named <code>inference-cache</code> by adding the flag <code>-v inference-cache:/tmp/cache</code>.</p>"},{"location":"quickstart/docker/#advanced-build-a-docker-container-from-scratch","title":"Advanced: Build a Docker Container from Scratch","text":"<p>To build a Docker image locally, first clone the Inference Server repository.</p> <pre><code>git clone https://github.com/roboflow/inference\n</code></pre> <p>Choose a Dockerfile from the following options, depending on the hardware you want to run Inference Server on.</p> <p>docker build</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.gpu \\\n-t roboflow/roboflow-inference-server-gpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-jetson-4.5.0 .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-jetson-4.6.1 .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson.5.1.1 \\\n-t roboflow/roboflow-inference-server-jetson-5.1.1 .\n</code></pre>"},{"location":"quickstart/docker_configuration_options/","title":"Docker Configuration Options","text":"<p>Inference servers have a number of configurable parameters which can be set using environment variables. To set an environment variable with the docker run command, use the -e flag with an argument, like this:</p> <pre><code>docker run -it --rm -e ENV_VAR_NAME=env_var_value -p 9001:9001 --gpus all roboflow/roboflow-inference-server-gpu:latest\n</code></pre>"},{"location":"quickstart/docker_configuration_options/#networking","title":"Networking","text":""},{"location":"quickstart/docker_configuration_options/#host","title":"Host","text":"<p>HOST: String (default = 0.0.0.0)</p> <p>Sets the host address used by HTTP interfaces.</p>"},{"location":"quickstart/docker_configuration_options/#inference-server-port","title":"Inference Server Port","text":"<p>PORT: Integer (default = 9001)</p> <p>Sets the port used by HTTP interfaces.</p>"},{"location":"quickstart/docker_configuration_options/#class-agnostic-nms","title":"Class Agnostic NMS","text":"<p>Variable: CLASS_AGNOSTIC_NMS</p> <p>Type: Boolean (default = False)</p> <p>Sets the default non-maximal suppression (NMS) behavior for detection type models (object detection, instance segmentation, etc.).  If True, the default NMS behavior will be class be class agnostic,  meaning overlapping detections from different classes may be removed based on the IoU threshold. If False, only overlapping detections from the same class will be considered for removal by NMS.</p>"},{"location":"quickstart/docker_configuration_options/#allow-origins","title":"Allow Origins","text":"<p>Variable: ALLOW_ORIGINS</p> <p>Type: String (default = \"\")</p> <p>Sets the allow_origins property on the CORSMiddleware used with FastAPI for HTTP interfaces. Multiple values can be provided separated by a comma (ex. ALLOW_ORIGINS=orig1.com,orig2.com).</p>"},{"location":"quickstart/docker_configuration_options/#clip-model-options","title":"CLIP Model Options","text":""},{"location":"quickstart/docker_configuration_options/#clip-version","title":"CLIP Version","text":"<p>Variable: CLIP_VERSION_ID</p> <p>Type: String (default = ViT-B-16)</p> <p>Sets the OpenAI CLIP version for use by all /clip routes. Available model versions are: RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, BiT-B-32, BiT-L-14-336px, and ViT-L-14.</p>"},{"location":"quickstart/docker_configuration_options/#clip-batch-size","title":"CLIP Batch Size","text":"<p>Variable: CLIP_MAX_BATCH_SIZE</p> <p>Type: Integer (default = 8)</p> <p>Sets the max batch size accepted by the clip model inference functions.</p>"},{"location":"quickstart/docker_configuration_options/#batch-size","title":"Batch Size","text":"<p>FIX_BATCH_SIZE: Boolean (default = False)</p> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p>"},{"location":"quickstart/docker_configuration_options/#license-server","title":"License Server","text":"<p>LICENSE_SERVER: String (default = None)</p> <p>Sets the address of a Roboflow license server.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-active-models","title":"Maximum Active Models","text":"<p>MAX_ACTIVE_MODELS: Integer (default = 8)</p> <p>Sets the maximum number of models the internal model manager will store in memory at one time. By default, the model queue will remove the least recently accessed model when making space for a new model.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-candidates","title":"Maximum Candidates","text":"<p>MAX_CANDIDATES: Integer (default = 3000)</p> <p>The maximum number of candidates for detection.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-detections","title":"Maximum Detections","text":"<p>MAX_DETECTIONS: Integer (default = 300)</p> <p>Sets the maximum number of detections returned by a model.</p>"},{"location":"quickstart/docker_configuration_options/#model-cache-directory","title":"Model Cache Directory","text":"<p>MODEL_CACHE_DIR: String (default = /tmp/cache)</p> <p>Sets the container path for the root model cache directory.</p>"},{"location":"quickstart/docker_configuration_options/#number-of-workers","title":"Number of Workers","text":"<p>NUM_WORKERS: Integer (default = 1)</p> <p>Sets the number of workers used by HTTP interfaces. </p>"},{"location":"quickstart/docker_configuration_options/#tensorrt-cache-directory","title":"TensorRT Cache Directory","text":"<p>TENSORRT_CACHE_PATH: String (default = MODEL_CACHE_DIR)</p> <p>Sets the container path to the TensorRT cache directory. Setting this path in conjunction with mounting a host volume can reduce the cold start time of TensorRT based servers.</p>"},{"location":"quickstart/explore_models/","title":"From Roboflow","text":"<p>With Inference, you can run any of the 50,000+ models available on Roboflow Universe. You can also run private, fine-tuned models that you have trained or uploaded to Roboflow.</p> <p>All models run on your own hardware.</p>"},{"location":"quickstart/explore_models/#run-a-model-from-roboflow-universe","title":"Run a Model From Roboflow Universe","text":"<p>In the first example, we showed how to run a people detection model. This model was hosted on Universe. Let's find another model to try.</p> <p>Go to the Roboflow Universe homepage and use the search bar to find a model.</p> <p></p> <p>Info</p> <p>Add \"model\" to your search query to only find models.</p> <p>Browse the search page to find a model.</p> <p></p> <p>When you have found a model, click on the model card to learn more. Click the \"Model\" link in the sidebar to get the information you need to use the model.</p> <p>Then, install Inference and supervision, which we will use to run our model and handle model predictions, respectively:</p> <pre><code>pip install inference supervision\n</code></pre> <p>Next, create a new Python file and add the following code:</p> <pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n# import supervision to visualize our results\nimport supervision as sv\n# import cv2 to helo load our image\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"people-walking.jpg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results[0].dict(by_alias=True, exclude_none=True))\n\n# create supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>Tip</p> <p>To see more models, check out the Pre-Trained Models page and Roboflow Universe.</p> <p>The <code>people-walking.jpg</code> file is hosted here.</p> <p>Replace <code>yolov8n-640</code> with the model ID you found on Universe, replace <code>image</code> with the image of your choosing, and be sure to export your API key:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>You should see your model's predictions visualized on your screen.</p> <p></p>"},{"location":"quickstart/explore_models/#run-a-private-fine-tuned-model","title":"Run a Private, Fine-Tuned Model","text":"<p>You can run models you have trained privately on Roboflow with Inference. To do so, first go to your Roboflow dashboard. Then, choose the model you want to run.</p> <p></p> <p>Click the \"Deploy\" link in the sidebar to find the information you will need to use your model with Inference.</p> <p>Copy the model ID on the page (in this case, <code>taylor-swift-records/3</code>).</p> <p></p> <p>Then, create a new Python file and add the following code:</p> <pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n# import supervision to visualize our results\nimport supervision as sv\n# import cv2 to helo load our image\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"taylor-swift-album-1989.jpeg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"taylor-swift-records/3\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results[0].dict(by_alias=True, exclude_none=True))\n\n# create supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>The <code>taylor-swift-album-1989.jpeg</code> file is hosted here.</p> <p>Replace <code>taylor-swift-records/3</code> with the model ID from your private model and ensure your API key is in your environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>You should see your model's predictions visualized on your screen.</p> <p></p>"},{"location":"quickstart/http_inference/","title":"HTTP Inference","text":"<p>The Roboflow Inference Server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we show how to run inference on object detection, classification, and segmentation models using the Inference Server.</p> <p>Currently, the server is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>To run inference with the server, we will:</p> <ol> <li>Install the server</li> <li>Download a model for use on the server</li> <li>Run inference</li> </ol>"},{"location":"quickstart/http_inference/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Start the server using <code>inference server start</code>. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"quickstart/http_inference/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions.</p> <p>There are two generations of routes in a Roboflow inference server To see what routes are available for a running inference server instance, visit the <code>/docs</code> route in a browser. Roboflow hosted inference endpoints (<code>detect.roboflow.com</code>) only support V1 routes.</p>"},{"location":"quickstart/http_inference/#run-inference-on-a-v2-route","title":"Run Inference on a v2 Route","text":"<p>Run</p> URLBase64 ImageBatch InferenceNumpy Array <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\"\nconfidence = 0.75\niou_thresh = 0.5\napi_key = \"YOUR API KEY\"\ntask = \"object_detection\"\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"url\",\n        \"value\": image_url,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>image_url</code>: The URL of the image you want to run inference on.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": img_str,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Roboflow object detection models support batching. Utilize batch inference by passing a list of image objects in a request payload:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        }\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\nimg_str = base64.b64encode(numpy_data)\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": img_str,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre>"},{"location":"quickstart/http_inference/#run-inference-on-a-v1-route","title":"Run Inference on a v1 Route","text":"<p>Run</p> URL <pre><code>The Roboflow hosted API uses the V1 route and requests take a slightly different form:\n\n```python\nimport requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\n\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}&amp;image={image_url}\",\n)\n\npredictions = res.json()\nprint(predictions)\n```\n\nAbove, specify:\n\n1. `project_id`, `model_version`: Your project ID and model version number. &lt;a href=\"https://docs.roboflow.com/api-reference/workspace-and-project-ids\" target=\"_blank\"&gt;Learn how to retrieve your project ID and model version number&lt;/a&gt;.\n2. `confidence`: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.\n3. `api_key`: Your Roboflow API key. &lt;a href=\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key\" target=\"_blank\"&gt;Learn how to retrieve your Roboflow API key&lt;/a&gt;.\n4. `task`: The type of task you want to run. Choose from `object_detection`, `classification`, or `segmentation`.\n5. `filename`: The path to the image you want to run inference on.\n\nThen, run the Python script:\n\n```\npython app.py\n```\n</code></pre> Base64 ImageNumPy ArrayBatch Inference <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}\",\n    data=img_str,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Numpy arrays can be pickled and passed to the inference server for quicker processing. Note, Roboflow hosted APIs to not accept numpy inputs for security reasons:</p> <pre><code>import requests\nimport cv2\nimport pickle\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\napi_key = \"YOUR API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\n\nres = requests.post(\n    f\"http://localhost:9001/{project_id}/{model_version}?api_key={api_key}&amp;image_type=numpy\",\n    data=numpy_data,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Batch inference is not currently supported by V1 routes.</p> <p>The code snippets above will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> or <code>localhost:9001/redoc</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/http_inference/#batching-requests","title":"Batching Requests","text":"<p>Object detection models trained with Roboflow support batching, which allow you to upload multiple images of any type at once:</p> <pre><code>infer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"url\",\n            \"value\": image_url_1,\n        },\n        {\n            \"type\": \"url\",\n            \"value\": image_url_2,\n        },\n        {\n            \"type\": \"url\",\n            \"value\": image_url_3,\n        },\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n</code></pre>"},{"location":"quickstart/inference_101/","title":"How Do I Run Inference?","text":"<p>There are three ways to run Inference:</p> <ul> <li>Using the Python SDK (Images and Videos)</li> <li>Using the Python HTTP SDK (Images)</li> <li>Using the HTTP API (Images for all languages)</li> </ul> <p>We document every method in the \"Inputs\" section of the Inference documentation.</p> <p>Below, we talk about when you would want to use each method.</p>"},{"location":"quickstart/inference_101/#using-the-python-sdk-images-and-videos","title":"Using the Python SDK (Images and Videos)","text":"<p>You can use the Python SDK to run models on images and videos directly using the Inference code, without using Docker.</p> <p>Any code example that imports from <code>inference.models</code> uses the model directly.</p> <p>To use the Python SDK, you need to install:</p> <pre><code>pip install inference\n</code></pre>"},{"location":"quickstart/inference_101/#python-http-sdk","title":"Python HTTP SDK","text":"<p>You can use the Python HTTP SDK to run models using Inference with Docker.</p> <p>Any code example that imports from <code>inference_sdk</code> uses the HTTP API.</p> <p>To use this method, you will need an Inference server running, or you can use the Roboflow endpoint for your model.</p>"},{"location":"quickstart/inference_101/#self-hosted-inference-server","title":"Self-Hosted Inference Server","text":"<p>You can set up and install and Inference server using:</p> <pre><code>pip install inference\ninference server start\n</code></pre>"},{"location":"quickstart/inference_101/#roboflow-hosted-api","title":"Roboflow Hosted API","text":"<p>First, run:</p> <pre><code>pip install inference inference-sdk\n</code></pre> <p>Then, use your Roboflow hosted API endpoint to access your model. You can find this in the Deploy tab of your Roboflow model.</p>"},{"location":"quickstart/inference_101/#http-sdk","title":"HTTP SDK","text":"<p>You can deploy your model with Inference and Docker and use the API in any programming language (i.e. Swift, Node.js, and more).</p> <p>To use this method, you will need an Inference server running. You can set up and install and Inference server using:</p>"},{"location":"quickstart/inference_101/#self-hosted-inference-server_1","title":"Self-Hosted Inference Server","text":"<p>You can set up and install and Inference server using:</p> <pre><code>pip install inference\ninference server start\n</code></pre>"},{"location":"quickstart/inference_101/#roboflow-hosted-api_1","title":"Roboflow Hosted API","text":"<p>Use your Roboflow hosted API endpoint to access your model. You can find this in the Deploy tab of your Roboflow model.</p>"},{"location":"quickstart/inference_101/#benefits-of-using-inference-over-http","title":"Benefits of Using Inference Over HTTP","text":"<p>You can run Inference directly from your codebase or using a HTTP microservice deployed with Docker.</p> <p>Running Inference this way can have several advantages:</p> <ul> <li>No Dependency Management: When running Inference within one of Roboflow's published Inference Server containers, all the dependencies are built and isolated so they wont interfere with other dependencies in your code.</li> <li>Microservice Architecture: Running Inference as a separate service allows you to operate and maintain your computer vision models separate from other logic within your software, including scaling up and down to meet dynamic loads.</li> <li>Roboflow Hosted API: Roboflow hosts a powerful and infinitely scalable version of the Roboflow Inference Server. This makes it even easier to integrate computer vision models into your software without adding any maintenance burden. And, since the Roboflow hosted APIs are running using the Inference package, it's easy to switch between using a hosted server and an on prem server without having to reinvent your client code.</li> <li>Non-Python Clients: Running Inference within an HTTP server allows you to interact with it from any language you prefer.</li> </ul>"},{"location":"quickstart/inference_101/#advanced-usage-interfaces","title":"Advanced Usage &amp; Interfaces","text":"<p>There are several advanced interfaces that enhance the capabilities of the base Inference package.</p> <ul> <li>Active Learning: Active learning helps improve your model over time by contributing real world data back to your Roboflow dataset in real time. Docs and Examples</li> <li>Parallel HTTP API: A highly parallel server capable of accepting requests from many different clients and batching them dynamically in real time to make the most efficient use of the host hardware. Docs and Examples</li> <li>Stream Manager API: An API for starting, stopping, and managing Inference Pipeline instances. This interfaces combines the advantages of running Inference realtime on a stream while also fitting nicely into a microservice architecture. Docs and Examples</li> </ul> <p>To learn more, contact the Roboflow sales team.</p>"},{"location":"quickstart/inference_notebook/","title":"Inference notebook","text":"<p>Roboflow Inference Servers come equipped with a built in Jupyterlab environment. This environment is the fastest way to get up and running with inference for development and testing. To use it, first start an inference server.</p> <p>The easiest way to start an inference server is with the inference CLI. Install it via pip:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Now run the <code>inference sever start</code> command. Be sure to specify the <code>--dev</code> flag so that the notebook environment is enabled (it is disabled by default).</p> <pre><code>inference server start --dev\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. This page contains links to resources and examples related to <code>inference</code>. It also contains a link to the built in Jupyterlab environment.</p> <p>From the landing page, select the button labeled \"Jump Into an Inference Enabled Notebook\" to open a new tab for the Jupyterlab environment. </p> <p>This Jupyterlab environment comes preloaded with several example notebooks and all of the dependencies needed to run <code>inference</code>.</p>"},{"location":"quickstart/licensing/","title":"Model Licenses","text":""},{"location":"quickstart/licensing/#inference-source-code-license","title":"Inference Source Code License","text":"<p>The Roboflow Inference code is distributed under an Apache 2.0 license.</p>"},{"location":"quickstart/licensing/#using-models-hosted-on-roboflow","title":"Using Models Hosted on Roboflow","text":"<p>To use a model hosted on Roboflow for commercial purposes, you need a Roboflow Enterprise license.</p> <p>Contact the Roboflow sales team to inquire about an enterprise license.</p>"},{"location":"quickstart/licensing/#model-code-licenses","title":"Model Code Licenses","text":"<p>The models supported by Roboflow Inference have their own licenses. View the licenses for supported models below.</p> model license <code>inference/models/clip</code> MIT <code>inference/models/gaze</code> MIT, Apache 2.0 <code>inference/models/sam</code> Apache 2.0 <code>inference/models/vit</code> Apache 2.0 <code>inference/models/yolact</code> MIT <code>inference/models/yolov5</code> AGPL-3.0 <code>inference/models/yolov7</code> GPL-3.0 <code>inference/models/yolov8</code> AGPL-3.0"},{"location":"quickstart/run_a_model/","title":"Run a model","text":"<p>Let's run a computer vision model with Inference. The quickest way to get started with Inference is to simply load a model, and then call the model's <code>infer(...)</code> method.</p>"},{"location":"quickstart/run_a_model/#install-inference","title":"Install Inference","text":"<p>First, we need to install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>To help us visualize our results in the example below, we will install Supervision:</p> <pre><code>pip install supervision\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p>"},{"location":"quickstart/run_a_model/#load-a-model-and-run-inference","title":"Load a Model and Run Inference","text":"<pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n\n# define the image url to use for inference\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n</code></pre> <p>In the code above, we loaded a model and then we used that model's <code>infer(...)</code> method to run an image through our computer vision model.</p> <p>Tip</p> <p>When you run inference on an image, the same augmentations you applied when you generated a version in Roboflow will be applied at inference time. This helps improve model performance.</p>"},{"location":"quickstart/run_a_model/#visualize-results","title":"Visualize Results","text":"<p>Running inference is fun but it's not much to look at. Let's add some code to visualize our results.</p> <pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n# import supervision to visualize our results\nimport supervision as sv\n# import cv2 to helo load our image\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"people-walking.jpg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results[0].dict(by_alias=True, exclude_none=True))\n\n# create supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>The <code>people-walking.jpg</code> file is hosted here.</p> <p></p>"},{"location":"quickstart/run_a_model/#summary","title":"Summary","text":"<p>Huzzah! We used Inference to load a computer vision model, run inference on an image, then visualize the results! But this is just the start. There are many different ways to use Inference and how you use it is likely to depend on your specific use case and deployment environment. Learn more about how to use inference here.</p>"},{"location":"quickstart/run_model_on_image/","title":"Predict on an Image Over HTTP","text":"<p>A Roboflow Inference server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we show how to run inference on object detection, classification, and segmentation models using Inference.</p> <p>Note</p> <p>Inference is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>You can run inference on images from:</p> <ol> <li>URLs, which will be downloaded from the internet</li> <li>File names, which will be read from disk</li> <li>PIL images</li> <li>NumPy arrays</li> </ol>"},{"location":"quickstart/run_model_on_image/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Start the server using <code>inference server start</code>. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"quickstart/run_model_on_image/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions.</p> <p>There are two generations of routes in a Roboflow inference server To see what routes are available for a running inference server instance, visit the <code>/docs</code> route in a browser. Roboflow hosted inference endpoints (<code>detect.roboflow.com</code>) only support V1 routes.</p>"},{"location":"quickstart/run_model_on_image/#run-inference-on-a-v2-route","title":"Run Inference on a v2 Route","text":"<p>Run</p> URLPIL ImageNumPy ArrayBatch Inference <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference_sdk\nfrom inference_sdk import InferenceHTTPClient,\n# import os to get the API_KEY from the environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\n# create a client object\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# run inference on the image\nresults = client.infer(image_url, model_id=f\"{project_id}/{model_version}\")\n\n# print the results\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>image_url</code>: The URL of the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference sdk\nfrom inference_sdk import InferenceHTTPClient\n# import PIL for loading image\nfrom PIL import Image\n# import os for getting api key from environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nfilename = \"path/to/local/image.jpg\"\n\n# create a client object\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# load the image\npil_image = Image.open(filename)\n\n# run inference\nresults = client.infer(pil_image, model_id=f\"{project_id}/{model_version}\")\n\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference-sdk\nfrom inference_sdk import InferenceHTTPClient\n# import opencv for image loading\nimport cv2\n# import os to read api key from environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nfilename = \"path/to/local/image.jpg\"\n\n# create client\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# load image with opencv\nnumpy_image = cv2.imread(filename)\n\n# run inference\nresults = client.infer(numpy_image, model_id=f\"{project_id}/{model_version}\")\n\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Roboflow object detection models support batching. Utilize batch inference by passing a list of image objects in a request payload:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        }\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>See more docs on the inference-sdk</p>"},{"location":"quickstart/run_model_on_image/#run-inference-on-a-v1-route","title":"Run Inference on a v1 Route","text":"<p>Run</p> URLBase64 ImageNumPy ArrayBatch Inference <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\"\n\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}&amp;image={image_url}\",\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}\",\n    data=img_str,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Numpy arrays can be pickled and passed to the inference server for quicker processing. Note, Roboflow hosted APIs to not accept numpy inputs for security reasons:</p> <pre><code>import requests\nimport cv2\nimport pickle\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\napi_key = \"YOUR API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\n\nres = requests.post(\n    f\"http://localhost:9001/{project_id}/{model_version}?api_key={api_key}&amp;image_type=numpy\",\n    data=numpy_data,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Batch inference is not currently supported by V1 routes.</p> <p>The code snippets above will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> or <code>localhost:9001/redoc</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/","title":"Predict on a Video, Webcam or RTSP Stream","text":"<p>You can run computer vision models on webcam stream frames, RTSP stream frames, and video frames with Inference.</p> <p>Follow our Run a Fine-Tuned Model on Images guide to learn how to find a model to run.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#installation","title":"Installation","text":"<p>To use fine-tuned models with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Learn more about using Roboflow API keys in Inference</p> <p>Then, install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"quickstart/run_model_on_rtsp_webcam/#inference-on-video","title":"Inference on Video","text":"<p>Next, create an Inference Pipeline. Once you have selected a model to run, create a new Python file and add the following code:</p> <pre><code># Import the InferencePipeline object\nfrom inference import InferencePipeline\n# Import the built in render_boxes sink for visualizing results\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\n# initialize a pipeline object\npipeline = InferencePipeline.init(\n    model_id=\"rock-paper-scissors-sxsw/11\", # Roboflow model to use\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=render_boxes, # Function to run after each prediction\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>This code will run a model on frames from a webcam stream. To use RTSP, set the <code>video_reference</code> value to an RTSP stream URL. To use video, set the <code>video_reference</code> value to a video file path.</p> <p>Predictions are annotated using the <code>render_boxes</code> helper function. You can specify any function to process each prediction in the <code>on_prediction</code> parameter.</p> <p>Replace <code>rock-paper-scissors-sxsw/11</code> with the model ID associated with the model you want to run.</p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Your webcam will open and you can see the model running:</p> <p>Tip</p> <p>When you run inference on an image, the same augmentations you applied when you generated a version in Roboflow will be applied at inference time. This helps improve model performance.</p> <p>Presto! We used an InferencePipeline to run inference on our webcam and learned how we could modify it to run on other video sources (like video files or RTSP streams). See the Inference Pipeline docs to learn more about other configurable parameters and built in sinks.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#define-custom-prediction-logic","title":"Define Custom Prediction Logic","text":"<p>In Inference a sink is a function used to execute logic on the inference results within an <code>InferencePipeline</code>. Inference has some built in sinks for convenience. We used one above to plot bounding boxes.</p> <p>Below, we describe how to define custom prediction logic.</p> <p>The <code>on_prediction</code> parameter in the <code>InferencePipeline</code> constructor allows you to define custom prediction handlers. You can use this to define custom logic for how predictions are processed.</p> <p>This function provides two parameters:</p> <ul> <li><code>predictions</code>: A dictionary that contains all predictions returned by the model for the frame, and;</li> <li><code>video_frame</code>: A dataclass</li> </ul> <p>A VideoFrame object contains:</p> <ul> <li><code>image</code>: The video frame as a NumPy array,</li> <li><code>frame_id</code>: The frame ID, and;</li> <li><code>frame_timestamp</code>: The timestamp of the frame.</li> </ul> <p>Let's start by just printing the frame ID to the console.</p> <pre><code>from inference import InferencePipeline\n# import VideoFrame for type hinting\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\n# define sink function\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    # print the frame ID of the video_frame object\n    print(f\"Frame ID: {video_frame.frame_id}\")\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\",\n    on_prediction=my_custom_sink,\n)\n\npipeline.start()\npipeline.join()\n</code></pre> <p>The output should look something like:</p> <pre><code>Frame ID: 1\nFrame ID: 2\nFrame ID: 3\n</code></pre> <p>Now let's do something a little more useful and use our custom sink to visualize our predictions.</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\n# import opencv to display our annotated images\nimport cv2\n# import supervision to help visualize our predictions\nimport supervision as sv\n\n# create a simple box annotator to use in our custom sink\nannotator = sv.BoxAnnotator()\n\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    # get the text labels for each prediction\n    labels = [p[\"class\"] for p in predictions[\"predictions\"]]\n    # load our predictions into the Supervision Detections api\n    detections = sv.Detections.from_inference(predictions)\n    # annotate the frame using our supervision annotator, the video_frame, the predictions (as supervision Detections), and the prediction labels\n    image = annotator.annotate(\n        scene=video_frame.image.copy(), detections=detections, labels=labels\n    )\n    # display the annotated image\n    cv2.imshow(\"Predictions\", image)\n    cv2.waitKey(1)\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\",\n    on_prediction=my_custom_sink,\n)\n\npipeline.start()\npipeline.join()\n</code></pre> <p>You should see something like this on your screen:</p> <p>And there you have it! We created a custom sink that takes the outputs of our Inference Pipeline, annotates an image, and displays it to our screen. See the Inference Pipeline docs to learn more about other configurable parameters and built in sinks.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#existing-video-sinks","title":"Existing Video Sinks","text":""},{"location":"quickstart/run_model_on_rtsp_webcam/#built-in-sinks","title":"Built In Sinks","text":"<p>Inference has several sinks built in that are ready to use.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#render_boxes","title":"<code>render_boxes(...)</code>","text":"<p>The render boxes sink is made to visualize predictions and overlay them on a stream. It uses Supervision annotators to render the predictions and display the annotated frame. It only works for Roboflow models that yields detection-based output (<code>object-detection</code>, <code>instance-segmentation</code>, <code>keypoint-detection</code>), yet not all details of predictions may be  displayed by default (like detected key-points).</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#udpsink","title":"<code>UDPSink(...)</code>","text":"<p>The UDP sink is made to broadcast predictions with a UDP port. This port can be listened to by client code for further processing. It uses Python-default json serialisation - so predictions must be serializable, otherwise error will be thrown.  </p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#multi_sink","title":"<code>multi_sink(...)</code>","text":"<p>The Multi-Sink is a way to combine multiple sinks so that multiple actions can happen on a single inference result.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#videofilesink","title":"<code>VideoFileSink(...)</code>","text":"<p>The Video File Sink visualizes predictions, similar to the <code>render_boxes(...)</code> sink, however, instead of displaying the annotated frames, it saves them to a video file. All constraints related to <code>render_boxes(...)</code> apply.</p>"},{"location":"quickstart/run_model_over_udp/","title":"Predict Over UDP","text":"<p>You can run Inference directly on frames using UDP.</p> <p>This is ideal for real-time use cases where reducing latency is essential (i.e. sports broadcasting).</p> <p>This feature only works on devices with a CUDA-enabled GPU.</p> <p>Inference has been used at sports broadcasting events around the world for real-time object detection.</p> <p>Follow our Run a Fine-Tuned Model on Images guide to learn how to find a model to run.</p>"},{"location":"quickstart/run_model_over_udp/#run-a-vision-model-on-a-udp-stream","title":"Run a Vision Model on a UDP Stream","text":"<p>To run inference on frames from a UDP stream, you will need to:</p> <ol> <li>Set up a listening server to receive predictions from Inference, and;</li> <li>Run Inference, connected directly to your stream.</li> </ol>"},{"location":"quickstart/run_model_over_udp/#authenticate-with-roboflow","title":"Authenticate with Roboflow","text":"<p>To use Inference with a UDP stream, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre>"},{"location":"quickstart/run_model_over_udp/#configure-a-listening-server","title":"Configure a Listening Server","text":"<p>You need a server to receive predictions from Inference. This server is where you can write custom logic to process predictions.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import socket\nimport json\nimport time\n\nfps_array = []\n\n# Create a datagram (UDP) socket\nUDPClientSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n\n# Bind to the given IP address and port\nUDPClientSocket.bind((ip, port))\n\nprint(f\"UDP server up and listening on http://{ip}:{port}\")\n\n# Listen for incoming datagrams\nwhile True:\n    t0 = time.time()\n\n    bytesAddressPair = UDPClientSocket.recvfrom(1024)\n    message = bytesAddressPair[0]\n    address = bytesAddressPair[1]\n\n    clientMsg = json.loads(message)\n    clientIP = \"Client IP Address:{}\".format(address)\n\n    print(clientMsg)\n    print(clientIP)\n\n    t = time.time() - t0\n    fps_array.append(1 / t)\n    fps_array[-150:]\n    fps_average = sum(fps_array) / len(fps_array)\n    print(\"AVERAGE FPS: \" + str(fps_average))\n\nstart_udp_server(\"localhost\", arguments.port)\n</code></pre> <p>Above, replace <code>port</code> with the port on which you want to run your server.</p>"},{"location":"quickstart/run_model_over_udp/#run-a-broadcasting-server","title":"Run a Broadcasting Server","text":"<ul> <li>set up socket</li> <li>render will broadcast</li> <li>https://hub.docker.com/repository/docker/roboflow/roboflow-inference-server-udp-gpu/general</li> </ul>"},{"location":"scripts/gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages.\"\"\"\n</pre> \"\"\"Generate the code reference pages.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>src = Path(__file__).parent.parent.parent / \"inference\"\nSKIP_MODULES = [\n    \"inference.enterprise.device_manager.command_handler\",\n    \"inference.enterprise.parallel.celeryconfig\",\n]\n</pre> src = Path(__file__).parent.parent.parent / \"inference\" SKIP_MODULES = [     \"inference.enterprise.device_manager.command_handler\",     \"inference.enterprise.parallel.celeryconfig\", ] In\u00a0[\u00a0]: Copied! <pre>for path in sorted(p for p in src.rglob(\"*.py\") if \"landing\" not in p.parts):\n    module_path = path.relative_to(src.parent).with_suffix(\"\")\n    doc_path = path.relative_to(src.parent).with_suffix(\".md\")\n    full_doc_path = Path(\"docs\", \"reference\", doc_path)\n\n    parts = list(module_path.parts)\n    identifier = \".\".join(parts)\n    if parts[-1] == \"__main__\" or parts[-1] == \"__init__\" or identifier in SKIP_MODULES:\n        print(\"SKIPPING\", identifier)\n        continue\n\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        fd.write(f\"::: {identifier}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(p for p in src.rglob(\"*.py\") if \"landing\" not in p.parts):     module_path = path.relative_to(src.parent).with_suffix(\"\")     doc_path = path.relative_to(src.parent).with_suffix(\".md\")     full_doc_path = Path(\"docs\", \"reference\", doc_path)      parts = list(module_path.parts)     identifier = \".\".join(parts)     if parts[-1] == \"__main__\" or parts[-1] == \"__init__\" or identifier in SKIP_MODULES:         print(\"SKIPPING\", identifier)         continue      nav[parts] = doc_path.as_posix()      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         fd.write(f\"::: {identifier}\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"docs/reference/nav.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"docs/reference/nav.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"using_inference/http_api/","title":"Http api","text":"<p>The HTTP Inference API provides a standard API through which to run inference on computer vision models. The HTTP API is a helpful way to treat your machine learning models as their own microservice. With this interface, you will run a docker container and make requests over HTTP. The requests contain all of the information Inference needs to run a computer vision model including the model ID, the input image data, and any configurable parameters used during processing (e.g. confidence threshold).</p>"},{"location":"using_inference/http_api/#quickstart","title":"Quickstart","text":""},{"location":"using_inference/http_api/#install-the-inference-server","title":"Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Start the server using <code>inference server start</code>. After you have started the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"using_inference/http_api/#run-inference","title":"Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions. The easiest way to interact with the Roboflow Inference server is to sue the Inference SDK. To do this first install it with pip:</p> <pre><code>pip install inference-sdk\n</code></pre> <p>Next, instantiate a client and use the <code>infer(...)</code> method:</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = \"1\"\nmodel_id = project_id + \"/\" + model_version\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\n\nresults = client.infer(image_url, model_id=model_id)\n</code></pre> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>Hint</p> <p>See full docs for the Inference SDK.</p>"},{"location":"using_inference/http_api/#visualize-results","title":"Visualize Results","text":"<pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nmodel_id = \"soccer-players-5fuqs/1\"\nimage_file = \"soccer.jpg\"\n\nimage = cv2.imread(image_file)\n\n#Configure client\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\", # route for local inference server\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"], # api key for your workspace\n)\n\n#Run inference\nresult = client.infer(image, model_id=model_id)\n\n#Load results into Supervision Detection API\ndetections = sv.Detections.from_inference(result)\n\n#Create Supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n#Extract labels array from inference results\nlabels = [p['class'] for p in result['predictions']]\n\n#Apply results to image using Supervision annotators\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\n#Write annotated image to file or display image\nwith sv.ImageSink(target_dir_path=\"./results/\", overwrite=True) as sink:\n    sink.save_image(annotated_image)\n#or sv.plot_image(annotated_image)\n</code></pre>"},{"location":"using_inference/http_api/#hosted-api","title":"Hosted API","text":"<p>Roboflow hosts a powerful and infinitely scalable version of the Roboflow Inference Server. This makes it even easier to integrate computer vision models into your software without adding any maintenance burden. And, since the Roboflow hosted APIs are running using the Inference package, it's easy to switch between using a hosted server and an on prem server without having to reinvent your client code. To use the hosted API, simply replace the <code>api_url</code> parameter passed to the Inference SDK client configuration. The hosted API base URL is <code>https://detect.roboflow.com</code>.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nproject_id = \"soccer-players-5fuqs/1\"\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nresults = client.infer(image_url, model_id=f\"{model_id}\")\n</code></pre>"},{"location":"using_inference/inference_pipeline/","title":"Inference Pipeline","text":"<p>The Inference Pipeline interface is made for streaming and is likely the best route to go for real time use cases.  It is an asynchronous interface that can consume many different video sources including local devices (like webcams),  RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>Now, since version <code>v0.9.18</code> <code>InferencePipeline</code> supports multiple sources of video at the same time! </p>"},{"location":"using_inference/inference_pipeline/#quickstart","title":"Quickstart","text":"<p>First, install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Next, create an Inference Pipeline:</p> <pre><code># import the InferencePipeline interface\nfrom inference import InferencePipeline\n# import a built-in sink called render_boxes (sinks are the logic that happens after inference)\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\napi_key = \"YOUR_ROBOFLOW_API_KEY\"\n\n# create an inference pipeline object\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\", # set the model id to a yolov8x model with in put size 1280\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\", # set the video reference (source of video), it can be a link/path to a video file, an RTSP stream url, or an integer representing a device id (usually 0 for built in webcams)\n    on_prediction=render_boxes, # tell the pipeline object what to do with each set of inference by passing a function\n    api_key=api_key, # provide your roboflow api key for loading models from the roboflow api\n)\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>Let's break down the example line by line:</p> <p><code>pipeline = InferencePipeline.init(...)</code></p> <p>Here, we are calling a class method of InferencePipeline.</p> <p><code>model_id=\"yolov8x-1280\"</code></p> <p>We set the model ID to a YOLOv8x model pre-trained on COCO with input resolution <code>1280x1280</code>.</p> <p><code>video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\"</code></p> <p>We set the video reference to a URL. Later we will show the various values that can be used as a video reference.</p> <p><code>on_prediction=render_boxes</code></p> <p>The <code>on_prediction</code> argument defines our sink (or a list of sinks).</p> <p><code>pipeline.start(); pipeline.join()</code></p> <p>Here, we start and join the thread that processes the video stream.</p>"},{"location":"using_inference/inference_pipeline/#what-is-video-reference","title":"What is video reference?","text":"<p>Inference Pipelines can consume many different types of video streams.</p> <ul> <li>Device Id (integer): Providing an integer instructs a pipeline to stream video from a local device, like a webcam. Typically, built in webcams show up as device <code>0</code>.</li> <li>Video File (string): Providing the path to a video file will result in the pipeline reading every frame from the file, running inference with the specified model, then running the <code>on_prediction</code> method with each set of resulting predictions.</li> <li>Video URL (string): Providing the path to a video URL is equivalent to providing a video file path and voids needing to first download the video.</li> <li>RTSP URL (string): Providing an RTSP URL will result in the pipeline streaming frames from an RTSP stream as fast as possible, then running the <code>on_prediction</code> callback on the latest available frame.</li> <li>Since version <code>0.9.18</code> - list of elements that may be any of values described above.</li> </ul>"},{"location":"using_inference/inference_pipeline/#how-the-inferencepipeline-works","title":"How the <code>InferencePipeline</code> works?","text":"<p><code>InferencePipeline</code> spins a video source consumer thread for each provided video reference. Frames from videos are grabbed by video multiplexer that awaits <code>batch_collection_timeout</code> (if source will not provide frame, smaller batch  will be passed to <code>on_video_frame(...)</code>, but missing frames and predictions will be filled with <code>None</code> before passing to <code>on_prediction(...)</code>). <code>on_prediction(...)</code> may work in <code>SEQUENTIAL</code> mode (only one element at once), or <code>BATCH</code>  mode - all batch elements at a time and that can be controlled by <code>sink_mode</code> parameter.</p> <p>For static video files, <code>InferencePipeline</code> processes all frames by default, for streams - it is possible to drop frames from the buffers - in favour of always processing the most recent data (when model inference is slow, more frames can be accumulated in buffer - stream processing drop older frames and only processes the most recent one).</p> <p>To enhance stability, in case of streams processing - video sources will be automatically re-connected once  connectivity is lost during processing. That is meant to prevent failures in production environment when the pipeline can run long hours and need to gracefully handle sources downtimes.</p>"},{"location":"using_inference/inference_pipeline/#how-to-provide-a-custom-inference-logic-to-inferencepipeline","title":"How to provide a custom inference logic to <code>InferencePipeline</code>","text":"<p>As of <code>inference&gt;=0.9.16</code>, Inference Pipelines support running custom inference logic. This means, instead of passing  a model ID, you can pass a custom callable. This callable should accept and <code>VideoFrame</code> return a dictionary with  results from the processing (as <code>on_video_frame</code> handler). It can be model predictions or results of any other processing you wish to execute. It is important to note that the sink being used (<code>on_prediction</code> handler you use) - must be adjusted to the specific format of <code>on_video_frame(...)</code> response. This way, you can shape video processing in a way you want.</p> <pre><code># This is example, reference implementation - you need to adjust the code to your purposes\nimport os\nimport json\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference import InferencePipeline\nfrom typing import Any, List\n\nTARGET_DIR = \"./my_predictions\"\n\nclass MyModel:\n\n  def __init__(self, weights_path: str):\n    self._model = your_model_loader(weights_path)\n\n  # before v0.9.18  \n  def infer(self, video_frame: VideoFrame) -&gt; Any:\n    return self._model(video_frame.image)\n\n  # after v0.9.18  \n  def infer(self, video_frames: List[VideoFrame]) -&gt; List[Any]: \n    # result must be returned as list of elements representing model prediction for single frame\n    # with order unchanged.\n    return self._model([v.image for v in video_frames])\n\ndef save_prediction(prediction: dict, video_frame: VideoFrame) -&gt; None:\n  with open(os.path.join(TARGET_DIR, f\"{video_frame.frame_id}.json\")) as f:\n    json.dump(prediction, f)\n\nmy_model = MyModel(\"./my_model.pt\")\n\npipeline = InferencePipeline.init_with_custom_logic(\n  video_reference=\"./my_video.mp4\",\n  on_video_frame=my_model.infer,\n  on_prediction=save_prediction,\n)\n\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre>"},{"location":"using_inference/inference_pipeline/#inferencepipeline-and-roboflow-workflows","title":"<code>InferencePipeline</code> and Roboflow <code>workflows</code>","text":"<p>Info</p> <p>This is feature preview. Please refer to workflows docs.</p> <p>Feature preview do not support multiple videos input!</p> <p>We are working to make <code>workflows</code> compatible with <code>InferencePipeline</code>. Since version <code>0.9.16</code> we introduce  an initializer to be used with workflow definitions. Here is the example:</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\ndef workflows_sink(\n    predictions: dict,\n    video_frame: VideoFrame,\n) -&gt; None:\n    render_boxes(\n        predictions[\"predictions\"][0],\n        video_frame,\n        display_statistics=True,\n    )\n\n\n# here you may find very basic definition of workflow - with a single object detection model.\n# Please visit workflows docs: https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows to\n# find more examples.\nworkflow_specification = {\n    \"specification\": {\n        \"version\": \"1.0\",\n        \"inputs\": [\n            {\"type\": \"InferenceImage\", \"name\": \"image\"},\n        ],\n        \"steps\": [\n            {\n                \"type\": \"ObjectDetectionModel\",\n                \"name\": \"step_1\",\n                \"image\": \"$inputs.image\",\n                \"model_id\": \"yolov8n-640\",\n                \"confidence\": 0.5,\n            }\n        ],\n        \"outputs\": [\n            {\"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.step_1.*\"},\n        ],\n    }\n}\npipeline = InferencePipeline.init_with_workflow(\n    video_reference=\"./my_video.mp4\",\n    workflow_specification=workflow_specification,\n    on_prediction=workflows_sink,\n)\n\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre>"},{"location":"using_inference/inference_pipeline/#sinks","title":"Sinks","text":"<p>Sinks define what an Inference Pipeline should do with each prediction. A sink is a function with signature:</p>"},{"location":"using_inference/inference_pipeline/#before-v0918","title":"Before <code>v0.9.18</code>","text":"<pre><code>from inference.core.interfaces.camera.entities import VideoFrame\n\n\ndef on_prediction(\n    predictions: dict,\n    video_frame: VideoFrame,\n) -&gt; None:\n    pass\n</code></pre> <p>The arguments are:</p> <ul> <li><code>predictions</code>: A dictionary that is the response object resulting from a call to a model's <code>infer(...)</code> method.</li> <li><code>video_frame</code>: A VideoFrame object containing metadata and pixel data from the video frame.</li> </ul>"},{"location":"using_inference/inference_pipeline/#after-v0918","title":"After <code>v0.9.18</code>","text":"<pre><code>from typing import Union, List, Optional\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef on_prediction(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        # SOME PROCESSING\n</code></pre> <p>See more info in Custom Sink section on how to create sink.</p>"},{"location":"using_inference/inference_pipeline/#usage","title":"Usage","text":"<p>You can also make <code>on_prediction</code> accepting other parameters that configure its behaviour, but those needs to be  latched in function closure before injection into <code>InferencePipeline</code> init methods.</p> <pre><code>from functools import partial\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference import InferencePipeline\n\n\ndef on_prediction(\n    predictions: dict,\n    video_frame: VideoFrame,\n    my_parameter: int,\n) -&gt; None:\n    # you need to implement your logic here, with `my_parameter` used\n    pass\n\npipeline = InferencePipeline.init(\n  video_reference=\"./my_video.mp4\",\n  model_id=\"yolov8n-640\",\n  on_prediction=partial(on_prediction, my_parameter=42),\n)\n</code></pre>"},{"location":"using_inference/inference_pipeline/#custom-sinks","title":"Custom Sinks","text":"<p>To create a custom sink, define a new function with the appropriate signature.</p> <pre><code>from typing import Union, List, Optional, Any\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef on_prediction(\n    predictions: Union[Any, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    if not issubclass(type(predictions), list):\n      # this is required to support both sequential and batch processing with single code\n      # if you use only one mode - you may create function that handles with only one type\n      # of input\n      predictions = [predictions]\n      video_frame = [video_frame]\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        # SOME PROCESSING\n</code></pre> <p>In <code>v0.9.18</code> we introduced <code>InferencePipeline</code> parameter called <code>sink_mode</code> - here is how it works. With <code>SinkMode.SEQUENTIAL</code> - each frame and prediction triggers separate call for sink, in case of <code>SinkMode.BATCH</code> -  list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None  values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>.  <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single  video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos -  sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also  possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p>"},{"location":"using_inference/inference_pipeline/#why-there-is-optional-in-listoptionaldict-and-listoptionalvideoframe","title":"Why there is <code>Optional</code> in  <code>List[Optional[dict]]</code> and <code>List[Optional[VideoFrame]]</code>?","text":"<p>It may happen that it is not possible to collect video frames from all the video sources (for instance when one of the  source disconnected and re-connection is attempted). <code>predictions</code> and <code>video_frame</code> are ordered matching the order of <code>video_reference</code> list of <code>InferencePipeline</code> and <code>None</code> elements will appear in position of missing frames. We provide this information to sink, as some sinks may require all predictions and video frames from the batch to be provided (even if missing) - for example: <code>render_boxes(...)</code> sink needs that information to maintain the position of frames in tiles mosaic.</p> <p>Info</p> <p>See our tutorial on creating a custom Inference Pipeline sink!</p> <p>prediction</p> <p>Predictions are provided to the sink as a dictionary containing keys:</p> <ul> <li><code>predictions</code>: predictions - either for single frame or batch of frames. Content depends on which model runs behind  <code>InferencePipeline</code> - for Roboflow models - it will come as dict or list of dicts. The schema of elements is given  below.</li> </ul> <p>Depending on the model output, predictions look differently. You must adjust sink to the prediction format. For instance, Roboflow object-detection prediction contains the following keys:</p> <ul> <li><code>x</code>: The center x coordinate of the predicted bounding box in pixels</li> <li><code>y</code>: The center y coordinate of the predicted bounding box in pixels</li> <li><code>width</code>: The width of the predicted bounding box in pixels</li> <li><code>height</code>: The height of the predicted bounding box in pixels</li> <li><code>confidence</code>: The confidence value of the prediction (between 0 and 1)</li> <li><code>class</code>: The predicted class name</li> <li><code>class_id</code>: The predicted class ID</li> </ul>"},{"location":"using_inference/inference_pipeline/#other-pipeline-configuration","title":"Other Pipeline Configuration","text":"<p>Inference Pipelines are highly configurable. Configurations include:</p> <ul> <li><code>max_fps</code>: Used to set the maximum rate of frame processing.</li> <li><code>confidence</code>: Confidence threshold used for inference.</li> <li><code>iou_threshold</code>: IoU threshold used for inference.</li> <li><code>video_source_properties</code>: Optional dictionary of properties to configure the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. See the OpenCV Documentation for a list of all possible properties.</li> </ul> <pre><code>from inference import InferencePipeline\npipeline = InferencePipeline.init(\n    ...,\n    max_fps=10,\n    confidence=0.75,\n    iou_threshold=0.4,\n    video_source_properties={\n        \"frame_width\": 1920.0,\n        \"frame_height\": 1080.0,\n        \"fps\": 30.0,\n    },\n)\n</code></pre> <p>See the reference docs for the full list of Inference Pipeline parameters.</p>"},{"location":"using_inference/inference_pipeline/#performance","title":"Performance","text":"<p>We tested the performance of Inference on a variety of hardware devices.</p> <p>Below are the results of our benchmarking tests for Inference.</p>"},{"location":"using_inference/inference_pipeline/#macbook-m2","title":"MacBook M2","text":"Test FPS yolov8-n ~26 yolov8-s ~12 yolov8-m ~5 <p>Tested against the same 1080p 60fps RTSP stream emitted by localhost.</p>"},{"location":"using_inference/inference_pipeline/#jetson-orin-nano","title":"Jetson Orin Nano","text":"Test FPS yolov8-n ~25 yolov8-s ~18 yolov8-m ~8 <p>With old version reaching at max 6-7 fps. This test was executed against 4K@60fps stream, which is not possible to be decoded in native pace due to resource constrains. New implementation proved to run without stability issues for few hours straight.</p>"},{"location":"using_inference/inference_pipeline/#tesla-t4","title":"Tesla T4","text":"<p>GPU workstation with Tesla T4 was able to run 4 concurrent HD streams at 15FPS utilising ~80% GPU - reaching over 60FPS throughput per GPU (against <code>yolov8-n</code>).</p>"},{"location":"using_inference/inference_pipeline/#migrating-from-inferencestream-to-inferencepipeline","title":"Migrating from <code>inference.Stream</code> to <code>InferencePipeline</code>","text":"<p>Inference is deprecating support for <code>inference.Stream</code>, our video stream inference interface. <code>inference.Stream</code> is being replaced with <code>InferencePipeline</code>, which has feature parity and achieves better performance. There are also new, more advanced features available in <code>InferencePipeline</code>.</p>"},{"location":"using_inference/inference_pipeline/#new-features-in-inferencepipeline","title":"New Features in <code>InferencePipeline</code>","text":""},{"location":"using_inference/inference_pipeline/#stability","title":"Stability","text":"<p>New implementation allows <code>InferencePipeline</code> to re-connect to a video source, eliminating the need to create additional logic to run inference against streams for long hours in fault-tolerant mode.</p>"},{"location":"using_inference/inference_pipeline/#granularity-of-control","title":"Granularity of control","text":"<p>New implementation let you decide how to handle video sources - and provided automatic selection of mode. Your videos will be processed frame-by-frame with each frame being passed to model, and streams will be processed in a way to provide continuous, up-to-date predictions on the most fresh frames - and the system will automatically adjust to performance of the hardware to ensure best experience.</p>"},{"location":"using_inference/inference_pipeline/#observability","title":"Observability","text":"<p>New implementation allows to create reports about InferencePipeline state in runtime - providing an easy way to build monitoring on top of it.</p>"},{"location":"using_inference/inference_pipeline/#migrate-from-inferencestream-to-inferencepipeline","title":"Migrate from <code>inference.Stream</code> to <code>InferencePipeline</code>","text":"<p>Let's assume you used <code>inference.Stream(...)</code> with your custom handlers:</p> <pre><code>import numpy as np\n\n\ndef on_prediction(predictions: dict, image: np.ndarray) -&gt; None:\n    pass\n</code></pre> <p>Now, the structure of handlers has changed into:</p> <pre><code>import numpy as np\n\ndef on_prediction(predictions, video_frame) -&gt; None:\n    pass\n</code></pre> <p>With predictions being still dict (passed as second parameter) in the same, standard Roboflow format, but <code>video_frame</code> is a dataclass with the following property:</p> <ul> <li><code>image</code>: which is video frame (<code>np.ndarray</code>)</li> <li><code>frame_id</code>: int value representing the place of the frame in stream order</li> <li><code>frame_timestamp</code>: time of frame grabbing - the exact moment when frame appeared in the file/stream   on the receiver side (<code>datetime.datetime</code>)</li> </ul> <p>Additionally, it eliminates the need of grabbing <code>.frame_id</code> from <code>inference.Stream()</code>.</p> <p><code>InferencePipeline</code> exposes interface to manage its state (possibly from different thread) - including functions like <code>.start()</code>, <code>.pause()</code>, <code>.terminate()</code>.</p>"},{"location":"using_inference/inference_pipeline/#migrate-to-changes-introduced-in-v0918","title":"Migrate to changes introduced in <code>v0.9.18</code>","text":"<p>List of changes: 1. <code>VideoFrame</code> got new parameter: <code>source_id</code> - indicating which video source yielded the frame 2. <code>on_prediction</code> callable signature changed: <pre><code>from typing import Callable, Any, Optional, List, Union\nfrom inference.core.interfaces.camera.entities import VideoFrame\n# OLD\nSinkHandler = Callable[[Any, VideoFrame], None]\n\n# NEW\nSinkHandler = Optional[\n    Union[\n        Callable[[Any, VideoFrame], None],\n        Callable[[List[Optional[Any]], List[Optional[VideoFrame]]], None],\n    ]\n]\n</code></pre> this change is non-breaking, as there is new parameter of <code>InferencePipeline.init*()</code> functions - <code>sink_mode</code> with default  value on <code>ADAPTIVE</code> - which forces single video frame and prediction to be provided for sink invocation if one video  only is specified. Old sinks were adjusted to work in dual mode - for instance in the demo you see <code>render_boxes(...)</code>  displaying image tiles.</p> <p>Example: <pre><code>from typing import Union, List, Optional\nimport json\n\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef save_prediction(predictions: dict, file_name: str) -&gt; None:\n  with open(file_name, \"w\") as f:\n    json.dump(predictions, f)\n\ndef on_prediction_old(predictions: dict, video_frame: VideoFrame) -&gt; None:\n  save_prediction(\n    predictions=predictions,\n    file_name=f\"frame_{video_frame.frame_id}.json\"\n  )\n\ndef on_prediction_new(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        save_prediction(\n        predictions=prediction,\n        file_name=f\"source_{frame.source_id}_frame_{frame.frame_id}.json\"\n      )\n</code></pre></p> <ol> <li><code>on_video_frame</code> callable used in InferencePipeline.init_with_custom_logic(...)<code>changed: Previously:</code>InferenceHandler = Callable[[VideoFrame], Any]<code>Now:</code>InferenceHandler = Callable[[List[VideoFrame]], List[Any]]`</li> </ol> <p>Example: <pre><code>from inference.core.interfaces.camera.entities import VideoFrame\nfrom typing import Any, List\n\nMY_MODEL = ...\n\n# before v0.9.18  \ndef on_video_frame_old(video_frame: VideoFrame) -&gt; Any:\n  return MY_MODEL(video_frame.image)\n\n# after v0.9.18  \ndef on_video_frame_new(video_frames: List[VideoFrame]) -&gt; List[Any]: \n  # result must be returned as list of elements representing model prediction for single frame\n  # with order unchanged.\n  return MY_MODEL([v.image for v in video_frames])\n</code></pre></p> <ol> <li>The interface for <code>PipelineWatchdog</code> changed - and there is also a side effect change in form of pipeline state report  that is emitted being changed.</li> </ol> <p>Old watchdog: <pre><code>class PipelineWatchDog(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def register_video_source(self, video_source: VideoSource) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_inference_started(\n        self, frame_timestamp: datetime, frame_id: int\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_prediction_ready(\n        self, frame_timestamp: datetime, frame_id: int\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_report(self) -&gt; Optional[PipelineStateReport]:\n        pass\n</code></pre></p> <p>New watchdog: <pre><code>class PipelineWatchDog(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def register_video_sources(self, video_sources: List[VideoSource]) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_inference_started(\n        self,\n        frames: List[VideoFrame],\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_prediction_ready(\n        self,\n        frames: List[VideoFrame],\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_report(self) -&gt; Optional[PipelineStateReport]:\n        pass\n</code></pre></p> <p>Old report: <pre><code>@dataclass(frozen=True)\nclass PipelineStateReport:\n    video_source_status_updates: List[StatusUpdate]\n    latency_report: LatencyMonitorReport\n    inference_throughput: float\n    source_metadata: Optional[SourceMetadata]\n</code></pre></p> <p>New report: <pre><code>@dataclass(frozen=True)\nclass PipelineStateReport:\n    video_source_status_updates: List[StatusUpdate]\n    latency_reports: List[LatencyMonitorReport]  # now - one report for each source\n    inference_throughput: float\n    sources_metadata: List[SourceMetadata] # now - one metadata for each source\n</code></pre></p> <p>If there was custom watchdog created on your end - reimplementation should be easy, as all the data passed to methods previously for single video source / frame are now provided for all sources / frames.</p>"},{"location":"using_inference/native_python_api/","title":"Native python api","text":"<p>The native python API is the most simple and involves accessing the base package APIs directly. Going this route, you will import Inference modules directly into your python code. You will load models, run inference, and handle the results all within your own logic. You will also need to manage the dependencies within your python environment. If you are creating a simple app or just testing, the native Python API is a great place to start.</p> <p>Using the native python API centers on loading models, then calling their <code>infer(...)</code> method to get inference results.</p>"},{"location":"using_inference/native_python_api/#quickstart","title":"Quickstart","text":"<p>This example shows how to load a model, run inference, then display the results.</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Next, import a model:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n</code></pre> <p>The <code>get_model</code> method is a utility function which will help us load a computer vision model from Roboflow. We load a model by referencing its <code>model_id</code>. For Roboflow models, the model ID is a combination of a project name and a version number <code>f\"{project_name}/{version_number}\"</code>.</p> <p>Hint</p> <p>You can find your models project name and version number in the Roboflow App. You can also browse public models that are ready to use on Roboflow Universe. In this example, we are using a special model ID that is an alias of a COCO pretrained model on Roboflow Universe. You can see the list of model aliases here.</p> <p>Next, we can run inference with our model by providing an input image:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nresults = model.infer(\"people-walking.jpg\") # replace with path to your image\n</code></pre> <p>The results object is an inference response object. It contains some meta data (e.g. processing time) as well as an array of the predictions. The type of response and its attributes will depend on the type of model. See all of the Inference Response objects.</p> <p>Now, lets visualize the results using Supervision:</p> <pre><code>from inference import get_model\nimport supervision as sv\nimport cv2\n\n#Load model\nmodel = get_model(model_id=\"yolov8x-1280\")\n\n#Load image with cv2\nimage = cv2.imread(\"people-walking.jpg\")\n\n#Run inference\nresults = model.infer(image)\n\n#Load results into Supervision Detection API\ndetections = sv.Detections.from_inference(\n    results[0].dict(by_alias=True, exclude_none=True)\n)\n\n#Create Supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n#Extract labels array from inference results\nlabels = [p.class_name for p in results[0].predictions]\n\n\n\n#Apply results to image using Supervision annotators\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\n#Write annotated image to file or display image\ncv2.imwrite(\"people-walking-annotated.jpg\", annotated_image)\n#or sv.plot_image(annotated_image)\n</code></pre> <p></p>"},{"location":"using_inference/native_python_api/#different-image-types","title":"Different Image Types","text":"<p>In the previous example, we saw that we can provide different image types to the <code>infer(...)</code> method. The <code>infer(...)</code> method accepts images in many forms including PIL images, OpenCV images (Numpy arrays), paths to local images, image URLs, and more. Under the hood, models use the <code>load_image(...)</code> method in the <code>image_utils</code> module.</p> <pre><code>from inference import get_model\n\nimport cv2\nfrom PIL import Image\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nimage_url = \"https://media.roboflow.com/inference/people-walking.jpg\"\nlocal_image_file = \"people-walking.jpg\"\npil_image = Image.open(local_image_file)\nnumpy_image = cv2.imread(local_image_file)\n\nresults = model.infer(image_url)\n#or     = model.infer(local_image_file)\n#or     = model.infer(pil_image)\n#or     = model.infer(numpy_image)\n</code></pre>"},{"location":"using_inference/native_python_api/#inference-parameters","title":"Inference Parameters","text":"<p>The <code>infer(...)</code> method accepts keyword arguments to set inference parameters. The example below shows setting the confidence threshold and the IoU threshold.</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nresults = model.infer(\"people-walking.jpg\", confidence=0.75, iou_threshold=0.5)\n</code></pre>"},{"location":"workflows/about/","title":"Inference Workflows","text":"<p>Inference Workflows allow you to define multi-step processes that run one or more models and returns a result based on the output of the models.</p> <p>With Inference workflows, you can:</p> <ul> <li>Detect, classify, and segment objects in images.</li> <li>Apply filters (i.e. process detections in a specific region, filter detections by confidence).</li> <li>Use Large Multimodal Models (LMMs) to make determinations at any stage in a workflow.</li> </ul> <p>You can build simple workflows in the Roboflow web interface that you can then deploy to your own device or the cloud using Inference.</p> <p>You can build more advanced workflows for use on your own devices by writing a workflow configuration directly in JSON.</p> <p>In this section of documentation, we describe what you need to know to create workflows.</p> <p>Here is an example structure for a workflow you can build with Inference Workflows:</p> <p></p> <p>Note</p> <p>We require a Roboflow Enterprise License to use this in production. See inference/enterpise/LICENSE.txt for details.</p>"},{"location":"workflows/absolute_static_crop/","title":"<code>AbsoluteStaticCrop</code> and <code>RelativeStaticCrop</code>","text":"<p>Crop regions of interest in an image.</p> <p>You can use absolute coordinates (integer pixel values) or relative coordinates (fraction of width and height in range [0.0, 1.0]).</p>"},{"location":"workflows/absolute_static_crop/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>AbsoluteStaticCrop</code> / <code>RelativeStaticCrop</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>x_center</code>: OX center coordinate of crop or reference to <code>InputParameter</code> - must be integer for <code>AbsoluteStaticCrop</code> or float in range [0.0, 1.0] in case of <code>RelativeStaticCrop</code></li> <li><code>y_center</code>: OY center coordinate of crop or reference to <code>InputParameter</code> - must be integer for <code>AbsoluteStaticCrop</code> or float in range [0.0, 1.0] in case of <code>RelativeStaticCrop</code></li> <li><code>width</code>: width of crop or reference to <code>InputParameter</code> - must be integer for <code>AbsoluteStaticCrop</code> or float in range [0.0, 1.0] in case of <code>RelativeStaticCrop</code></li> <li><code>height</code>: height of crop or reference to <code>InputParameter</code> - must be integer for <code>AbsoluteStaticCrop</code> or float in range [0.0, 1.0] in case of <code>RelativeStaticCrop</code></li> </ul>"},{"location":"workflows/absolute_static_crop/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>crops</code> - <code>image</code> cropped based on step parameters</li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> </ul>"},{"location":"workflows/active_learning/","title":"<code>ActiveLearningDataCollector</code>","text":"<p>Collect data and predictions that flow through workflows for use in active learning.</p> <p>This block is built on the foundations of Roboflow Active Learning capabilities implemented in  <code>active_learning</code> module.</p>"},{"location":"workflows/active_learning/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>ActiveLearningDataCollector</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>predictions</code> - selector pointing to outputs of detections models output of the detections model: [<code>ObjectDetectionModel</code>,  <code>KeypointsDetectionModel</code>, <code>InstanceSegmentationModel</code>, <code>DetectionFilter</code>, <code>DetectionsConsensus</code>, <code>YoloWorld</code>] (then use <code>$steps.&lt;det_step_name&gt;.predictions</code>) or outputs of classification [<code>ClassificationModel</code>] (then use <code>$steps.&lt;cls_step_name&gt;.top</code>) (required)</li> <li><code>target_dataset</code> - name of Roboflow dataset / project to be used as target for collected data (required)</li> <li><code>target_dataset_api_key</code> - optional API key to be used for data registration. This may help in a scenario when data are to be registered cross-workspaces. If not provided - the API key from a request would be used to register data ( applicable for Universe models predictions to be saved in private workspaces and for models that were trained in the same  workspace (not necessarily within the same project)).</li> <li><code>disable_active_learning</code> - boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request - overrides all AL config. (optional, default: <code>False</code>)</li> <li><code>active_learning_configuration</code> - optional configuration of Active Learning data sampling in the exact format provided in <code>active_learning</code> docs</li> </ul>"},{"location":"workflows/active_learning/#step-outputs","title":"Step outputs","text":"<p>No outputs are declared. This sep is supposed to cause side effect in form of data sampling and registration. </p>"},{"location":"workflows/active_learning/#important-notes","title":"Important Notes","text":"<ul> <li>This block is implemented in non-async way - which means that in certain cases it can block event loop causing parallelization not feasible. This is not the case when running in <code>inference</code> HTTP container. At Roboflow  hosted platform - registration cannot be executed as background task - so its duration must be added into expected  latency</li> <li>Be careful in enabling / disabling AL at the level of steps - remember that when  predicting from each model, <code>inference</code> HTTP API tries to get Active Learning config from the project that model belongs to and register datapoint. To prevent that from happening - model steps can be provided with  <code>disable_active_learning=True</code> parameter. Then the only place where AL registration happens is <code>ActiveLearningDataCollector</code>.</li> <li>Be careful with names of sampling strategies if you define Active Learning configuration -  you should keep them unique not only within a single config, but globally in project - otherwise limits accounting may not work well.</li> </ul>"},{"location":"workflows/classify_objects/","title":"<code>ClassificationModel</code>","text":"<p>Run a classification model.</p>"},{"location":"workflows/classify_objects/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>ClassificationModel</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>model_id</code>: must be either valid Roboflow model ID or selector to  input parameter (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>disable_active_learning</code>: optional boolean flag to control Active Learning at request level - can be selector to  input parameter </li> <li><code>confidence</code>: optional float value in range [0, 1] with threshold - can be selector to  input parameter </li> <li><code>active_learning_target_dataset</code>: optional name of target dataset (or reference to <code>InferenceParemeter</code>)  dictating that AL should collect data to a different dataset than the one declared by the model</li> </ul>"},{"location":"workflows/classify_objects/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>top</code> - top class</li> <li><code>confidence</code> - confidence of prediction</li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>classification</code> model</li> </ul>"},{"location":"workflows/classify_objects_multi/","title":"<code>MultiLabelClassificationModel</code>","text":"<p>Run a multi-label classification model.</p>"},{"location":"workflows/classify_objects_multi/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>MultiLabelClassificationModel</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>model_id</code>: must be either valid Roboflow model ID or selector to  input parameter (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>disable_active_learning</code>: optional boolean flag to control Active Learning at request level - can be selector to  input parameter </li> <li><code>confidence</code>: optional float value in range [0, 1] with threshold - can be selector to  input parameter </li> <li><code>active_learning_target_dataset</code>: optional name of target dataset (or reference to <code>InferenceParemeter</code>)  dictating that AL should collect data to a different dataset than the one declared by the model</li> </ul>"},{"location":"workflows/classify_objects_multi/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>predicted_classes</code> - top classes</li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>classification</code> model</li> </ul>"},{"location":"workflows/compare_clip_vectors/","title":"<code>ClipComparison</code>","text":"<p>Compare CLIP image and text embeddings.</p>"},{"location":"workflows/compare_clip_vectors/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>ClipComparison</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>text</code>: reference to <code>InputParameter</code> of list of texts to compare against <code>image</code> using Clip model</li> </ul>"},{"location":"workflows/compare_clip_vectors/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>similarity</code> - for each element of <code>image</code> - list of float values representing similarity to each element of <code>text</code></li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> </ul>"},{"location":"workflows/condition/","title":"<code>Condition</code>","text":"<p>You can use the <code>Condition</code> block to control the flow of a workflow based on the result of a step.</p> <p>Important</p> <p><code>Condition</code> step is only capable to operate, when single image is provided to the input of the <code>workflow</code> (or more precisely, both <code>left</code> and <code>right</code> if provided with reference, then the reference can only hold value for a result of operation made against single input). This is to prevent situation when evaluation of condition for multiple images yield different execution paths.  </p>"},{"location":"workflows/condition/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>Condition</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>left</code>: left operand of <code>operator</code>, can be actual value, reference to input or step output (required)</li> <li><code>right</code>: left operand of <code>operator</code>, can be actual value, reference to input or step output (required)</li> <li><code>operator</code>: one of <code>equal</code>, <code>not_equal</code>, <code>lower_than</code>, <code>greater_than</code>, <code>lower_or_equal_than</code>, <code>greater_or_equal_than</code>,  <code>in</code>, <code>str_starts_with</code> (meaning <code>left</code> ends with <code>right</code>), <code>str_ends_with</code> (meaning <code>left</code> starts with <code>right</code>),  <code>str_contains</code> (meaning <code>left</code> contains <code>right</code>) (required)</li> <li><code>step_if_true</code>: reference to the step that will be executed if condition is true (required)</li> <li><code>step_if_false</code>: reference to the step that will be executed if condition is false (required)</li> </ul>"},{"location":"workflows/create_and_run/","title":"How to Create and Run a Workflow","text":"<p>Workflows allow you to define multi-step processes that run one or more models and return a result based on the output of the models.</p> <p>You can create and deploy workflows in the cloud or in Inference.</p> <p>To create an advanced workflow for use with Inference, you need to define a specification. A specification is a JSON document that states:</p> <ol> <li>The version of workflows you are using.</li> <li>The expected inputs.</li> <li>The steps the workflow should run (i.e. models to run, filters to apply, etc.).</li> <li>The expected output format.</li> </ol> <p>In this guide, we walk through how to create a basic workflow that completes three steps:</p> <ol> <li>Run a model to detect objects in an image.</li> <li>Crops each region.</li> <li>Runs OCR on each region.</li> </ol> <p>You can use the guidance below as a template to learn the structure of workflows, or verbatim to create your own detect-then-OCR workflows.</p>"},{"location":"workflows/create_and_run/#step-1-define-an-input","title":"Step #1: Define an Input","text":"<p>Workflows require a specification to run. A specification takes the follwoing form:</p> <pre><code>SPECIFICATION = {\n    \"specification\": {\n        \"version\": \"1.0\",\n        \"inputs\": [],\n        \"steps\": [],\n        \"outputs\": []\n    }\n}\n</code></pre> <p>Within this structure, we need to define our:</p> <ol> <li>Model inputs</li> <li>The steps to run</li> <li>The expected output</li> </ol> <p>First, let's define our inputs.</p> <p>For this workflow, we will specify an image input:</p> <pre><code>\"steps\": [\n{ \"type\": \"InferenceImage\", \"name\": \"image\" },   # definition of input image\n]\n</code></pre>"},{"location":"workflows/create_and_run/#step-2-define-processing-steps","title":"Step #2: Define Processing Steps","text":"<p>Next, we need to define our processing steps. For this guide, we want to:</p> <ol> <li>Run a model to detect license plates.</li> <li>Crop each license plate.</li> <li>Run OCR on each license plate.</li> </ol> <p>We can define these steps as follows:</p> <pre><code>\"steps\": [\n{\n\"type\": \"ObjectDetectionModel\",   # definition of object detection model\n\"name\": \"plates_detector\",  \"image\": \"$inputs.image\",  # linking input image into detection model\n\"model_id\": \"vehicle-registration-plates-trudk/2\",  # pointing model to be used\n},\n{\n\"type\": \"DetectionOffset\",  # DocTR model usually works better if there is slight padding around text to be detected - hence we are offseting predictions\n\"name\": \"offset\",\n\"predictions\": \"$steps.plates_detector.predictions\",  # reference to the object detection model output\n\"offset_x\": 200,  # value of offset\n\"offset_y\": 40,  # value of offset\n},\n{\n\"type\": \"Crop\",   # we would like to run OCR against each and every plate detected - hece we are cropping inputr image using offseted predictions\n\"name\": \"cropping\",\n\"image\": \"$inputs.image\",  # we need to point image to crop\n\"detections\": \"$steps.offset.predictions\",  # we need to point detections that will be used to crop image (in this case - we use offseted prediction)\n},        {\n\"type\": \"OCRModel\",  # we define OCR model\n\"name\": \"step_ocr\",\n\"image\": \"$steps.cropping.crops\",  # OCR model as an input takes a reference to crops that were created based on detections\n},\n],\n</code></pre>"},{"location":"workflows/create_and_run/#step-3-define-an-output","title":"Step #3: Define an Output","text":"<p>Finally, we need to define the output for our workflow:</p> <pre><code>\"outputs\": [\n{ \"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.plates_detector.predictions\" },  # output with object detection model predictions\n{ \"type\": \"JsonField\", \"name\": \"image\", \"selector\": \"$steps.plates_detector.image\" },  # output with image metadata - required by `supervision`\n{ \"type\": \"JsonField\", \"name\": \"recognised_plates\", \"selector\": \"$steps.step_ocr.result\" },  # field that will retrieve OCR result\n{ \"type\": \"JsonField\", \"name\": \"crops\", \"selector\": \"$steps.cropping.crops\" },  # crops that were made based on plates detections - used here just to ease visualisation\n]   </code></pre>"},{"location":"workflows/create_and_run/#step-4-run-your-workflow","title":"Step #4: Run Your Workflow","text":"<p>Now that we have our specification, we can run our workflow using the Inference SDK.</p> Run Locally with InferenceRun in the Roboflow Cloud <p>Use <code>inference_cli</code> to start server</p> <pre><code>inference server start\n</code></pre> <pre><code>from inference_sdk import InferenceHTTPClient, VisualisationResponseFormat, InferenceConfiguration\nimport supervision as sv\nimport cv2\nfrom matplotlib import pyplot as plt\n\nclient = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",\n    api_key=\"YOUR_API_KEY\"\n)\n\nclient.configure(\n    InferenceConfiguration(output_visualisation_format=VisualisationResponseFormat.NUMPY)\n)\n\nlicense_plate_image_1 = cv2.imread(\"./images/license_plate_1.jpg\")\n\nlicense_plate_result_1 = client.infer_from_workflow(\n    specification=READING_PLATES_SPECIFICATION[\"specification\"],\n    images={\"image\": license_plate_image_1},\n)\n\nplt.title(f\"Recognised plate: {license_plate_result_1['recognised_plates']}\")\nplt.imshow(license_plate_result_1[\"crops\"][0][\"value\"][:, :, ::-1])\nplt.show()\n</code></pre> <p>Here are the results:</p> <p></p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=\"YOUR_API_KEY\"\n)\n\nclient.configure(\n    InferenceConfiguration(output_visualisation_format=VisualisationResponseFormat.NUMPY)\n)\n\nlicense_plate_image_1 = cv2.imread(\"./images/license_plate_1.jpg\")\n\nlicense_plate_result_1 = client.infer_from_workflow(\n    specification=READING_PLATES_SPECIFICATION[\"specification\"],\n    images={\"image\": license_plate_image_1},\n)\n\nplt.title(f\"Recognised plate: {license_plate_result_1['recognised_plates']}\")\nplt.imshow(license_plate_result_1[\"crops\"][0][\"value\"][:, :, ::-1])\nplt.show()\n</code></pre> <p>Here are the results:</p> <p></p>"},{"location":"workflows/create_and_run/#next-steps","title":"Next Steps","text":"<p>Now that you have created and run your first workflow, you can explore our other supported blocks and create a more complex workflow.</p> <p>Refer to our Supported Blocks documentation to learn more about what blocks are supported.</p>"},{"location":"workflows/crop/","title":"<code>Crop</code>","text":"<p>Create dynamic crops of all regions returned as bounding boxes from an object detection or segmentation model.</p>"},{"location":"workflows/crop/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>Crop</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>detections</code>: must be a reference to <code>predictions</code> property of steps: [<code>ObjectDetectionModel</code>,  <code>KeypointsDetectionModel</code>, <code>InstanceSegmentationModel</code>, <code>DetectionFilter</code>, <code>DetectionsConsensus</code>, <code>YoloWorld</code>] (required)</li> </ul>"},{"location":"workflows/crop/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>crops</code> - <code>image</code> cropped based on <code>detections</code></li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> </ul>"},{"location":"workflows/detect_barcodes/","title":"<code>BarcodeDetection</code>","text":"<p>Detect the location and value barcodes in an image.</p>"},{"location":"workflows/detect_barcodes/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>BarcodeDetection</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> </ul>"},{"location":"workflows/detect_barcodes/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions<ul> <li>Note: <code>predictions.data</code> is a string which is populated with the data contents of the barcode.</li> </ul> </li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to</li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>barcode-detection</code> model</li> </ul>"},{"location":"workflows/detect_keypoints/","title":"<code>KeypointsDetectionModel</code>","text":"<p>Run inference on a keypoint detection model.</p>"},{"location":"workflows/detect_keypoints/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>KeypointsDetectionModel</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>model_id</code>: must be either valid Roboflow model ID or selector to  input parameter (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>disable_active_learning</code>: optional boolean flag to control Active Learning at request level - can be selector to  input parameter </li> <li><code>confidence</code>: optional float value in range [0, 1] with threshold - can be selector to  input parameter </li> <li><code>class_agnostic_nms</code>: optional boolean flag to control NMS - can be selector to  input parameter </li> <li><code>class_filter</code>: optional list of classes using as filter - can be selector to  input parameter </li> <li><code>iou_threshold</code>: optional float value in range [0, 1] with NMS IoU threshold - can be selector to  input parameter. Default: <code>0.3</code>.</li> <li><code>max_detections</code>: optional integer parameter of NMS - can be selector to input parameter </li> <li><code>max_candidates</code>: optional integer parameter of NMS - can be selector to input parameter </li> <li><code>keypoint_confidence</code>: optional float value in range [0, 1] with keypoints confidence threshold - can be selector to  input parameter </li> <li><code>active_learning_target_dataset</code>: optional name of target dataset (or reference to <code>InferenceParemeter</code>)  dictating that AL should collect data to a different dataset than the one declared by the model</li> </ul>"},{"location":"workflows/detect_keypoints/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>keypoint-detection</code> model</li> </ul>"},{"location":"workflows/detect_objects/","title":"<code>ObjectDetectionModel</code>","text":"<p>Detect objects using an object detection model.</p>"},{"location":"workflows/detect_objects/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>ObjectDetectionModel</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>model_id</code>: must be either valid Roboflow model ID or selector to  input parameter (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>disable_active_learning</code>: optional boolean flag to control Active Learning at request level - can be selector to  input parameter </li> <li><code>confidence</code>: optional float value in range [0, 1] with threshold - can be selector to  input parameter </li> <li><code>class_agnostic_nms</code>: optional boolean flag to control NMS - can be selector to  input parameter </li> <li><code>class_filter</code>: optional list of classes using as filter - can be selector to  input parameter </li> <li><code>iou_threshold</code>: optional float value in range [0, 1] with NMS IoU threshold - can be selector to  input parameter. Default: <code>0.3</code>.</li> <li><code>max_detections</code>: optional integer parameter of NMS - can be selector to input parameter </li> <li><code>max_candidates</code>: optional integer parameter of NMS - can be selector to input parameter </li> <li><code>active_learning_target_dataset</code>: optional name of target dataset (or reference to <code>InferenceParemeter</code>)  dictating that AL should collect data to a different dataset than the one declared by the model</li> </ul>"},{"location":"workflows/detect_objects/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>object-detection</code> model</li> </ul>"},{"location":"workflows/detect_qr_codes/","title":"<code>QRCodeDetection</code>","text":"<p>Detect the location of QR codes in an image.</p>"},{"location":"workflows/detect_qr_codes/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>QRCodeDetection</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> </ul>"},{"location":"workflows/detect_qr_codes/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions<ul> <li>Note: <code>predictions.data</code> is a string which is populated with the data contents of the QR code.</li> </ul> </li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to</li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>qrcode-detection</code> model</li> </ul>"},{"location":"workflows/filter_detections/","title":"<code>DetectionFilter</code>","text":"<p>Filter predictions from detection models based on conditions defined.</p>"},{"location":"workflows/filter_detections/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>DetectionFilter</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>predictions</code>: reference to <code>predictions</code> output of the detections model: [<code>ObjectDetectionModel</code>,  <code>KeypointsDetectionModel</code>, <code>InstanceSegmentationModel</code>, <code>DetectionFilter</code>, <code>DetectionsConsensus</code>, <code>DetectionOffset</code>, <code>YoloWorld</code>] (required)</li> <li><code>filter_definition</code>: definition of the filter (required)</li> </ul> <p>Filter definition can be either <code>DetectionFilterDefinition</code> <pre><code>{\n\"type\": \"DetectionFilterDefinition\",\n\"field_name\": \"confidence\",\n\"operator\": \"greater_or_equal_than\",\n\"reference_value\": 0.2\n}\n</code></pre> or <code>CompoundDetectionFilterDefinition</code> <pre><code>{\n\"type\": \"CompoundDetectionFilterDefinition\",\n\"left\": {\n\"type\": \"DetectionFilterDefinition\",\n\"field_name\": \"class_name\",\n\"operator\": \"equal\",\n\"reference_value\": \"car\"\n},\n\"operator\": \"and\",\n\"right\": {\n\"type\": \"DetectionFilterDefinition\",\n\"field_name\": \"confidence\",\n\"operator\": \"greater_or_equal_than\",\n\"reference_value\": 0.2\n}\n}\n</code></pre></p> <p>where <code>DetectionFilterDefinition</code> uses binary operator and the left operand is detection field pointed by <code>field_name</code> and right operand is <code>reference_value</code>. <code>\"operaror\"</code> can be filled with values: * <code>equal</code> (field value equal to <code>reference_value</code>) * <code>not_equal</code> * <code>lower_than</code> * <code>greater_than</code> * <code>lower_or_equal_than</code> * <code>greater_or_equal_than</code> * <code>in</code> (field value in range of <code>reference_value</code>) * <code>str_starts_with</code> (field value - string - starts from <code>reference_value</code>) * <code>str_ends_with</code> (field value - string - ends with <code>reference_value</code>) * <code>str_contains</code> (field value - string - contains substring pointed in <code>reference_value</code>)</p> <p>In case if <code>CompoundDetectionFilterDefinition</code>, logical operators <code>or</code>, <code>and</code> can be used to combine simple filters. This let user define recursive structure of filters.</p>"},{"location":"workflows/filter_detections/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting parent model type</li> </ul>"},{"location":"workflows/modes_of_running/","title":"Modes of Running Workflows","text":"<p>Workflows can be executed in <code>local</code> environment, or <code>remote</code> environment can be used. <code>local</code> means that model steps will be executed within the context of process running the code. <code>remote</code> will re-direct model steps into remote API using HTTP requests to send images and get predictions back. </p> <p>When <code>workflows</code> are used directly, in Python code - <code>compile_and_execute(...)</code> and <code>compile_and_execute_async(...)</code> functions accept <code>step_execution_mode</code> parameter that controls the execution mode.</p> <p>Additionally, <code>max_concurrent_steps</code> parameter dictates how many steps in parallel can be executed. This will improve efficiency of <code>remote</code> execution (up to the limits of remote API capacity) and can improve <code>local</code> execution if <code>model_manager</code> instance is capable of running parallel requests (only using extensions from  <code>inference.enterprise.parallel</code>).</p> <p>There are environmental variables that controls <code>workflows</code> behaviour:</p> <ul> <li><code>DISABLE_WORKFLOW_ENDPOINTS</code> - disabling workflows endpoints from HTTP API</li> <li><code>WORKFLOWS_STEP_EXECUTION_MODE</code> - with values <code>local</code> and <code>remote</code> allowed to control how <code>workflows</code> are executed in <code>inference</code> HTTP container</li> <li><code>WORKFLOWS_REMOTE_API_TARGET</code> - with values <code>hosted</code> and <code>self-hosted</code> allowed - to point API to be used in <code>remote</code> execution mode</li> <li><code>LOCAL_INFERENCE_API_URL</code> will be used if <code>WORKFLOWS_REMOTE_API_TARGET=self-hosted</code> and  <code>WORKFLOWS_STEP_EXECUTION_MODE=remote</code></li> <li><code>WORKFLOWS_MAX_CONCURRENT_STEPS</code> - max concurrent steps to be allowed by <code>workflows</code> executor</li> <li><code>WORKFLOWS_REMOTE_EXECUTION_MAX_STEP_BATCH_SIZE</code> - max batch size for requests into remote API made when <code>remote</code> execution mode is chosen</li> <li><code>WORKFLOWS_REMOTE_EXECUTION_MAX_STEP_CONCURRENT_REQUESTS</code> - max concurrent requests to be possible in scope of single step execution when <code>remote</code> execution mode is chosen</li> </ul>"},{"location":"workflows/ocr/","title":"<code>OCRModel</code>","text":"<p>Run Optical Character Recognition on a model.</p>"},{"location":"workflows/ocr/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>OCRModel</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> </ul>"},{"location":"workflows/ocr/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>result</code> - details of predictions</li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>ocr</code> model</li> </ul>"},{"location":"workflows/offset_detections/","title":"<code>DetectionOffset</code>","text":"<p>Apply fixed offset on width and height of detections.</p>"},{"location":"workflows/offset_detections/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>DetectionOffset</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>predictions</code>: reference to <code>predictions</code> output of the detections model: [<code>ObjectDetectionModel</code>,  <code>KeypointsDetectionModel</code>, <code>InstanceSegmentationModel</code>, <code>DetectionFilter</code>, <code>DetectionsConsensus</code>, <code>DetectionOffset</code>, <code>YoloWorld</code>] (required)</li> <li><code>offset_x</code>: reference to input parameter of integer value for detection width offset (required)</li> <li><code>offset_y</code>: reference to input parameter of integer value for detection height offset (required)</li> </ul>"},{"location":"workflows/offset_detections/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting parent model type</li> </ul>"},{"location":"workflows/reach_consensus/","title":"<code>DetectionsConsensus</code>","text":"<p>Combine predictions from multiple detections models to make a decision about object presence.</p> <p>Steps checks for object presence (according to configurable criteria), combines detections and decides on requested objects presence (based on overlap of predictions from different models). It works for  <code>object-detection</code>, <code>instance-segmentation</code> and <code>keypoint-detection</code> models, but consensus output is only applied at detections level. </p> <p>Step executes following operations (in order): * get only the predictions from <code>classes_to_consider</code> (if specified) * for every prediction finds predictions with max overlap from all other sources (at most one per source) that reaches  <code>iou_threshold</code>  * for each group of overlapping predictions from different sources - if the size of group is at least <code>required_votes</code> and merged boxe meet <code>confidence</code> threshold - those are discarded from the pool of detections to be  picked up and are merged into element of <code>predictions</code> output that can be called consensus output.  <code>class_aware</code> parameter decides if class names matter while merging - should be <code>False</code> when different class names are  produced by different models  but the visual concept that models predict is the same. * merge is done based on <code>detections_merge_confidence_aggregation</code> and <code>detections_merge_coordinates_aggregation</code>  parameters that control how to pick the merged box class, confidence and box coordinates * once all elements of consensus outputs are ready, the step prepares <code>object_present</code> status and <code>presence_confidence</code> that form a summary of consensus output. One may state <code>required_objects</code> as integer or dict mapping class name to required instance of objects. In the final state, the step logic will check if required number of objects (possibly from different classes) are detected in consensus output. If that's the case - <code>object_present</code> field will be <code>True</code> and <code>presence_confidence</code> will be calculated (using <code>presence_confidence_aggregation</code> method). Otherwise - <code>presence_confidence</code> will be an empty dict. In the case of <code>class_aware=False</code>:   * when <code>required_objects</code> is dict with class to count mapping - effective <code>required_objects</code> will be sum of dictionary    values   * the <code>presence_confidence</code> will hold <code>any_object</code> key with confidence aggregated among all merged detections. </p> <p></p>"},{"location":"workflows/reach_consensus/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>DetectionsConsensus</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>predictions</code>: list of selectors pointing to outputs of detections models output of the detections model: [<code>ObjectDetectionModel</code>,  <code>KeypointsDetectionModel</code>, <code>InstanceSegmentationModel</code>, <code>DetectionFilter</code>, <code>DetectionsConsensus</code>, <code>YoloWorld</code>] (required, must contain at least 2 elements)</li> <li><code>required_votes</code>: number of models that must agree on the detection - integer or selector pointing at <code>InferenceParameter</code> (required)</li> <li><code>class_aware</code>: flag deciding if class names are taken into account when finding overlapping bounding boxes from multiple models and object presence check. Can be <code>bool</code> or selector to <code>InferenceParameter</code>. Default: <code>True</code></li> <li><code>iou_threshold</code>: optional float value in range [0, 1] with IoU threshold that must be meet to consider two bounding boxes overlapping. Can be float or selector to <code>InferenceParameter</code>. Default: <code>0.3</code>.</li> <li><code>confidence</code>: optional float value in range [0, 1] minimal confidence of aggregated detection that must be met to  be taken into account in presence assessment and consensus procedure. For prior-consensus filtering - use confidence threshold at model level or <code>DetectionsFilter</code>. Default: <code>0.0</code>.</li> <li><code>classes_to_consider</code>: Optional list of classes to consider in consensus procedure. Can be list of <code>str</code> or selector to <code>InferenceParameter</code>. Default: <code>None</code> - in this case  classes filtering of predictions will not be enabled.</li> <li><code>required_objects</code> - If given, it holds the number of objects that must be present in merged results, to assume that  object presence is reached. Can be selector to <code>InferenceParameter</code>, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.</li> <li><code>presence_confidence_aggregation</code> - mode dictating aggregation of confidence scores and classes both in case of object presence deduction procedure. One of <code>average</code>, <code>max</code>, <code>min</code>. Default: <code>max</code>.</li> <li><code>detections_merge_confidence_aggregation</code> - mode dictating aggregation of confidence scores and classes both in case of boxes consensus procedure.  One of <code>average</code>, <code>max</code>, <code>min</code>. Default: <code>average</code>. While using for merging overlapping boxes,  against classes - <code>average</code> equals to majority vote, <code>max</code> - for the class of detection with max confidence, <code>min</code> - for the class of detection with min confidence.</li> <li><code>detections_merge_coordinates_aggregation</code> - mode dictating aggregation of bounding boxes. One of <code>average</code>, <code>max</code>, <code>min</code>.  Default: <code>average</code>. <code>average</code> means taking mean from all boxes coordinates, <code>min</code> - taking smallest box, <code>max</code> - taking  largest box.</li> </ul>"},{"location":"workflows/reach_consensus/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines (can be <code>undefined</code> if all sources of predictions give no prediction)</li> <li><code>object_present</code> - for each input image, boolean flag with information whether or not objects specified in config are present</li> <li><code>presence_confidence</code> - for each input image, for each present class - aggregated confidence indicating presence of objects</li> <li><code>prediction_type</code> - denoting <code>object-detection</code> prediction (as this format is effective even if other detections  models are combined)</li> </ul>"},{"location":"workflows/segment_objects/","title":"<code>InstanceSegmentationModel</code>","text":"<p>Run an instance segmentation model.</p>"},{"location":"workflows/segment_objects/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>InstanceSegmentationModel</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>model_id</code>: must be either valid Roboflow model ID or selector to  input parameter (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>disable_active_learning</code>: optional boolean flag to control Active Learning at request level - can be selector to  input parameter </li> <li><code>confidence</code>: optional float value in range [0, 1] with threshold - can be selector to  input parameter </li> <li><code>class_agnostic_nms</code>: optional boolean flag to control NMS - can be selector to  input parameter </li> <li><code>class_filter</code>: optional list of classes using as filter - can be selector to  input parameter </li> <li><code>iou_threshold</code>: optional float value in range [0, 1] with NMS IoU threshold - can be selector to  input parameter. Default: <code>0.3</code>.</li> <li><code>max_detections</code>: optional integer parameter of NMS - can be selector to input parameter </li> <li><code>max_candidates</code>: optional integer parameter of NMS - can be selector to input parameter </li> <li><code>mask_decode_mode</code>: optional parameter of post-processing - can be selector to input parameter </li> <li><code>tradeoff_factor</code>: optional parameter of post-processing - can be selector to  input parameter </li> <li><code>active_learning_target_dataset</code>: optional name of target dataset (or reference to <code>InferenceParemeter</code>)  dictating that AL should collect data to a different dataset than the one declared by the model</li> </ul>"},{"location":"workflows/segment_objects/#step-outputs","title":"Step outputs:","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>instance-segmentation</code> model</li> </ul>"},{"location":"workflows/understanding/","title":"Understanding Workflows","text":"<p>Important</p> <p>This document talks about how Workflows work in depth. This guide is recommended for advanced users.</p>"},{"location":"workflows/understanding/#how-to-create-workflow-specification","title":"How to create workflow specification?","text":""},{"location":"workflows/understanding/#workflow-specification-deep-dive","title":"Workflow specification deep dive","text":"<p>Workflow specification is defined via a JSON document in the following format: <pre><code>{\n\"specification\": {\n\"version\": \"1.0\",\n\"inputs\": [],\n\"steps\": [],\n\"outputs\": []\n}\n}\n</code></pre></p> <p>In general, we have three main elements of specification:</p> <ul> <li><code>inputs</code> - the section where we define all parameters that can be passed in the execution time by <code>inference</code> user.</li> <li><code>steps</code> - the section where we define computation steps, their interconnections, connections to <code>inputs</code> and <code>outputs</code>.</li> <li><code>outputs</code> - the section where we define all fields that needs to be rendered in the final result</li> </ul>"},{"location":"workflows/understanding/#how-can-we-refer-between-elements-of-specification","title":"How can we refer between elements of specification?","text":"<p>To create a graph of computations, we need to define links between steps - in order to do it - we need to have a  way to refer to specific elements. By convention, the following references are allowed:  <code>${type_of_element}.{name_of_element}</code> and <code>${type_of_element}.{name_of_element}.{property}</code>.</p> <p>Examples:</p> <ul> <li><code>$inputs.image</code> - reference to an input called <code>image</code></li> <li><code>$steps.my_step.predictions</code> - reference to a step called <code>my_step</code> and its property <code>predictions</code> Additionally, defining outputs, it is allowed (since <code>v0.9.14</code>) to use wildcard selector (<code>${type_of_element}.{name_of_element}.*</code>) with intention to extract all properties of given step.</li> </ul> <p>In the code, we usually call references selectors.</p>"},{"location":"workflows/understanding/#how-can-we-define-inputs","title":"How can we define <code>inputs</code>?","text":"<p>At the moment, the compiler supports two types of inputs <code>InferenceParameter</code> and <code>InferenceImage</code>.</p>"},{"location":"workflows/understanding/#inferenceimage","title":"<code>InferenceImage</code>","text":"<p>This input is reserved to represent image or list of images. Definition format: <pre><code>{\"type\": \"InferenceImage\", \"name\": \"my_image\"}\n</code></pre> When creating <code>InferenceImage</code> you do not point a specific image - you just create a placeholder that will be linked with other element of the graph. This placeholder will be substituted with actual image when you run the workflow  graph and provide input parameter called <code>my_image</code> that can be <code>np.ndarray</code> or other formats that <code>inference</code> support, like:</p> <pre><code>{\n\"type\": \"url\",\n\"value\": \"https://here.is/my/image.jpg\"\n}\n</code></pre>"},{"location":"workflows/understanding/#inferenceparameter","title":"<code>InferenceParameter</code>","text":"<p>Similar to <code>InferenceImage</code> - <code>InferenceParameter</code> creates a placeholder for a parameters that can be used in runtime to alter execution of workflow graph. <pre><code>{\"type\": \"InferenceParameter\", \"name\": \"confidence_threshold\", \"default_value\": 0.5}\n</code></pre> <code>InferenceParameters</code> may be optionally defined with default values that will be used, if no actual parameter  of given name is present in user-defined input while executing the workflow graph. Type of parameter is not explicitly defined, but will be checked in runtime, prior to execution based on types of parameters that steps using this parameters can accept.</p>"},{"location":"workflows/understanding/#how-can-we-define-steps","title":"How can we define <code>steps</code>?","text":"<p>Compiler supports multiple type of steps (that will be described later), but let's see how to define a simple one, that would be responsible for making prediction from object-detection model: <pre><code>{\n\"type\": \"ObjectDetectionModel\",\n\"name\": \"my_object_detection\",\n\"image\": \"$inputs.image\",\n\"model_id\": \"yolov8n-640\"\n}\n</code></pre> You can see that the step must have its type associated (that's how we link JSON document elements into code definitions) and name (unique within all steps). Another required parameters are <code>image</code> and <code>model_id</code>.</p> <p>In case of <code>image</code> - we use reference to the input - that's how we create a link between parameter that will be provided in runtime and computational step. Steps parameters can be also provided as predefined values (like <code>model_id</code> in thiscase). Majority of parameters can be defined both as references to inputs (or outputs of other steps) and predefined values.</p>"},{"location":"workflows/understanding/#how-can-we-define-outputs","title":"How can we define <code>outputs</code>?","text":"<p>Definition of single output looks like that: <pre><code>{\"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.step_1.predictions\"}\n</code></pre> it defines a single output dictionary key (of name <code>predictions</code>) that will be created. <code>selector</code> field creates a link between step output and result. In this case, selector points <code>step_1</code> and its property - <code>predictions</code>.</p> <p>Additionally, optional parameter <code>coordinates_system</code> can be defined with one of two values (<code>\"own\", \"parent\"</code>). This parameter defaults to <code>parent</code> and describe the coordinate system of detections that should be used. This setting is only important in case of more complicated graphs (where we crop based on predicted detections and later on make another detections on each and every crop).</p>"},{"location":"workflows/understanding/#example","title":"Example","text":"<p>In the following example, we create a pipeline that at first makes classification first. Based on results (the top class), <code>step_2</code> decides which object detection model to use (if model predicts car, <code>step_3</code> will be executed, <code>step_4</code> will be used otherwise).</p> <p>Result is build from the outputs of all models. Always one of field <code>step_3_predictions</code> and <code>step_4_predictions</code> will be empty due to conditional execution.</p> <pre><code>{\n\"specification\": {\n\"version\": \"1.0\",\n\"inputs\": [\n{\"type\": \"InferenceImage\", \"name\": \"image\"}\n],\n\"steps\": [\n{\n\"type\": \"ClassificationModel\",\n\"name\": \"step_1\",\n\"image\": \"$inputs.image\",\n\"model_id\": \"vehicle-classification-eapcd/2\",\n\"confidence\": 0.4\n},\n{\n\"type\": \"Condition\",\n\"name\": \"step_2\",\n\"left\": \"$steps.step_1.top\",\n\"operator\": \"equal\",\n\"right\": \"Car\",\n\"step_if_true\": \"$steps.step_3\",\n\"step_if_false\": \"$steps.step_4\"\n},\n{\n\"type\": \"ObjectDetectionModel\",\n\"name\": \"step_3\",\n\"image\": \"$inputs.image\",\n\"model_id\": \"yolov8n-640\",\n\"confidence\": 0.5,\n\"iou_threshold\": 0.4\n},\n{\n\"type\": \"ObjectDetectionModel\",\n\"name\": \"step_4\",\n\"image\": \"$inputs.image\",\n\"model_id\": \"yolov8n-1280\",\n\"confidence\": 0.5,\n\"iou_threshold\": 0.4\n}\n],\n\"outputs\": [\n{\"type\": \"JsonField\", \"name\": \"top_class\", \"selector\": \"$steps.step_1.top\"},\n{\"type\": \"JsonField\", \"name\": \"step_3_predictions\", \"selector\": \"$steps.step_3.predictions\"},\n{\"type\": \"JsonField\", \"name\": \"step_4_predictions\", \"selector\": \"$steps.step_4.predictions\"}\n]  }\n}\n</code></pre>"},{"location":"workflows/understanding/#the-notion-of-parents-in-workflows","title":"The notion of parents in <code>workflows</code>","text":"<p>Let's imagine a scenario when we have a graph definition that requires inference from object detection model on input  image. For each image that we have as an input - there will be most likely several detections. There is nothing that prevents us to do something with those detections. For instance, we can crop original image to extract RoIs with objects that the model detected. For each crop, we may then apply yet another, specialised object detection model to  detect lower resolution details. As you probably know, when <code>inference</code> makes prediction, it outputs the coordinates of detections scaled to the size of input image. </p> <p>But in this example, the input image is unknown when we start the process - those will be inferred by first model. To make it possible to combine predictions, we introduced <code>parent_id</code> identifier of prediction. It will be randomly generated string or name of input element that is responsible for certain prediction. </p> <p>In our example, each detection from first model will be assigned unique identifier (<code>detection_id</code>). This identifier will be a <code>parent_id</code> for each prediction that is made based on the crop originated in detection. What is more, each output can be <code>coordinates_system</code> parameter deciding how to present the result. If <code>parent</code> coordinates mode is selected - detections made against crop will be translated to the coordinates of original image that was submitted. Thanks to that, results can be easily overlay on the input image (for instance using <code>supervision</code> library). </p>"},{"location":"workflows/use_lmm/","title":"<code>LMM</code>","text":"<p>Use Large Language models with <code>workflows</code>. With this block in place, one may prompt both <code>GPT-4V</code> and <code>CogVLM</code> models and combine their outputs with other workflow components, effectively building powerful applications without single line of code written. </p> <p>The <code>LMM</code> block allows to specify structure of expected output, automatically inject the specification into prompt and parse expected structure into block outputs that are accessible (and can be referred) by other <code>workflows</code> components. LMMs may occasionally produce non-parsable results according to specified output structure - in that cases, outputs will be filled with <code>not_detected</code> value.</p>"},{"location":"workflows/use_lmm/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>LMM</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>prompt</code>: must be string of reference to <code>InferenceParameter</code> - value holds unconstrained text prompt to LMM model  (required).   </li> <li><code>lmm_type</code>: must be string of reference to <code>InferenceParameter</code> - value holds the type of LMM model to be used -  allowed values: <code>gpt_4v</code> and <code>cog_vlm</code> (required)</li> <li><code>lmm_config</code>: (optional) structure that has the following schema: <pre><code>{\n\"max_tokens\": 450,\n\"gpt_image_detail\": \"low\",\n\"gpt_model_version\": \"gpt-4-vision-preview\"\n}\n</code></pre> to control inner details of LMM prompting. All parameters now are suited to control GPT API calls. Default for max tokens is <code>450</code>, <code>gpt_image_detail</code> default is <code>auto</code> (allowed values: <code>low</code>, <code>auto</code>, <code>high</code>),  <code>gpt_model_version</code> is <code>gpt-4-vision-preview</code>.</li> <li><code>remote_api_key</code> - optional string or reference to <code>InferenceParameter</code> that holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code> and do not require additional API key for CogVLM calls.</li> <li><code>json_output</code>: optional <code>dict[str, str]</code> (pointing expected output JSON field name to its description) or reference to <code>InferenceParameter</code> with such dict. This field is used to instruct model on expected output  format. One may not specify field names: <code>[\"raw_output\", \"structured_output\", \"image\", \"parent_id\"]</code>, due to the fact that keys from <code>json_output</code> dict will be registered as block outputs (to be referred by other blocks) and cannot collide with basic outputs of that block. Additional outputs **will only be registered if defined in-place,  not via <code>InferenceParameter</code>).</li> </ul>"},{"location":"workflows/use_lmm/#step-outputs","title":"Step outputs","text":"<ul> <li><code>raw_output</code> - raw output of LMM for each input image</li> <li><code>structured_output</code> - if <code>json_output</code> is specified, whole parsed dictionary for each input image will be placed in this field,  otherwise for each image, empty dict will be returned</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li>for each of <code>json_output</code> - dedicated field will be created (with values provided image-major) - and those can be  referred as normal outputs (<code>$steps.{step_name}.{field_name}</code>).</li> </ul>"},{"location":"workflows/use_lmm/#important-notes","title":"Important notes","text":"<ul> <li><code>CogVLM</code> can only be used in <code>self-hosted</code> API - as Roboflow platform does not support such model.  Use <code>inference server start</code> on a machine with GPU to test that model.</li> </ul>"},{"location":"workflows/use_lmm_classification/","title":"<code>LMMForClassification</code>","text":"<p>Use LMMs (both <code>GPT-4V</code> and <code>CogVLM</code> models) as zero-shot classification blocks.</p>"},{"location":"workflows/use_lmm_classification/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>LMM</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>lmm_type</code>: must be string of reference to <code>InferenceParameter</code> - value holds the type of LMM model to be used -  allowed values: <code>gpt_4v</code> and <code>cog_vlm</code> (required)</li> <li><code>classes</code> - non-empty list of class names (strings) or reference to <code>InferenceParameter</code> that holds this value.  Classes are presented to LMM in prompt and model is asked to produce structured classification output (required).</li> <li><code>remote_api_key</code> - optional string or reference to <code>InferenceParameter</code> that holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code> and do not require additional API key for CogVLM calls.</li> <li><code>lmm_config</code>: (optional) structure that has the following schema: <pre><code>{\n\"max_tokens\": 450,\n\"gpt_image_detail\": \"low\",\n\"gpt_model_version\": \"gpt-4-vision-preview\"\n}\n</code></pre> to control inner details of LMM prompting. All parameters now are suited to control GPT API calls. Default for max tokens is <code>450</code>, <code>gpt_image_detail</code> default is <code>auto</code> (allowed values: <code>low</code>, <code>auto</code>, <code>high</code>),  <code>gpt_model_version</code> is <code>gpt-4-vision-preview</code>.</li> </ul>"},{"location":"workflows/use_lmm_classification/#step-outputs","title":"Step outputs","text":"<ul> <li><code>raw_output</code> - raw output of LMM for each input image</li> <li><code>top</code> - name of predicted class for each image</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - type of prediction output: <code>classification</code></li> </ul>"},{"location":"workflows/use_lmm_classification/#important-notes","title":"Important notes","text":"<ul> <li><code>CogVLM</code> can only be used in <code>self-hosted</code> API - as Roboflow platform does not support such model.  Use <code>inference server start</code> on a machine with GPU to test that model.</li> </ul>"},{"location":"workflows/workflows_contribution/","title":"How to create custom <code>workflows</code> block","text":"<p>Note</p> <p>We have a plan to simplify creation of new blocks. That means changes into internal design of blocks and their interface. The document present current state which is surely suboptimal. We accept feedback and ideas for improvements in GitHub issues.</p> <p>Note</p> <p>Fundamental context knowledge about the nature of <code>workflows</code> is here. We recommend  acknowledging this document before further reading.</p>"},{"location":"workflows/workflows_contribution/#what-elements-make-workflows-block","title":"What elements make <code>workflows</code> block?","text":"<p>Each <code>workflows</code> block has:</p> <ul> <li>block manifest in form of <code>pydantic</code> entity - providing structure for JSON step definition and a logic to validate input</li> <li>function to execute step logic with the following signature:</li> </ul> <pre><code>async def run_xxx_step(\n    step: YourStepType,\n    runtime_parameters: Dict[str, Any],\n    outputs_lookup: OutputsLookup,\n    model_manager: ModelManager,\n    api_key: Optional[str],\n    step_execution_mode: StepExecutionMode,\n) -&gt; Tuple[NextStepReference, OutputsLookup]:\n    ...\n</code></pre> <p>Both manifest and <code>run_xxx_step()</code> function will be used by <code>workflow</code> compiler and executor modules. Manifest  holds the source of truth regarding required inputs and the rules dictating which steps may be connected with each  other.</p>"},{"location":"workflows/workflows_contribution/#what-steps-need-to-be-completed-to-successfully-add-workflows-block","title":"What steps need to be completed to successfully add <code>workflows</code> block?","text":"<ol> <li>Define the block manifest entity and implement all necessary validation logic</li> <li>Implement step execution logic</li> <li>Register the step in execution engine module adding entry to <code>STEP_TYPE2EXECUTOR_MAPPING</code></li> <li>Register manifest entity in workflow specification adding entry into <code>StepType</code> union</li> <li>At this step, you should be able to add newly created block into JSON <code>workflow</code> definition and run it using one of <code>workflows</code> execution entrypoint (Python package function or HTTP endpoint)</li> </ol> <p>Initial design of <code>workflows</code> was intended to make compiler and execution engine do heavy-lifting in terms of organising execution, but there still may be needs to re-design those core modules if we find corner-cases that are not handled. Please report them in GitHub issues.</p>"},{"location":"workflows/workflows_contribution/#how-to-define-block-manifest","title":"How to define block manifest?","text":"<p>Note</p> <p>Package with blocks manifest is located here</p> <p>Block manifests are located here  in <code>inference</code> repository structure. Creating new one, you should start from:</p> <pre><code>from typing import Literal, Set, Optional, Any\nfrom pydantic import BaseModel\nfrom inference.enterprise.workflows.entities.steps import StepInterface\nfrom inference.enterprise.workflows.entities.base import GraphNone\n\nclass MyStep(BaseModel, StepInterface):\n    type: Literal[\"MyStep\"]\n    name: str\n    ... # place here other inputs that block takes\n\n    def get_input_names(self) -&gt; Set[str]:\n        ...  # Supposed to give the name of all fields expected to be possible for compiler to plug values into\n\n    def get_output_names(self) -&gt; Set[str]:\n        ... # Supposed to give the name of all fields expected to represent outputs to be referred by other blocks\n\n    def validate_field_selector(\n        self, field_name: str, input_step: GraphNone, index: Optional[int] = None\n    ) -&gt; None:\n        ...  # Supposed to validate the type of input is referred\n\n    def validate_field_binding(self, field_name: str, value: Any) -&gt; None:\n\"\"\"\n        Supposed to validate the type of value that is to be bounded with field as a result of graph\n        execution (values passed by client to invocation, as well as constructed during graph execution)\n        \"\"\"\n        ...  \n\n    def get_type(self) -&gt; str:\n        return self.type\n</code></pre> <p>Let's discuss one-by-one the elements of manifest.</p>"},{"location":"workflows/workflows_contribution/#pydantic-model-fields","title":"Pydantic model fields","text":"<p>We require <code>type</code> and <code>name</code> fields to be defined. Rest is up to you. Let's assume that we deal with step that accept image and additional threshold parameter. Then step definition would look like that:</p> <pre><code>from typing import Literal, Union\nfrom pydantic import BaseModel\nfrom inference.enterprise.workflows.entities.steps import StepInterface\n\nclass MyStep(BaseModel, StepInterface):\n    type: Literal[\"MyStep\"]\n    name: str\n    image: str\n    confidence: Union[float, str]\n</code></pre> <p>The idea behind <code>workflows</code> is to be able to set the parameters directly in JSON definition of steps, but also make it possible to defer injection of parameters to <code>workflows</code> runtime, when specific values would either been calculated or provided by users as additional (static) input. </p> <p>What happens with <code>image</code> here - we say that it is of type <code>str</code>, with intention of that string to hold reference to either user input or other step output. That's why we do not have this field of type <code>np.ndarray</code> or any other that usually holds image data. </p> <p>With <code>confidence</code>, however, we may want to define the value either in JSON definition of <code>workflow</code>, or as a reference. That's why we allow either <code>float</code> value to be defined or <code>str</code>.</p> <p>We would also want to be able to validate <code>workflows</code> definitions using <code>pydantic</code> validation engine. To make that happen, you need to create custom validator method for specific fields:</p> <pre><code>from typing import Literal, Union, Any, List, Optional\nfrom pydantic import BaseModel, field_validator\nfrom inference.enterprise.workflows.entities.steps import StepInterface\nfrom inference.enterprise.workflows.entities.validators import (\n    validate_image_is_valid_selector,\n    validate_field_is_in_range_zero_one_or_empty_or_selector,\n)\n\nclass MyStep(BaseModel, StepInterface):\n    type: Literal[\"MyStep\"]\n    name: str\n    image: str\n    confidence: Union[Optional[float], str]\n\n    @field_validator(\"image\")\n    @classmethod\n    def validate_image(cls, value: Any) -&gt; Union[str, List[str]]:\n        validate_image_is_valid_selector(value=value)\n        return value\n\n    @field_validator(\"confidence\")\n    @classmethod\n    def confidence_must_be_selector_or_number(\n        cls, value: Any\n    ) -&gt; Union[Optional[float], str]:\n        validate_field_is_in_range_zero_one_or_empty_or_selector(value=value)\n        return value\n</code></pre> <p>In this example, you can see that our <code>image</code> field that hold <code>str</code> is only allowed to hold special kind of string -  namely selector that refers to specific element of <code>workflow</code>.</p> <p>It would be tedious to create custom validators for each and every field of each and every block. That's why we  have module with utils useful for validation that can be chained together to get desired effect.  See <code>inference.enterprise.workflows.entities.validators</code> module</p>"},{"location":"workflows/workflows_contribution/#why-do-i-need-other-methods-from-the-step-interface","title":"Why do I need other methods from the step interface?","text":"<p><code>Pydantic</code> validation is very important in making sure that what is sent as JSON definition of <code>workflow</code> is actually a  valid one, but <code>StepInterface</code> requires you to implement a couple of additional methods. Let's discover their purpose.</p> <p><code>get_input_names(...)</code> allows compiler to discover the \"placeholders\" which can be filled with values in the runtime. You should return a set of field names defined in entity that are possible to hold selectors - and that will be possible to substitute with actual values in the runtime.</p> <p><code>get_output_names(...)</code> is the way to define block outputs - that can be used in selectors of other steps in <code>workflow</code></p> <p><code>get_type(...)</code> should simply return value of <code>type</code> field</p> <p>Two important, and potentially confusing methods are: <code>validate_field_selector(...)</code> and <code>validate_field_binding(...)</code>.</p>"},{"location":"workflows/workflows_contribution/#validation-of-field-selector","title":"Validation of field selector","text":"<p><code>validate_field_selector(...)</code> is used during execution graph construction stage of compiler. That method is supposed  to validate selectors defined in block fields - in particular type of steps / inputs that selector refers to. In our case:</p> <pre><code>from typing import Literal, Union, Optional\nfrom pydantic import BaseModel\nfrom inference.enterprise.workflows.entities.steps import StepInterface\nfrom inference.enterprise.workflows.entities.validators import (\n    is_selector,\n    validate_selector_holds_image,\n    validate_selector_is_inference_parameter,\n)\nfrom inference.enterprise.workflows.entities.base import GraphNone\nfrom inference.enterprise.workflows.errors import ExecutionGraphError\n\nclass MyStep(BaseModel, StepInterface):\n    type: Literal[\"MyStep\"]\n    name: str\n    image: str\n    confidence: Union[Optional[float], str]\n\n    # ... pydantic validation skipped for readability\n\n    def validate_field_selector(\n        self, field_name: str, input_step: GraphNone, index: Optional[int] = None\n    ) -&gt; None:\n        if not is_selector(selector_or_value=getattr(self, field_name)):\n            raise ExecutionGraphError(\n                f\"Attempted to validate selector value for field {field_name}, but field is not selector.\"\n            )\n        validate_selector_holds_image(\n            step_type=self.type,\n            field_name=field_name,\n            input_step=input_step,\n        )\n        validate_selector_is_inference_parameter(\n            step_type=self.type,\n            field_name=field_name,\n            input_step=input_step,\n            applicable_fields={\"confidence\"},\n        )\n</code></pre> <p>Compiler is going to use <code>validate_field_selector(...)</code> only against detected selectors - so initial check should be done to catch corner-cases when it does not work and provide meaningful error message. In the next stage, <code>validate_selector_holds_image(...)</code> that triggers if <code>field_name=image</code> by default is going to check if <code>input_step</code> that was referred by the selector (and delivered to the method automatically by compiler) is an element of the graph that holds image in their output (<code>InferenceImage</code> or step with image output). In final stage - <code>validate_selector_is_inference_parameter(...)</code> that triggers on field <code>confidence</code> will check  if the <code>input_step</code> is <code>InferenceParameter</code> which is the only <code>workflow</code> element supposed to provide static value from user input in runtime.</p> <p>Additional parameter, called <code>index</code> will only be filled by compiler if specific manifest field is a list of selectors, then validation will happen for each element separately.</p>"},{"location":"workflows/workflows_contribution/#validation-of-input-binding","title":"Validation of input binding","text":"<p><code>validate_field_binding(...)</code> is used by compiler while substituting selectors with values provided as user input into <code>workflow</code> execution. It plays similar role to <code>pydantic</code> validation, just in context of user input available in runtime.</p> <p>Let's see how we can validate input binding in case of our example block:</p> <pre><code>from typing import Literal, Union, Optional, Any\nfrom pydantic import BaseModel\nfrom inference.enterprise.workflows.entities.steps import StepInterface\nfrom inference.enterprise.workflows.entities.validators import (\n    validate_image_biding,\n    validate_field_has_given_type\n)\nfrom inference.enterprise.workflows.errors import VariableTypeError\n\nclass MyStep(BaseModel, StepInterface):\n    type: Literal[\"MyStep\"]\n    name: str\n    image: str\n    confidence: Union[Optional[float], str]\n\n    # ... pydantic validation skipped for readability\n    # ... validate_field_selector(...) skipped for readability\n\n    def validate_field_binding(self, field_name: str, value: Any) -&gt; None:\n        if field_name == \"image\":\n            validate_image_biding(value=value)\n        elif field_name == \"confidence\":\n            validate_field_has_given_type(\n                field_name=field_name,\n                allowed_types=[float, type(None)],\n                value=value,\n                error=VariableTypeError,\n            )\n</code></pre>"},{"location":"workflows/workflows_contribution/#full-implementation-of-manifest","title":"Full implementation of manifest","text":"<pre><code>from typing import Literal, Union, Any, List, Optional, Set\nfrom pydantic import BaseModel, field_validator\nfrom inference.enterprise.workflows.entities.steps import StepInterface\nfrom inference.enterprise.workflows.entities.base import GraphNone\nfrom inference.enterprise.workflows.entities.validators import (\n    validate_image_is_valid_selector,\n    validate_field_is_in_range_zero_one_or_empty_or_selector,\n    is_selector,\n    validate_selector_holds_image,\n    validate_selector_is_inference_parameter,\n    validate_image_biding,\n    validate_field_has_given_type,\n)\nfrom inference.enterprise.workflows.errors import ExecutionGraphError, VariableTypeError\n\n\nclass MyStep(BaseModel, StepInterface):\n    type: Literal[\"MyStep\"]\n    name: str\n    image: str\n    confidence: Union[Optional[float], str]\n\n    @field_validator(\"image\")\n    @classmethod\n    def validate_image(cls, value: Any) -&gt; Union[str, List[str]]:\n        validate_image_is_valid_selector(value=value)\n        return value\n\n    @field_validator(\"confidence\")\n    @classmethod\n    def confidence_must_be_selector_or_number(\n        cls, value: Any\n    ) -&gt; Union[Optional[float], str]:\n        validate_field_is_in_range_zero_one_or_empty_or_selector(value=value)\n        return value\n\n    def get_input_names(self) -&gt; Set[str]:\n        return {\"image\", \"confidence\"}\n\n    def get_output_names(self) -&gt; Set[str]:\n        return {\"prediction\"}  # adjust this to the use-case\n\n    def validate_field_selector(\n        self, field_name: str, input_step: GraphNone, index: Optional[int] = None\n    ) -&gt; None:\n        if not is_selector(selector_or_value=getattr(self, field_name)):\n            raise ExecutionGraphError(\n                f\"Attempted to validate selector value for field {field_name}, but field is not selector.\"\n            )\n        validate_selector_holds_image(\n            step_type=self.type,\n            field_name=field_name,\n            input_step=input_step,\n        )\n        validate_selector_is_inference_parameter(\n            step_type=self.type,\n            field_name=field_name,\n            input_step=input_step,\n            applicable_fields={\"confidence\"},\n        )\n\n    def validate_field_binding(self, field_name: str, value: Any) -&gt; None:\n        if field_name == \"image\":\n            validate_image_biding(value=value)\n        elif field_name == \"confidence\":\n            validate_field_has_given_type(\n                field_name=field_name,\n                allowed_types=[float, type(None)],\n                value=value,\n                error=VariableTypeError,\n            )\n\n    def get_type(self) -&gt; str:\n        return self.type\n</code></pre>"},{"location":"workflows/workflows_contribution/#how-to-implement-block-logic","title":"How to implement block logic?","text":"<p>Let's start from defining the placeholder function:</p> <pre><code>from typing import Dict, Any, Optional, Tuple\nfrom inference.core.managers.base import ModelManager\nfrom inference.enterprise.workflows.complier.steps_executors.types import (\n    NextStepReference,\n    OutputsLookup,\n)\nfrom inference.enterprise.workflows.complier.entities import StepExecutionMode\n\n\nasync def run_my_step(\n    step: MyStep,\n    runtime_parameters: Dict[str, Any],\n    outputs_lookup: OutputsLookup,\n    model_manager: ModelManager,\n    api_key: Optional[str],\n    step_execution_mode: StepExecutionMode,\n) -&gt; Tuple[NextStepReference, OutputsLookup]:\n    ...\n</code></pre> <p><code>runtime_parameters</code> is dict with user parameters provided as an input for execution. </p> <p><code>OutputsLookup</code> is a dictionary with each step output.</p> <p><code>model_manager</code>, <code>api_key</code> are <code>inference</code> entities to deal with models.</p> <p><code>step_execution_mode</code> - dictates how step should be executed - locally, within boundary of process running workflow,  or as request to remote API (if applicable).</p> <p>What this function returns is optionally the reference of next step (in case of blocks that alter flow of execution) and <code>output_lookup</code> filled with step outputs.</p> <p>There are two important concepts that need to be discussed:</p> <ul> <li> <p>how to get actual values of parameters from <code>runtime_parameters</code> and <code>outputs_lookup</code></p> </li> <li> <p>how to register step outputs in <code>outputs_lookup</code></p> </li> </ul>"},{"location":"workflows/workflows_contribution/#resolution-of-parameters-at-block-level","title":"Resolution of parameters at block level","text":"<p>Note</p> <p>That's probably the most tedious and not needed element of block creation, as that could be fully resolved on the executor side. We'll try to make that better in next iteration.</p> <p>Note</p> <p>Package with blocks logic is located here</p> <p>In runtime, <code>runtime_parameters</code> and <code>outputs_lookup</code> holds actual values of parameters needed for execution, whereas <code>step</code> hold instance of block manifest entity, with combination of specific values and references at the fields level. To resolve all of those sources of data into values you should calculate results - you need to use helper functions: <code>resolve_parameter(...)</code> and <code>get_image(...)</code> - for images.</p> <p>Let's see how that would look like:</p> <pre><code>from typing import Dict, Any, Optional, Tuple\nfrom inference.core.managers.base import ModelManager\nfrom inference.enterprise.workflows.complier.steps_executors.types import (\n    NextStepReference,\n    OutputsLookup,\n)\nfrom inference.enterprise.workflows.complier.entities import StepExecutionMode\nfrom inference.enterprise.workflows.complier.steps_executors.utils import (\n    get_image,\n    resolve_parameter,\n)\n\nasync def run_my_step(\n    step: MyStep,\n    runtime_parameters: Dict[str, Any],\n    outputs_lookup: OutputsLookup,\n    model_manager: ModelManager,\n    api_key: Optional[str],\n    step_execution_mode: StepExecutionMode,\n) -&gt; Tuple[NextStepReference, OutputsLookup]:\n    images = get_image(   # image always is returned in list - single entry format {\"type\": \"...\", \"value\": \"...\"} matches image representation in `inference` server\n        step=step,\n        runtime_parameters=runtime_parameters,\n        outputs_lookup=outputs_lookup,\n    )\n    confidence = resolve_parameter(\n        selector_or_value=step.confidence,\n        runtime_parameters=runtime_parameters,\n        outputs_lookup=outputs_lookup,\n    )\n    ...\n</code></pre> <p>Then you need to make the processing (possibly including operations on <code>ModelManager</code> to get predictions from model). Representation of elements of <code>images</code> matches standard <code>inference</code> format - you can use <code>load_image(...)</code> function from  core of <code>inference</code> to get <code>np.array</code>.</p> <p>We shall now discuss the structure of <code>outputs_lookup</code>. It is dictionary that maps step name to it's output. Function should only add values under its step name, not modify existing values (which may lead to unexpected side effects).  Each block define outputs (via <code>get_output_names(...)</code>). As a value saved under step name you should place a dictionary, with keys being all elements of block manifest <code>get_output_names(...)</code> result. Under each of the key representing output  name you should save list of results - ordered by the order of images in the <code>image</code> list.</p> <p>Let's see how that would look like in practice:</p> <pre><code>from typing import Dict, Any, Optional, Tuple\nfrom inference.core.managers.base import ModelManager\nfrom inference.enterprise.workflows.complier.steps_executors.types import (\n    NextStepReference,\n    OutputsLookup,\n)\nfrom inference.enterprise.workflows.complier.entities import StepExecutionMode\nfrom inference.enterprise.workflows.complier.steps_executors.utils import (\n    get_image,\n    resolve_parameter,\n)\nfrom inference.enterprise.workflows.complier.utils import construct_step_selector\n\n\nasync def run_my_step(\n    step: MyStep,\n    runtime_parameters: Dict[str, Any],\n    outputs_lookup: OutputsLookup,\n    model_manager: ModelManager,\n    api_key: Optional[str],\n    step_execution_mode: StepExecutionMode,\n) -&gt; Tuple[NextStepReference, OutputsLookup]:\n    images = get_image(   # image always is returned in list - single entry format {\"type\": \"...\", \"value\": \"...\"} matches image representation in `inference` server\n        step=step,\n        runtime_parameters=runtime_parameters,\n        outputs_lookup=outputs_lookup,\n    )\n    confidence = resolve_parameter(\n        selector_or_value=step.confidence,\n        runtime_parameters=runtime_parameters,\n        outputs_lookup=outputs_lookup,\n    )\n    predictions = []\n    for single_image in images:\n        # ... make predictions, model\n        predictions.append({\"top\": \"cat\"})\n    outputs_lookup[construct_step_selector(step_name=step.name)] = {\"prediction\": predictions}\n    return None, outputs_lookup\n</code></pre>"},{"location":"workflows/workflows_contribution/#registration-of-the-step","title":"Registration of the step","text":"<p>To make step ready to be used, you need to register block in execution engine module:</p> <pre><code>STEP_TYPE2EXECUTOR_MAPPING = {\n    # ...,\n    \"MyStep\": run_my_step,\n}\n</code></pre> <p>and make changes in workflow specification: <pre><code>StepType = Annotated[\n    Union[\n        # ...,\n        MyStep\n    ],\n    Field(discriminator=\"type\"),\n]\n</code></pre></p> <p>At this point - your block should be ready to go!</p>"},{"location":"workflows/yolo_world/","title":"<code>YoloWorld</code>","text":"<p>This <code>workflows</code> block is supposed to bring Yolo World model to the <code>workflows</code> world! You can use it in a very similar way as other object detection models within <code>workflows</code>.</p> <p>Important</p> <p>This step for now only works in Python package and <code>inference</code> HTTP container hosted locally. Hosted Roboflow platform does not expose this model - hence you cannot use workflow with this step against <code>https://detect.roboflow.com</code> API and you cannot use it in combination with <code>remote</code> execution when remote target is set to <code>hosted</code> (applies for Python package and <code>inference</code> HTTP container).</p>"},{"location":"workflows/yolo_world/#step-parameters","title":"Step parameters","text":"<ul> <li><code>type</code>: must be <code>YoloWorld</code> (required)</li> <li><code>name</code>: must be unique within all steps - used as identifier (required)</li> <li><code>image</code>: must be a reference to input of type <code>InferenceImage</code> or <code>crops</code> output from steps executing cropping ( <code>Crop</code>, <code>AbsoluteStaticCrop</code>, <code>RelativeStaticCrop</code>) (required)</li> <li><code>class_names</code> - must be reference to parameter or list of strings with names of classes to be detected - Yolo World  model makes it possible to predict across classes that you pass in the runtime - so in each request to <code>workflows</code> you  may detect different objects without model retraining. (required)</li> <li><code>version</code> - allows to specify model version. It is optional parameter, but when value is given it must be one of  [<code>s</code>, <code>m</code>, <code>l</code>]</li> <li><code>confidence</code> - optional parameter to specify confidence threshold. If given - must be number in range <code>[0.0, 1.0]</code></li> </ul>"},{"location":"workflows/yolo_world/#step-outputs","title":"Step outputs","text":"<ul> <li><code>predictions</code> - details of predictions</li> <li><code>image</code> - size of input image, that <code>predictions</code> coordinates refers to </li> <li><code>parent_id</code> - identifier of parent image / associated detection that helps to identify predictions with RoI in case of multi-step pipelines</li> <li><code>prediction_type</code> - denoting <code>keypoint-detection</code> model</li> </ul>"}]}